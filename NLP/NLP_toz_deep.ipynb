{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3eafcfee-6a83-416c-b087-fa322648cd6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\cupy\\_environment.py:584: UserWarning: \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "  CuPy may not function correctly because multiple CuPy packages are installed\n",
      "  in your environment:\n",
      "\n",
      "    cupy, cupy-cuda12x\n",
      "\n",
      "  Follow these steps to resolve this issue:\n",
      "\n",
      "    1. For all packages listed above, run the following command to remove all\n",
      "       existing CuPy installations:\n",
      "\n",
      "         $ pip uninstall <package_name>\n",
      "\n",
      "      If you previously installed CuPy via conda, also run the following:\n",
      "\n",
      "         $ conda uninstall cupy\n",
      "\n",
      "    2. Install the appropriate CuPy package.\n",
      "       Refer to the Installation Guide for detailed instructions.\n",
      "\n",
      "         https://docs.cupy.dev/en/stable/install.html\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "  warnings.warn(f'''\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Используемое устройство: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "import numba as nb \n",
    "from numba import njit, prange,jit\n",
    "import cupy as cp\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Инициализация устройств\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Используемое устройство: {device}\")\n",
    "\n",
    "# Конфигурация\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a33322ee-8842-4b74-9383-8c7ee16ff29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка данных...\n"
     ]
    }
   ],
   "source": [
    "# Функция для анализа текста с CatBoost\n",
    "@njit(parallel=True)\n",
    "def analyze_text_with_catboost(texts, vectorizer, model):\n",
    "    X = vectorizer.transform(texts)\n",
    "    results = np.empty(len(texts), dtype=np.float32)\n",
    "    for i in prange(len(texts)):\n",
    "        results[i] = model.predict_proba(X[i])[0][1]\n",
    "    return results\n",
    "    \n",
    "\n",
    "# Загрузка данных\n",
    "#@njit\n",
    "def load_data(path):\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "print(\"Загрузка данных...\")\n",
    "df = load_data('toxic_comments.csv')\n",
    "\n",
    "# Явное приведение к строке\n",
    "\n",
    "def convert_to_string(obj):\n",
    "    \n",
    "    return str(obj).encode('utf-8').decode('utf-8') \n",
    "    \n",
    "#df['text'] = convert_to_string(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51d4931c-6702-428d-8ed3-194e548d2155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Предобработка текста...\n"
     ]
    }
   ],
   "source": [
    "# Предобработка текста\n",
    "\n",
    "def preprocess_text_gpu(texts):\n",
    "    processed = []\n",
    "    for i in prange(len(texts)):\n",
    "        text = str(texts[i]).lower()\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        processed.append(text)\n",
    "    return processed\n",
    "\n",
    "print(\"Предобработка текста...\")\n",
    "df['text_clean'] = preprocess_text_gpu(df['text'].values)\n",
    "y = df['toxic'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6016c054-9c8a-4b91-a24a-3c728d001267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>explanation\\nwhy the edits made under my usern...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>daww he matches this background colour im seem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>hey man im really not trying to edit war its j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>\\nmore\\ni cant make any real suggestions on im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>you sir are my hero any chance you remember wh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  toxic  \\\n",
       "0           0  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1           1  D'aww! He matches this background colour I'm s...      0   \n",
       "2           2  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3           3  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4           4  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "                                          text_clean  \n",
       "0  explanation\\nwhy the edits made under my usern...  \n",
       "1  daww he matches this background colour im seem...  \n",
       "2  hey man im really not trying to edit war its j...  \n",
       "3  \\nmore\\ni cant make any real suggestions on im...  \n",
       "4  you sir are my hero any chance you remember wh...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c9c13c2-a5ce-49b4-8162-2258084ec793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF токенизация...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
       "\twith 3357097 stored elements and shape (159292, 5000)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. TF-IDF Токенизация\n",
    "print(\"TF-IDF токенизация...\")\n",
    "tfidf = TfidfVectorizer(max_features=5000,ngram_range=(1, 5),stop_words= 'english')\n",
    "X_tfidf = tfidf.fit_transform(df['text_clean'])\n",
    "X_tfidf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33a296cc-d908-41a7-9c11-7365ebc036e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec токенизация...\n",
      "Обучение Word2Vec модели...\n",
      "Создание Word2Vec векторов...\n"
     ]
    }
   ],
   "source": [
    "# 2. Word2Vec Токенизация\n",
    "\n",
    "def tokenize_texts(texts):\n",
    "    tokens = []\n",
    "    for i in prange(len(texts)):\n",
    "        tokens.append(simple_preprocess(str(texts[i]), deacc=True))\n",
    "    return tokens\n",
    "\n",
    "print(\"Word2Vec токенизация...\")\n",
    "sentences = tokenize_texts(df['text_clean'].values)    \n",
    "\n",
    "print(\"Обучение Word2Vec модели...\")\n",
    "w2v_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "\n",
    "def text_to_vector_gpu(texts, model):\n",
    "    vectors = np.empty((len(texts), model.vector_size), dtype=np.float32)\n",
    "    for i in prange(len(texts)):\n",
    "        words = simple_preprocess(str(texts[i]), deacc=True)\n",
    "        word_vecs = [model.wv[word] for word in words if word in model.wv]\n",
    "        vectors[i] = np.mean(word_vecs, axis=0) if len(word_vecs) > 0 else np.zeros(model.vector_size)\n",
    "    return vectors\n",
    "\n",
    "print(\"Создание Word2Vec векторов...\")\n",
    "X_w2v = text_to_vector_gpu(df['text_clean'].values, w2v_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f593358-58c2-47d2-9c82-6d7509f3e578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Инициализация BERT...\n",
      "BERT токенизация...\n",
      "Получение BERT эмбеддингов...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 4978/4978 [2:20:44<00:00,  1.70s/it]\n"
     ]
    }
   ],
   "source": [
    "# 3. BERT Токенизация и получение эмбеддингов\n",
    "print(\"Инициализация BERT...\")\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('unitary/toxic-bert') #('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained(('unitary/toxic-bert')).to(device)\n",
    "\n",
    "\n",
    "def bert_tokenize(texts):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    for text in texts:\n",
    "        encoded = bert_tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=MAX_LEN,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids.append(encoded['input_ids'])\n",
    "        attention_masks.append(encoded['attention_mask'])\n",
    "    \n",
    "    return torch.cat(input_ids, dim=0), torch.cat(attention_masks, dim=0)\n",
    "\n",
    "print(\"BERT токенизация...\")\n",
    "input_ids, attention_masks = bert_tokenize(df['text_clean'].tolist())\n",
    "\n",
    "\n",
    "def get_bert_embeddings(input_ids, attention_masks):\n",
    "    bert_model.eval()\n",
    "    embeddings = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(input_ids), BATCH_SIZE)):\n",
    "            batch_ids = input_ids[i:i+BATCH_SIZE].to(device)\n",
    "            batch_masks = attention_masks[i:i+BATCH_SIZE].to(device)\n",
    "            \n",
    "            outputs = bert_model(batch_ids, attention_mask=batch_masks)\n",
    "            embeddings.append(outputs.last_hidden_state[:,0,:].cpu().numpy())\n",
    "    \n",
    "    return np.concatenate(embeddings, axis=0)\n",
    "\n",
    "print(\"Получение BERT эмбеддингов...\")\n",
    "X_bert = get_bert_embeddings(input_ids, attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c0bfc5-eb32-497b-9d34-08e2f17392a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2bc56320-524e-4923-bb69-1c7ab62bc0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Модели с GPU поддержкой\n",
    "models = {\n",
    "    'CatBoost': CatBoostClassifier(task_type='GPU', iterations=500, verbose=0),\n",
    "    'XGBoost': XGBClassifier(tree_method='gpu_hist', gpu_id=0),\n",
    "    'LightGBM': LGBMClassifier(device='gpu')\n",
    "}\n",
    "\n",
    "# Кросс-валидация\n",
    "def evaluate_model(X, y, model, n_splits=3):\n",
    "    scores = []\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    fold_size = X.shape[0] // n_splits\n",
    "    \n",
    "    for i in range(n_splits):\n",
    "        test_idx = indices[i*fold_size:(i+1)*fold_size]\n",
    "        train_idx = np.concatenate([indices[:i*fold_size], indices[(i+1)*fold_size:]])\n",
    "        \n",
    "        model.fit(X[train_idx], y[train_idx])\n",
    "        y_pred = model.predict(X[test_idx])\n",
    "        scores.append(f1_score(y[test_idx], y_pred))\n",
    "    \n",
    "    return np.mean(scores), np.std(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11b79f94-66de-4838-becc-eabda60044de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159292"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.array(X_bert            ))\n",
    "X_tfidf.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe99710-73b3-4225-a24b-2950b20f3555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Оценка моделей:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: less than 75% GPU memory available for training. Free: 950.1027355 Total: 2047.875\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Оценка всех комбинаций\n",
    "results = []\n",
    "\n",
    "print(\"\\nОценка моделей:\")\n",
    "for name, model in models.items():\n",
    "    # TF-IDF\n",
    "    mean_f1, std_f1 = evaluate_model(X_tfidf, y, model)\n",
    "    results.append({'Модель': name, 'Токенизация': 'TF-IDF', 'F1': mean_f1, 'Std': std_f1})\n",
    "    \n",
    "    # Word2Vec\n",
    "    mean_f1, std_f1 = evaluate_model(X_w2v, y, model)\n",
    "    results.append({'Модель': name, 'Токенизация': 'Word2Vec', 'F1': mean_f1, 'Std': std_f1})\n",
    "    \n",
    "    # BERT\n",
    "    mean_f1, std_f1 = evaluate_model(X_bert, y, model)\n",
    "    results.append({'Модель': name, 'Токенизация': 'BERT', 'F1': mean_f1, 'Std': std_f1})\n",
    "\n",
    "# Результаты\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nРезультаты кросс-валидации:\")\n",
    "print(results_df)\n",
    "\n",
    "# Анализ с CatBoost\n",
    "print(\"\\nАнализ текста с CatBoost...\")\n",
    "best_model = CatBoostClassifier(task_type='GPU', iterations=500, verbose=0)\n",
    "best_model.fit(X_tfidf, y)\n",
    "text_analysis = analyze_text_with_catboost(df['text_clean'].values, tfidf, best_model)\n",
    "\n",
    "# Визуализация\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(text_analysis, bins=50, alpha=0.7)\n",
    "plt.title('Распределение вероятностей токсичности')\n",
    "plt.xlabel('Вероятность токсичности')\n",
    "plt.ylabel('Количество комментариев')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "results_df.groupby(['Модель', 'Токенизация'])['F1'].mean().unstack().plot(kind='bar')\n",
    "plt.title('Сравнение методов токенизации')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d53e460-86bc-4eac-ad70-46321db67558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN модель\n",
    "def build_cnn_model(vocab_size=5000, max_len=100):\n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size, 64, input_length=max_len),\n",
    "        Conv1D(64, 5, activation='relu'),\n",
    "        GlobalMaxPooling1D(),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Кросс-валидация\n",
    "def evaluate_models(X, y, vectorizers, models):\n",
    "    results = []\n",
    "    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    \n",
    "    for vec_name, vec in vectorizers.items():\n",
    "        for model_name, model in models.items():\n",
    "            pipeline = Pipeline([('vectorizer', vec), ('model', model)])\n",
    "            scores = cross_val_score(pipeline, X, y, cv=skf, scoring='f1', n_jobs=-1)\n",
    "            results.append({\n",
    "                'vectorizer': vec_name,\n",
    "                'model': model_name,\n",
    "                'f1_mean': np.mean(scores),\n",
    "                'f1_std': np.std(scores)\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7207aacf-c1c0-4e79-a17a-18944b953074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оценка для каждого типа токенизации\n",
    "results_simple = evaluate_models(X['text_simple'], y, vectorizers, models)\n",
    "results_lemmatized = evaluate_models(X['text_lemmatized'], y, vectorizers, models)\n",
    "results_pos = evaluate_models(X['text_pos'], y, vectorizers, models)\n",
    "torch.cuda.empty_cache()\n",
    "# Объединение результатов\n",
    "results_all = pd.concat([\n",
    "    results_simple.assign(tokenization='simple'),\n",
    "    results_lemmatized.assign(tokenization='lemmatized'),\n",
    "    results_pos.assign(tokenization='pos_tags')\n",
    "])\n",
    "\n",
    "# CNN оценка\n",
    "def evaluate_cnn(X_texts, y, tokenization_type):\n",
    "    tokenizer = Tokenizer(num_words=5000)\n",
    "    tokenizer.fit_on_texts(X_texts)\n",
    "    X_seq = tokenizer.texts_to_sequences(X_texts)\n",
    "    X_pad = pad_sequences(X_seq, maxlen=100)\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    f1_scores = []\n",
    "    \n",
    "    for train_idx, test_idx in skf.split(X_pad, y):\n",
    "        model = build_cnn_model()\n",
    "        model.fit(X_pad[train_idx], y[train_idx], epochs=3, batch_size=64, verbose=0)\n",
    "        y_pred = (model.predict(X_pad[test_idx]) > 0.5).astype(int)\n",
    "        f1_scores.append(f1_score(y[test_idx], y_pred))\n",
    "    \n",
    "    return {\n",
    "        'vectorizer': 'embedding',\n",
    "        'model': 'CNN',\n",
    "        'f1_mean': np.mean(f1_scores),\n",
    "        'f1_std': np.std(f1_scores),\n",
    "        'tokenization': tokenization_type\n",
    "    }\n",
    "\n",
    "# Добавляем CNN результаты\n",
    "cnn_results = [\n",
    "    evaluate_cnn(X['text_simple'], y, 'simple'),\n",
    "    evaluate_cnn(X['text_lemmatized'], y, 'lemmatized'),\n",
    "    evaluate_cnn(X['text_pos'], y, 'pos_tags')\n",
    "]\n",
    "\n",
    "results_all = pd.concat([results_all, pd.DataFrame(cnn_results)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c2016c-ae6c-4e53-a674-3b667f0892db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Анализ результатов\n",
    "print(\"Лучшие комбинации по типу токенизации:\")\n",
    "print(results_all.groupby('tokenization').apply(lambda x: x.nlargest(3, 'f1_mean')))\n",
    "\n",
    "print(\"\\nЛучшие комбинации по модели:\")\n",
    "print(results_all.groupby('model').apply(lambda x: x.nlargest(3, 'f1_mean')))\n",
    "\n",
    "print(\"\\nЛучшие комбинации по векторйзеру:\")\n",
    "print(results_all.groupby('vectorizer').apply(lambda x: x.nlargest(3, 'f1_mean')))\n",
    "\n",
    "# Визуализация\n",
    "plt.figure(figsize=(15, 8))\n",
    "for token_type in results_all['tokenization'].unique():\n",
    "    subset = results_all[results_all['tokenization'] == token_type]\n",
    "    plt.errorbar(subset['model'], subset['f1_mean'], yerr=subset['f1_std'], \n",
    "                fmt='o', label=token_type, capsize=5)\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('Сравнение моделей по типам токенизации')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c48f39-f18e-4ab4-8dfc-524ef01371d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd830625-0f6e-463d-826f-48110c3638ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
