{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mihail-Chr/projects/blob/main/ozon/opimize_pipline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U pip\n",
        "#!pip install --use-pep517 suod\n",
        "!pip install \"polars>=1.25,<1.29\"\n",
        "!pip install catboost yake threadpoolctl\n",
        "!pip install polars-splitters\n"
      ],
      "metadata": {
        "id": "J3nY78nONVAX"
      },
      "id": "J3nY78nONVAX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ed6b663-b3ec-496c-ba50-195fb99185e4",
      "metadata": {
        "id": "0ed6b663-b3ec-496c-ba50-195fb99185e4"
      },
      "outputs": [],
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from catboost import CatBoostClassifier, Pool\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.feature_selection import mutual_info_classif, SelectKBest\n",
        "from sklearn.metrics import f1_score, confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import gc\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "try:\n",
        "    import cudf\n",
        "    import cuml\n",
        "    GPU_AVAILABLE = True\n",
        "except ImportError:\n",
        "    GPU_AVAILABLE = False\n",
        "\n",
        "RANDOM_STATE = 255\n",
        "np.random.seed(RANDOM_STATE)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25b23fc8-260d-4e31-bdc7-a3a1e6038e7c",
      "metadata": {
        "id": "25b23fc8-260d-4e31-bdc7-a3a1e6038e7c"
      },
      "source": [
        "## Инициализация"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3e0c9ac-18ef-4d44-9d07-53200292bdf9",
      "metadata": {
        "id": "b3e0c9ac-18ef-4d44-9d07-53200292bdf9"
      },
      "outputs": [],
      "source": [
        "class OptimizedTextAwarePipeline:\n",
        "\n",
        "    def __init__(self, target='resolution'):\n",
        "        self.target = target\n",
        "        self.random_state = RANDOM_STATE\n",
        "        self.col_drop = ['id', 'ItemID', 'SellerID']\n",
        "        self.hard_threshold_low = 0.2\n",
        "        self.hard_threshold_high = 0.8\n",
        "\n",
        "        self.text_columns = []\n",
        "        self.models = {}\n",
        "        self.features = {}\n",
        "        self.label_encoders = {}  # Для кодирования категориальных признаков\n",
        "\n",
        "        # GPU detection\n",
        "        self.gpu_available = GPU_AVAILABLE\n",
        "        self.device_type = \"GPU\" if self.gpu_available else \"CPU\"\n",
        "\n",
        "        # Настройка обработки текста для CatBoost\n",
        "        self.text_processing_params = {\n",
        "            'tokenizers': [{'tokenizer_id': 'Space', 'separator_type': 'ByDelimiter', 'delimiter': ' '}],\n",
        "            'dictionaries': [{'dictionary_id': 'Word', 'max_dictionary_size': 50000}],\n",
        "            'feature_calcers': [{'calcer_id': 'BoW', 'top_tokens_count': 1000}]\n",
        "        }\n",
        "\n",
        "        # Оптимизированные параметры CatBoost\n",
        "        self.base_catboost_params = {\n",
        "            'random_seed': self.random_state,\n",
        "            'task_type': self.device_type,\n",
        "            'verbose': False,\n",
        "            'eval_metric': 'F1',\n",
        "            'loss_function': 'Logloss',\n",
        "            'use_best_model': True,\n",
        "            'early_stopping_rounds': 30,\n",
        "            'thread_count': -1,\n",
        "            'allow_writing_files': False,\n",
        "            'text_processing': self.text_processing_params\n",
        "        }\n",
        "\n",
        "        if self.gpu_available:\n",
        "            self.base_catboost_params.update({\n",
        "                'gpu_ram_part': 0.8,\n",
        "                'used_ram_limit': '8gb'\n",
        "            })\n",
        "\n",
        "    def identify_text_columns(self, df: pl.DataFrame):\n",
        "        \"\"\"Определяем текстовые колонки\"\"\"\n",
        "        self.text_columns = [col for col, dtype in df.schema.items()\n",
        "                             if dtype == pl.String and col not in self.col_drop + [self.target]]\n",
        "        print(f\"Найдено текстовых колонок: {len(self.text_columns)}\")\n",
        "        return self.text_columns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9acb08e1-a340-466a-937e-313277a26039",
      "metadata": {
        "id": "9acb08e1-a340-466a-937e-313277a26039"
      },
      "source": [
        "## Генерация continuous/categorical признаков"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "601f851f-857e-4e66-82c8-55f86cf77844",
      "metadata": {
        "id": "601f851f-857e-4e66-82c8-55f86cf77844"
      },
      "outputs": [],
      "source": [
        "def advanced_feature_engineering(self, df: pl.DataFrame, stage: str) -> pl.DataFrame:\n",
        "\n",
        "    print(f\"Feature engineering для стадии {stage}...\")\n",
        "\n",
        "    # Получаем числовые колонки\n",
        "    numeric_cols = []\n",
        "    for col, dtype in df.schema.items():\n",
        "        if col not in self.col_drop + [self.target] + self.text_columns:\n",
        "            if dtype in [pl.Int8, pl.Int16, pl.Int32, pl.Int64, pl.Float32, pl.Float64]:\n",
        "                numeric_cols.append(col)\n",
        "\n",
        "    # ПРАВИЛЬНО: Разделяем на continuous и categorical\n",
        "    continuous_cols = []\n",
        "    categorical_cols = []\n",
        "\n",
        "    for col in numeric_cols:\n",
        "        if col in df.columns:\n",
        "            unique_count = df[col].n_unique()\n",
        "            if unique_count <= 50:  # Категориальные\n",
        "                categorical_cols.append(col)\n",
        "            else:  # Непрерывные\n",
        "                continuous_cols.append(col)\n",
        "\n",
        "    print(f\"Continuous: {len(continuous_cols)}, Categorical: {len(categorical_cols)}\")\n",
        "\n",
        "    expressions = []\n",
        "\n",
        "    # 1. Трансформации ТОЛЬКО для continuous признаков\n",
        "    for col in continuous_cols[:15]:\n",
        "        if col in df.columns:\n",
        "            expressions.extend([\n",
        "                pl.when(pl.col(col) > 0).then(pl.col(col).log()).otherwise(0).alias(f\"{col}_log\"),\n",
        "                pl.when(pl.col(col) >= 0).then(pl.col(col).sqrt()).otherwise(0).alias(f\"{col}_sqrt\"),\n",
        "                (pl.col(col) ** 2).alias(f\"{col}_sq\"),\n",
        "                pl.when(pl.col(col) != 0).then(1.0 / pl.col(col)).otherwise(0).alias(f\"{col}_inv\")\n",
        "            ])\n",
        "\n",
        "    # 2. Парные взаимодействия для continuous\n",
        "    if len(continuous_cols) >= 2:\n",
        "        top_continuous = continuous_cols[:6]  # Ограничиваем для производительности\n",
        "        for i in range(len(top_continuous)):\n",
        "            for j in range(i+1, len(top_continuous)):\n",
        "                col1, col2 = top_continuous[i], top_continuous[j]\n",
        "                if col1 in df.columns and col2 in df.columns:\n",
        "                    expressions.extend([\n",
        "                        pl.when(pl.col(col2) != 0).then(pl.col(col1) / pl.col(col2)).otherwise(0).alias(f\"{col1}_{col2}_ratio\"),\n",
        "                        (pl.col(col1) * pl.col(col2)).alias(f\"{col1}_{col2}_prod\"),\n",
        "                        (pl.col(col1) - pl.col(col2)).alias(f\"{col1}_{col2}_diff\")\n",
        "                    ])\n",
        "\n",
        "    # 3. Percentile-based features для continuous\n",
        "    for col in continuous_cols[:10]:\n",
        "        if col in df.columns:\n",
        "            q75 = df[col].quantile(0.75)\n",
        "            q25 = df[col].quantile(0.25)\n",
        "            median = df[col].median()\n",
        "\n",
        "            expressions.extend([\n",
        "                (pl.col(col) > q75).cast(pl.Int8).alias(f\"{col}_q75_flag\"),\n",
        "                (pl.col(col) < q25).cast(pl.Int8).alias(f\"{col}_q25_flag\"),\n",
        "                (pl.col(col) > median).cast(pl.Int8).alias(f\"{col}_median_flag\")\n",
        "            ])\n",
        "\n",
        "    # Применяем трансформации батчами\n",
        "    if expressions:\n",
        "        batch_size = 50\n",
        "        for i in range(0, len(expressions), batch_size):\n",
        "            batch = expressions[i:i+batch_size]\n",
        "            try:\n",
        "                df = df.with_columns(batch)\n",
        "            except Exception as e:\n",
        "                print(f\"Ошибка в batch {i}: {e}\")\n",
        "                continue\n",
        "\n",
        "    print(f\"Сгенерировано {len(expressions)} новых признаков для {stage}\")\n",
        "    return df\n",
        "\n",
        "OptimizedTextAwarePipeline.advanced_feature_engineering = advanced_feature_engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1c9b4d5-2f95-49c8-830e-46ef67f029d7",
      "metadata": {
        "id": "b1c9b4d5-2f95-49c8-830e-46ef67f029d7"
      },
      "source": [
        "## Улучшенный отбор признаков"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1daf21b5-2ef7-4620-a46c-6cdd5ffce058",
      "metadata": {
        "id": "1daf21b5-2ef7-4620-a46c-6cdd5ffce058"
      },
      "outputs": [],
      "source": [
        "def improved_feature_selection(self, df_pandas: pd.DataFrame, stage: str, max_features: int = 30) -> list:\n",
        "\n",
        "    print(f\"Feature selection для {stage} (макс. {max_features} признаков)...\")\n",
        "\n",
        "    # Получаем все признаки кроме служебных\n",
        "    feature_cols = [col for col in df_pandas.columns\n",
        "                   if col not in self.col_drop + [self.target]]\n",
        "\n",
        "    if len(feature_cols) < 2:\n",
        "        return feature_cols\n",
        "\n",
        "    # Разделяем на числовые и текстовые\n",
        "    numeric_features = []\n",
        "    text_features = []\n",
        "\n",
        "    for col in feature_cols:\n",
        "        if col in self.text_columns:\n",
        "            text_features.append(col)\n",
        "        else:\n",
        "            try:\n",
        "                # Проверяем, является ли колонка числовой\n",
        "                pd.to_numeric(df_pandas[col], errors='raise')\n",
        "                numeric_features.append(col)\n",
        "            except (ValueError, TypeError):\n",
        "                # Если не числовая, кодируем как категориальную\n",
        "                if col not in self.label_encoders:\n",
        "                    self.label_encoders[col] = LabelEncoder()\n",
        "                    df_pandas[col] = self.label_encoders[col].fit_transform(df_pandas[col].fillna('missing'))\n",
        "                numeric_features.append(col)\n",
        "\n",
        "    selected_features = []\n",
        "\n",
        "    # Mutual information для числовых признаков\n",
        "    if numeric_features and len(numeric_features) > 0:\n",
        "        X_numeric = df_pandas[numeric_features].fillna(0)\n",
        "        y = df_pandas[self.target]\n",
        "\n",
        "        try:\n",
        "            mi_selector = SelectKBest(mutual_info_classif, k=min(max_features-5, len(numeric_features)))\n",
        "            mi_selector.fit(X_numeric, y)\n",
        "            selected_numeric = [numeric_features[i] for i in mi_selector.get_support(indices=True)]\n",
        "            selected_features.extend(selected_numeric)\n",
        "        except Exception as e:\n",
        "            print(f\"MI selection failed: {e}, using correlation\")\n",
        "            # Fallback на корреляцию\n",
        "            corr_with_target = X_numeric.corrwith(y).abs().sort_values(ascending=False)\n",
        "            selected_features.extend(corr_with_target.head(max_features-5).index.tolist())\n",
        "\n",
        "    # Добавляем важные текстовые признаки\n",
        "    selected_features.extend(text_features[:5])\n",
        "\n",
        "    # Ограничиваем до max_features\n",
        "    selected_features = selected_features[:max_features]\n",
        "\n",
        "    print(f\"Выбрано {len(selected_features)} признаков\")\n",
        "    return selected_features\n",
        "\n",
        "OptimizedTextAwarePipeline.improved_feature_selection = improved_feature_selection"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f4effe2-8953-4ae2-95b3-e98576d60372",
      "metadata": {
        "id": "3f4effe2-8953-4ae2-95b3-e98576d60372"
      },
      "source": [
        "## Визуализация результатов стадии"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd548d9d-6f1b-449d-b5b0-801de768225c",
      "metadata": {
        "id": "cd548d9d-6f1b-449d-b5b0-801de768225c"
      },
      "outputs": [],
      "source": [
        "def visualize_stage_results(self, model, X, y, features, stage_name):\n",
        "\n",
        "    # Предсказания\n",
        "    preds = model.predict(X)\n",
        "    probas = model.predict_proba(X)[:, 1]\n",
        "\n",
        "    # Создаем фигуру с подграфиками\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    fig.suptitle(f'Результаты модели {stage_name}', fontsize=16, fontweight='bold')\n",
        "\n",
        "    # 1. Confusion Matrix\n",
        "    cm = confusion_matrix(y, preds)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0,0])\n",
        "    axes[0,0].set_title('Confusion Matrix')\n",
        "    axes[0,0].set_xlabel('Predicted')\n",
        "    axes[0,0].set_ylabel('Actual')\n",
        "\n",
        "    # 2. Feature Importance\n",
        "    if hasattr(model, 'get_feature_importance'):\n",
        "        importances = model.get_feature_importance()\n",
        "        feat_imp = pd.DataFrame({\n",
        "            'feature': features,\n",
        "            'importance': importances\n",
        "        }).sort_values('importance', ascending=True).tail(10)\n",
        "\n",
        "        axes[0,1].barh(feat_imp['feature'], feat_imp['importance'])\n",
        "        axes[0,1].set_title('Top 10 Feature Importance')\n",
        "        axes[0,1].set_xlabel('Importance')\n",
        "\n",
        "    # 3. Probability Distribution\n",
        "    axes[1,0].hist(probas[y==0], bins=50, alpha=0.7, label='Class 0', color='red')\n",
        "    axes[1,0].hist(probas[y==1], bins=50, alpha=0.7, label='Class 1', color='green')\n",
        "    axes[1,0].axvline(self.hard_threshold_low, color='blue', linestyle='--', label=f'Low threshold ({self.hard_threshold_low})')\n",
        "    axes[1,0].axvline(self.hard_threshold_high, color='blue', linestyle='--', label=f'High threshold ({self.hard_threshold_high})')\n",
        "    axes[1,0].set_xlabel('Predicted Probability')\n",
        "    axes[1,0].set_ylabel('Count')\n",
        "    axes[1,0].set_title('Probability Distribution')\n",
        "    axes[1,0].legend()\n",
        "\n",
        "    # 4. Classification Report (текст)\n",
        "    cr = classification_report(y, preds, output_dict=True)\n",
        "    report_text = f\"\"\"Classification Report:\n",
        "\n",
        "            Precision (Class 0): {cr['0']['precision']:.3f}\n",
        "            Recall (Class 0): {cr['0']['recall']:.3f}\n",
        "            F1-Score (Class 0): {cr['0']['f1-score']:.3f}\n",
        "\n",
        "            Precision (Class 1): {cr['1']['precision']:.3f}\n",
        "            Recall (Class 1): {cr['1']['recall']:.3f}\n",
        "            F1-Score (Class 1): {cr['1']['f1-score']:.3f}\n",
        "\n",
        "            Overall F1-Score: {cr['macro avg']['f1-score']:.3f}\n",
        "            Accuracy: {cr['accuracy']:.3f}\"\"\"\n",
        "\n",
        "    axes[1,1].text(0.1, 0.5, report_text, fontsize=10, verticalalignment='center',\n",
        "                  transform=axes[1,1].transAxes, family='monospace')\n",
        "    axes[1,1].set_title('Performance Metrics')\n",
        "    axes[1,1].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "OptimizedTextAwarePipeline.visualize_stage_results = visualize_stage_results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73d637c9-a914-4ba8-880e-354a104a2dbe",
      "metadata": {
        "id": "73d637c9-a914-4ba8-880e-354a104a2dbe"
      },
      "source": [
        "## Запись уверенных предсказаний с корректной индексацией"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20342bc4-9cfa-42b0-a745-2fa3e672b4e0",
      "metadata": {
        "id": "20342bc4-9cfa-42b0-a745-2fa3e672b4e0"
      },
      "outputs": [],
      "source": [
        "def filter_confident_predictions(self, model, X, y, df_original, stage_name):\n",
        "    \"\"\"\"\"\"\n",
        "    print(f\"Фильтрация confident predictions для {stage_name}...\")\n",
        "\n",
        "    probas = model.predict_proba(X)[:, 1]\n",
        "    preds = model.predict(X)\n",
        "\n",
        "    # Определяем confident предсказания\n",
        "    low_confident = (probas < self.hard_threshold_low) & (preds == 0) & (y == 0)\n",
        "    high_confident = (probas > self.hard_threshold_high) & (preds == 1) & (y == 1)\n",
        "    confident_mask = low_confident | high_confident\n",
        "\n",
        "    # Получаем ID из исходного датафрейма\n",
        "    all_ids = df_original['id'].to_list()\n",
        "\n",
        "    # Разделяем на confident и remaining\n",
        "    confident_ids = [all_ids[i] for i in range(len(confident_mask)) if confident_mask[i]]\n",
        "    remaining_ids = [all_ids[i] for i in range(len(confident_mask)) if not confident_mask[i]]\n",
        "\n",
        "    df_confident = df_original.filter(pl.col('id').is_in(confident_ids))\n",
        "    df_remaining = df_original.filter(pl.col('id').is_in(remaining_ids))\n",
        "\n",
        "    print(f\"Confident: {len(confident_ids)}, Remaining: {len(remaining_ids)}\")\n",
        "\n",
        "    return df_confident, df_remaining\n",
        "\n",
        "OptimizedTextAwarePipeline.filter_confident_predictions = filter_confident_predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f4ebe10-8bb6-4b4f-8639-26afd7a241e4",
      "metadata": {
        "id": "3f4ebe10-8bb6-4b4f-8639-26afd7a241e4"
      },
      "source": [
        "## Обучение стадий с визуализацией"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aae71fad-3f0b-47b4-a0cc-67312e596857",
      "metadata": {
        "id": "aae71fad-3f0b-47b4-a0cc-67312e596857"
      },
      "outputs": [],
      "source": [
        "def train_stage(self, df: pl.DataFrame, stage_name: str, max_features: int):\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"ОБУЧЕНИЕ СТАДИИ {stage_name.upper()}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Размер выборки: {len(df)}\")\n",
        "\n",
        "    # Feature engineering\n",
        "    df_processed = self.advanced_feature_engineering(df, stage_name)\n",
        "    df_pandas = df_processed.to_pandas()\n",
        "\n",
        "    # Feature selection\n",
        "    features = self.improved_feature_selection(df_pandas, stage_name, max_features)\n",
        "\n",
        "    if len(features) == 0:\n",
        "        print(f\"Нет признаков для {stage_name}!\")\n",
        "        return None, df_pandas, features\n",
        "\n",
        "    X = df_pandas[features].copy()\n",
        "    y = df_pandas[self.target].copy()\n",
        "\n",
        "    # Обработка текстовых признаков\n",
        "    for col in features:\n",
        "        if col in self.text_columns:\n",
        "            X[col] = X[col].fillna('missing').astype(str)\n",
        "\n",
        "    # Определяем категориальные признаки\n",
        "    cat_features_idx = []\n",
        "    text_features_idx = []\n",
        "\n",
        "    for i, col in enumerate(features):\n",
        "        if col in self.text_columns:\n",
        "            text_features_idx.append(i)\n",
        "        elif X[col].dtype == 'object' or (X[col].nunique() <= 50 and X[col].dtype in ['int64', 'int32']):\n",
        "            cat_features_idx.append(i)\n",
        "\n",
        "    print(f\"Всего признаков: {len(features)}\")\n",
        "    print(f\"Категориальных: {len(cat_features_idx)}\")\n",
        "    print(f\"Текстовых: {len(text_features_idx)}\")\n",
        "\n",
        "    # Создаем Pool\n",
        "    train_pool = Pool(X, y, cat_features=cat_features_idx, text_features=text_features_idx)\n",
        "\n",
        "    # Настройки модели для разных стадий\n",
        "    stage_params = self.base_catboost_params.copy()\n",
        "    if stage_name == 'hard':\n",
        "        stage_params.update({'iterations': 400, 'depth': 6, 'learning_rate': 0.1})\n",
        "    elif stage_name == 'soft':\n",
        "        stage_params.update({'iterations': 600, 'depth': 8, 'learning_rate': 0.08})\n",
        "    else:  # error\n",
        "        stage_params.update({'iterations': 800, 'depth': 10, 'learning_rate': 0.05})\n",
        "\n",
        "    # Обучение модели\n",
        "    model = CatBoostClassifier(**stage_params)\n",
        "    model.fit(train_pool, verbose=False)\n",
        "\n",
        "    # Cross-validation score\n",
        "    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=255)\n",
        "    cv_scores = []\n",
        "\n",
        "    for train_idx, val_idx in skf.split(X, y):\n",
        "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "        temp_pool = Pool(X_train, y_train, cat_features=cat_features_idx, text_features=text_features_idx)\n",
        "        temp_model = CatBoostClassifier(**stage_params)\n",
        "        temp_model.fit(temp_pool, verbose=False)\n",
        "\n",
        "        preds = temp_model.predict(X_val)\n",
        "        f1 = f1_score(y_val, preds)\n",
        "        cv_scores.append(f1)\n",
        "\n",
        "    cv_f1 = np.mean(cv_scores)\n",
        "    print(f\"CV F1-score: {cv_f1:.4f} ± {np.std(cv_scores):.4f}\")\n",
        "\n",
        "    # Сохраняем модель и признаки\n",
        "    self.models[stage_name] = model\n",
        "    self.features[stage_name] = features\n",
        "\n",
        "    # Визуализация результатов\n",
        "    self.visualize_stage_results(model, X, y, features, stage_name)\n",
        "\n",
        "    return model, df_pandas, features\n",
        "\n",
        "OptimizedTextAwarePipeline.train_stage = train_stage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24812b38-d000-4b26-ac51-a9e66c2047f1",
      "metadata": {
        "id": "24812b38-d000-4b26-ac51-a9e66c2047f1"
      },
      "outputs": [],
      "source": [
        "## Общая визуализация пайплайна"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d462992-44ea-46e2-af6e-0ba48e60ede7",
      "metadata": {
        "id": "5d462992-44ea-46e2-af6e-0ba48e60ede7"
      },
      "outputs": [],
      "source": [
        "def visualize_pipeline_overview(self, stage_stats):\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    fig.suptitle('Обзор пайплайна Text-Aware Classification', fontsize=16, fontweight='bold')\n",
        "\n",
        "    # 1. Размер выборки по стадиям\n",
        "    stages = list(stage_stats.keys())\n",
        "    sample_sizes = [stage_stats[stage]['sample_size'] for stage in stages]\n",
        "\n",
        "    axes[0,0].bar(stages, sample_sizes, color=['#ff7f0e', '#2ca02c', '#d62728'])\n",
        "    axes[0,0].set_title('Размер выборки по стадиям')\n",
        "    axes[0,0].set_ylabel('Количество образцов')\n",
        "    for i, v in enumerate(sample_sizes):\n",
        "        axes[0,0].text(i, v + max(sample_sizes)*0.01, str(v), ha='center', va='bottom')\n",
        "\n",
        "    # 2. F1-score по стадиям\n",
        "    f1_scores = [stage_stats[stage]['f1_score'] for stage in stages if 'f1_score' in stage_stats[stage]]\n",
        "    if f1_scores:\n",
        "        axes[0,1].bar(stages[:len(f1_scores)], f1_scores, color=['#ff7f0e', '#2ca02c', '#d62728'])\n",
        "        axes[0,1].set_title('F1-Score по стадиям')\n",
        "        axes[0,1].set_ylabel('F1-Score')\n",
        "        axes[0,1].set_ylim(0, 1)\n",
        "        for i, v in enumerate(f1_scores):\n",
        "            axes[0,1].text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')\n",
        "\n",
        "    # 3. Количество признаков по стадиям\n",
        "    feature_counts = [stage_stats[stage]['feature_count'] for stage in stages if 'feature_count' in stage_stats[stage]]\n",
        "    if feature_counts:\n",
        "        axes[1,0].bar(stages[:len(feature_counts)], feature_counts, color=['#ff7f0e', '#2ca02c', '#d62728'])\n",
        "        axes[1,0].set_title('Количество признаков по стадиям')\n",
        "        axes[1,0].set_ylabel('Количество признаков')\n",
        "        for i, v in enumerate(feature_counts):\n",
        "            axes[1,0].text(i, v + max(feature_counts)*0.01, str(v), ha='center', va='bottom')\n",
        "\n",
        "    # 4. Общая статистика\n",
        "    total_samples = sum(sample_sizes)\n",
        "    processing_reduction = (sample_sizes[0] - sample_sizes[-1]) / sample_sizes[0] * 100 if len(sample_sizes) > 1 else 0\n",
        "\n",
        "    stats_text = f\"\"\"Общая статистика пайплайна:\n",
        "\n",
        "        Всего образцов в начале: {sample_sizes[0]:,}\n",
        "        Образцов на последней стадии: {sample_sizes[-1]:,}\n",
        "        Сокращение объема обработки: {processing_reduction:.1f}%\n",
        "\n",
        "        Количество стадий: {len(stages)}\n",
        "        Использование GPU: {'Да' if self.gpu_available else 'Нет'}\n",
        "        Текстовых признаков: {len(self.text_columns)}\n",
        "\n",
        "        Стратегия: Поэтапная фильтрация\n",
        "        confident predictions с уменьшением\n",
        "        сложности задачи на каждой стадии\"\"\"\n",
        "\n",
        "    axes[1,1].text(0.05, 0.5, stats_text, fontsize=10, verticalalignment='center',\n",
        "                  transform=axes[1,1].transAxes, family='monospace')\n",
        "    axes[1,1].set_title('Статистика пайплайна')\n",
        "    axes[1,1].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "OptimizedTextAwarePipeline.visualize_pipeline_overview = visualize_pipeline_overview"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbcc700f-8167-4099-99f4-e2745770fcee",
      "metadata": {
        "id": "cbcc700f-8167-4099-99f4-e2745770fcee"
      },
      "source": [
        "## Полное обучение"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bac3c3a-23ad-417e-96fd-51ba2b13741f",
      "metadata": {
        "id": "2bac3c3a-23ad-417e-96fd-51ba2b13741f"
      },
      "outputs": [],
      "source": [
        "def fit(self, train_df: pl.DataFrame):\n",
        "\n",
        "    print(\"🚀 ЗАПУСК ОПТИМИЗИРОВАННОГО TEXT-AWARE PIPELINE\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Определяем текстовые колонки\n",
        "    self.identify_text_columns(train_df)\n",
        "\n",
        "    # Заполняем пропуски\n",
        "    expressions = []\n",
        "    for col, dtype in train_df.schema.items():\n",
        "        if col not in self.col_drop + [self.target]:\n",
        "            if dtype in [pl.Int8, pl.Int16, pl.Int32, pl.Int64, pl.Float32, pl.Float64]:\n",
        "                expressions.append(pl.col(col).fill_null(0).alias(col))\n",
        "            elif dtype == pl.String:\n",
        "                expressions.append(pl.col(col).fill_null(\"missing\").alias(col))\n",
        "\n",
        "    if expressions:\n",
        "        train_df = train_df.with_columns(expressions)\n",
        "\n",
        "    current_df = train_df\n",
        "    stages = [('hard', 25), ('soft', 35), ('error', 50)]\n",
        "    stage_stats = {}\n",
        "\n",
        "    for stage_name, max_features in stages:\n",
        "        stage_stats[stage_name] = {'sample_size': len(current_df)}\n",
        "\n",
        "        if len(current_df) < 50:  # Минимальный размер для обучения\n",
        "            print(f\"Недостаточно данных для стадии {stage_name}: {len(current_df)}\")\n",
        "            break\n",
        "\n",
        "        # Обучение стадии\n",
        "        model, df_pandas, features = self.train_stage(current_df, stage_name, max_features)\n",
        "\n",
        "        if model is None:\n",
        "            break\n",
        "\n",
        "        stage_stats[stage_name].update({\n",
        "            'feature_count': len(features),\n",
        "            'f1_score': np.mean([f1_score(df_pandas[self.target], model.predict(df_pandas[features])) for _ in [1]])\n",
        "        })\n",
        "\n",
        "        # Фильтрация confident predictions\n",
        "        X = df_pandas[features].copy()\n",
        "        y = df_pandas[self.target].copy()\n",
        "\n",
        "        # Обработка текстовых признаков для фильтрации\n",
        "        for col in features:\n",
        "            if col in self.text_columns:\n",
        "                X[col] = X[col].fillna('missing').astype(str)\n",
        "\n",
        "        confident, remaining = self.filter_confident_predictions(model, X, y, current_df, stage_name)\n",
        "\n",
        "        current_df = remaining\n",
        "        gc.collect()\n",
        "\n",
        "        if len(current_df) == 0:\n",
        "            print(\"Все образцы обработаны уверенными предсказаниями!\")\n",
        "            break\n",
        "\n",
        "    # Общая визуализация\n",
        "    self.visualize_pipeline_overview(stage_stats)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"✅ ОБУЧЕНИЕ ПАЙПЛАЙНА ЗАВЕРШЕНО!\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    return self\n",
        "\n",
        "OptimizedTextAwarePipeline.fit = fit"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7dbad1bd-871e-4077-80e9-afdd71153e6e",
      "metadata": {
        "id": "7dbad1bd-871e-4077-80e9-afdd71153e6e"
      },
      "source": [
        "## Предсказание с поэтапной обработкой"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a7e9f37-7129-4eeb-b2cb-abaab9680e22",
      "metadata": {
        "id": "5a7e9f37-7129-4eeb-b2cb-abaab9680e22"
      },
      "outputs": [],
      "source": [
        "def predict(self, test_df: pl.DataFrame):\n",
        "\n",
        "    print(\"🔮 НАЧАЛО ПРЕДСКАЗАНИЯ\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Определяем текстовые колонки\n",
        "    self.identify_text_columns(test_df)\n",
        "\n",
        "    # Заполняем пропуски\n",
        "    expressions = []\n",
        "    for col, dtype in test_df.schema.items():\n",
        "        if col not in self.col_drop:\n",
        "            if dtype in [pl.Int8, pl.Int16, pl.Int32, pl.Int64, pl.Float32, pl.Float64]:\n",
        "                expressions.append(pl.col(col).fill_null(0).alias(col))\n",
        "            elif dtype == pl.String:\n",
        "                expressions.append(pl.col(col).fill_null(\"missing\").alias(col))\n",
        "\n",
        "    if expressions:\n",
        "        test_df = test_df.with_columns(expressions)\n",
        "\n",
        "    current_df = test_df\n",
        "    all_predictions = {}\n",
        "\n",
        "    for stage_name in ['hard', 'soft', 'error']:\n",
        "        if stage_name not in self.models or len(current_df) == 0:\n",
        "            continue\n",
        "\n",
        "        print(f\"Предсказание для стадии {stage_name}: {len(current_df)} образцов\")\n",
        "\n",
        "        model = self.models[stage_name]\n",
        "        features = self.features[stage_name]\n",
        "\n",
        "        # Feature engineering\n",
        "        df_processed = self.advanced_feature_engineering(current_df, stage_name)\n",
        "        df_pandas = df_processed.to_pandas()\n",
        "\n",
        "        # Проверяем доступность признаков\n",
        "        available_features = [f for f in features if f in df_pandas.columns]\n",
        "        if len(available_features) == 0:\n",
        "            print(f\"Нет доступных признаков для {stage_name}\")\n",
        "            continue\n",
        "\n",
        "        X_test = df_pandas[available_features].copy()\n",
        "\n",
        "        # Обработка текстовых признаков\n",
        "        for col in available_features:\n",
        "            if col in self.text_columns:\n",
        "                X_test[col] = X_test[col].fillna('missing').astype(str)\n",
        "            elif col in self.label_encoders:\n",
        "                # Применяем сохраненный encoder\n",
        "                X_test[col] = self.label_encoders[col].transform(X_test[col].fillna('missing'))\n",
        "\n",
        "        # Предсказания\n",
        "        probas = model.predict_proba(X_test)[:, 1]\n",
        "        preds = model.predict(X_test)\n",
        "\n",
        "        ids = current_df['id'].to_list()\n",
        "\n",
        "        # Определяем confident predictions\n",
        "        low_confident = (probas < self.hard_threshold_low) & (preds == 0)\n",
        "        high_confident = (probas > self.hard_threshold_high) & (preds == 1)\n",
        "        confident_mask = low_confident | high_confident\n",
        "\n",
        "        # Сохраняем confident predictions\n",
        "        for i, test_id in enumerate(ids):\n",
        "            if confident_mask[i]:\n",
        "                all_predictions[test_id] = preds[i]\n",
        "\n",
        "        # Оставляем только неуверенные для следующей стадии\n",
        "        remaining_ids = [ids[i] for i in range(len(confident_mask)) if not confident_mask[i]]\n",
        "        current_df = current_df.filter(pl.col('id').is_in(remaining_ids))\n",
        "\n",
        "        confident_count = np.sum(confident_mask)\n",
        "        print(f\"Confident predictions: {confident_count}, Remaining: {len(remaining_ids)}\")\n",
        "\n",
        "    # Финальные предсказания\n",
        "    test_ids = test_df['id'].to_list()\n",
        "    final_predictions = [all_predictions.get(test_id, 0) for test_id in test_ids]\n",
        "\n",
        "    # Статистика\n",
        "    coverage = len(all_predictions) / len(test_ids) * 100\n",
        "    unique_vals, counts = np.unique(final_predictions, return_counts=True)\n",
        "\n",
        "    print(f\"\\nСтатистика предсказаний:\")\n",
        "    print(f\"Coverage: {coverage:.1f}%\")\n",
        "    for val, count in zip(unique_vals, counts):\n",
        "        percentage = count / len(final_predictions) * 100\n",
        "        print(f\"Class {val}: {count} ({percentage:.1f}%)\")\n",
        "\n",
        "    print(\"✅ ПРЕДСКАЗАНИЕ ЗАВЕРШЕНО!\")\n",
        "    return np.array(final_predictions)\n",
        "\n",
        "OptimizedTextAwarePipeline.predict = predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "800d49f2-f4a1-48a5-b046-1e26817e2981",
      "metadata": {
        "id": "800d49f2-f4a1-48a5-b046-1e26817e2981"
      },
      "outputs": [],
      "source": [
        "## Запуск"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19450c05-07e9-4e6d-8bd1-12a2365cfd3a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19450c05-07e9-4e6d-8bd1-12a2365cfd3a",
        "outputId": "83cbf484-8fe6-4185-e3c8-8b91dcb25559"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔥 ЗАГРУЗКА ОПТИМИЗИРОВАННОГО PIPELINE\n",
            "📊 Загрузка тренировочных данных...\n",
            "Размер тренировочной выборки: 197198\n",
            "🚀 ЗАПУСК ОПТИМИЗИРОВАННОГО TEXT-AWARE PIPELINE\n",
            "================================================================================\n",
            "Найдено текстовых колонок: 4\n",
            "\n",
            "============================================================\n",
            "ОБУЧЕНИЕ СТАДИИ HARD\n",
            "============================================================\n",
            "Размер выборки: 197198\n",
            "Feature engineering для стадии hard...\n",
            "Continuous: 34, Categorical: 3\n",
            "Сгенерировано 135 новых признаков для hard\n",
            "Feature selection для hard (макс. 25 признаков)...\n"
          ]
        }
      ],
      "source": [
        "# Функция для запуска полного процесса\n",
        "def run_optimized_pipeline(train_path: str, test_path: str):\n",
        "\n",
        "    try:\n",
        "        print(\"🔥 ЗАГРУЗКА ОПТИМИЗИРОВАННОГО PIPELINE\")\n",
        "\n",
        "        # Создание и обучение пайплайна\n",
        "        pipeline = OptimizedTextAwarePipeline()\n",
        "\n",
        "        # Загрузка данных\n",
        "        print(\"📊 Загрузка тренировочных данных...\")\n",
        "        train_df = pl.read_csv(train_path)\n",
        "        print(f\"Размер тренировочной выборки: {len(train_df)}\")\n",
        "\n",
        "        # Обучение\n",
        "        pipeline.fit(train_df)\n",
        "\n",
        "        # Загрузка тестовых данных\n",
        "        print(\"🧪 Загрузка тестовых данных...\")\n",
        "\n",
        "        app_test = 'ml_ozon_сounterfeit_new_test.csv'\n",
        "        df_test_old = pl.read_csv(test_path)\n",
        "        df_test_append = pl.read_csv(app_test)\n",
        "\n",
        "        test_df = df_test_old.vstack(df_test_append)\n",
        "\n",
        "\n",
        "        print(f\"Размер тестовой выборки: {len(test_df)}\")\n",
        "\n",
        "        # Предсказание\n",
        "        predictions = pipeline.predict(test_df)\n",
        "\n",
        "        # Создание submission\n",
        "        submission = pd.DataFrame({\n",
        "            'id': test_df['id'].to_list(),\n",
        "            'prediction': predictions\n",
        "        })\n",
        "\n",
        "        submission.to_csv('optimized_submission.csv', index=False)\n",
        "        print(\"\\n✅ Результаты сохранены в optimized_submission.csv\")\n",
        "        print(\"🎉 PIPELINE ВЫПОЛНЕН УСПЕШНО!\")\n",
        "\n",
        "        return pipeline, predictions\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Ошибка выполнения: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None, None\n",
        "OptimizedTextAwarePipeline.run_optimized_pipeline = run_optimized_pipeline\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    train_path = '/content/drive/MyDrive/data/ml_ozon_сounterfeit_train.csv' # Укажите путь к тренировочным данным\n",
        "    test_path = '/content/drive/MyDrive/data/ml_ozon_сounterfeit_test.csv'    # Укажите путь к тестовым данным\n",
        "\n",
        "    pipeline, predictions = run_optimized_pipeline(train_path, test_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c78dc99e-4041-432a-8485-53b762900a59",
      "metadata": {
        "id": "c78dc99e-4041-432a-8485-53b762900a59"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6ad6288-eb10-42f7-9118-9723d994dc78",
      "metadata": {
        "id": "b6ad6288-eb10-42f7-9118-9723d994dc78"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e21aa7ef-2e36-4fcf-ab08-df8f9be139e8",
      "metadata": {
        "id": "e21aa7ef-2e36-4fcf-ab08-df8f9be139e8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b1382dd-e732-43c6-bae6-4627ba01c59c",
      "metadata": {
        "id": "4b1382dd-e732-43c6-bae6-4627ba01c59c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2e60af9-e3c9-4f7e-8071-bbdf3cecdbc9",
      "metadata": {
        "id": "d2e60af9-e3c9-4f7e-8071-bbdf3cecdbc9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5328ce7a-6f67-4d0c-9ed4-2baba63e956c",
      "metadata": {
        "id": "5328ce7a-6f67-4d0c-9ed4-2baba63e956c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64cb7e44-e131-4800-a5e7-1b5712fa358d",
      "metadata": {
        "id": "64cb7e44-e131-4800-a5e7-1b5712fa358d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c76a7bac-db2c-4157-a097-299c588e675d",
      "metadata": {
        "id": "c76a7bac-db2c-4157-a097-299c588e675d"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}