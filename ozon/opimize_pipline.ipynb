{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mihail-Chr/projects/blob/main/ozon/opimize_pipline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U pip\n",
        "#!pip install --use-pep517 suod\n",
        "!pip install \"polars>=1.25,<1.29\"\n",
        "!pip install catboost yake threadpoolctl\n",
        "!pip install polars-splitters\n"
      ],
      "metadata": {
        "id": "J3nY78nONVAX"
      },
      "id": "J3nY78nONVAX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ed6b663-b3ec-496c-ba50-195fb99185e4",
      "metadata": {
        "id": "0ed6b663-b3ec-496c-ba50-195fb99185e4"
      },
      "outputs": [],
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from catboost import CatBoostClassifier, Pool\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.feature_selection import mutual_info_classif, SelectKBest\n",
        "from sklearn.metrics import f1_score, confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import gc\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "try:\n",
        "    import cudf\n",
        "    import cuml\n",
        "    GPU_AVAILABLE = True\n",
        "except ImportError:\n",
        "    GPU_AVAILABLE = False\n",
        "\n",
        "RANDOM_STATE = 255\n",
        "np.random.seed(RANDOM_STATE)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25b23fc8-260d-4e31-bdc7-a3a1e6038e7c",
      "metadata": {
        "id": "25b23fc8-260d-4e31-bdc7-a3a1e6038e7c"
      },
      "source": [
        "## –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3e0c9ac-18ef-4d44-9d07-53200292bdf9",
      "metadata": {
        "id": "b3e0c9ac-18ef-4d44-9d07-53200292bdf9"
      },
      "outputs": [],
      "source": [
        "class OptimizedTextAwarePipeline:\n",
        "\n",
        "    def __init__(self, target='resolution'):\n",
        "        self.target = target\n",
        "        self.random_state = RANDOM_STATE\n",
        "        self.col_drop = ['id', 'ItemID', 'SellerID']\n",
        "        self.hard_threshold_low = 0.2\n",
        "        self.hard_threshold_high = 0.8\n",
        "\n",
        "        self.text_columns = []\n",
        "        self.models = {}\n",
        "        self.features = {}\n",
        "        self.label_encoders = {}  # –î–ª—è –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "\n",
        "        # GPU detection\n",
        "        self.gpu_available = GPU_AVAILABLE\n",
        "        self.device_type = \"GPU\" if self.gpu_available else \"CPU\"\n",
        "\n",
        "        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è CatBoost\n",
        "        self.text_processing_params = {\n",
        "            'tokenizers': [{'tokenizer_id': 'Space', 'separator_type': 'ByDelimiter', 'delimiter': ' '}],\n",
        "            'dictionaries': [{'dictionary_id': 'Word', 'max_dictionary_size': 50000}],\n",
        "            'feature_calcers': [{'calcer_id': 'BoW', 'top_tokens_count': 1000}]\n",
        "        }\n",
        "\n",
        "        # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã CatBoost\n",
        "        self.base_catboost_params = {\n",
        "            'random_seed': self.random_state,\n",
        "            'task_type': self.device_type,\n",
        "            'verbose': False,\n",
        "            'eval_metric': 'F1',\n",
        "            'loss_function': 'Logloss',\n",
        "            'use_best_model': True,\n",
        "            'early_stopping_rounds': 30,\n",
        "            'thread_count': -1,\n",
        "            'allow_writing_files': False,\n",
        "            'text_processing': self.text_processing_params\n",
        "        }\n",
        "\n",
        "        if self.gpu_available:\n",
        "            self.base_catboost_params.update({\n",
        "                'gpu_ram_part': 0.8,\n",
        "                'used_ram_limit': '8gb'\n",
        "            })\n",
        "\n",
        "    def identify_text_columns(self, df: pl.DataFrame):\n",
        "        \"\"\"–û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏\"\"\"\n",
        "        self.text_columns = [col for col, dtype in df.schema.items()\n",
        "                             if dtype == pl.String and col not in self.col_drop + [self.target]]\n",
        "        print(f\"–ù–∞–π–¥–µ–Ω–æ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫: {len(self.text_columns)}\")\n",
        "        return self.text_columns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9acb08e1-a340-466a-937e-313277a26039",
      "metadata": {
        "id": "9acb08e1-a340-466a-937e-313277a26039"
      },
      "source": [
        "## –ì–µ–Ω–µ—Ä–∞—Ü–∏—è continuous/categorical –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "601f851f-857e-4e66-82c8-55f86cf77844",
      "metadata": {
        "id": "601f851f-857e-4e66-82c8-55f86cf77844"
      },
      "outputs": [],
      "source": [
        "def advanced_feature_engineering(self, df: pl.DataFrame, stage: str) -> pl.DataFrame:\n",
        "\n",
        "    print(f\"Feature engineering –¥–ª—è —Å—Ç–∞–¥–∏–∏ {stage}...\")\n",
        "\n",
        "    # –ü–æ–ª—É—á–∞–µ–º —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏\n",
        "    numeric_cols = []\n",
        "    for col, dtype in df.schema.items():\n",
        "        if col not in self.col_drop + [self.target] + self.text_columns:\n",
        "            if dtype in [pl.Int8, pl.Int16, pl.Int32, pl.Int64, pl.Float32, pl.Float64]:\n",
        "                numeric_cols.append(col)\n",
        "\n",
        "    # –ü–†–ê–í–ò–õ–¨–ù–û: –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ continuous –∏ categorical\n",
        "    continuous_cols = []\n",
        "    categorical_cols = []\n",
        "\n",
        "    for col in numeric_cols:\n",
        "        if col in df.columns:\n",
        "            unique_count = df[col].n_unique()\n",
        "            if unique_count <= 50:  # –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ\n",
        "                categorical_cols.append(col)\n",
        "            else:  # –ù–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–µ\n",
        "                continuous_cols.append(col)\n",
        "\n",
        "    print(f\"Continuous: {len(continuous_cols)}, Categorical: {len(categorical_cols)}\")\n",
        "\n",
        "    expressions = []\n",
        "\n",
        "    # 1. –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¢–û–õ–¨–ö–û –¥–ª—è continuous –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "    for col in continuous_cols[:15]:\n",
        "        if col in df.columns:\n",
        "            expressions.extend([\n",
        "                pl.when(pl.col(col) > 0).then(pl.col(col).log()).otherwise(0).alias(f\"{col}_log\"),\n",
        "                pl.when(pl.col(col) >= 0).then(pl.col(col).sqrt()).otherwise(0).alias(f\"{col}_sqrt\"),\n",
        "                (pl.col(col) ** 2).alias(f\"{col}_sq\"),\n",
        "                pl.when(pl.col(col) != 0).then(1.0 / pl.col(col)).otherwise(0).alias(f\"{col}_inv\")\n",
        "            ])\n",
        "\n",
        "    # 2. –ü–∞—Ä–Ω—ã–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –¥–ª—è continuous\n",
        "    if len(continuous_cols) >= 2:\n",
        "        top_continuous = continuous_cols[:6]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
        "        for i in range(len(top_continuous)):\n",
        "            for j in range(i+1, len(top_continuous)):\n",
        "                col1, col2 = top_continuous[i], top_continuous[j]\n",
        "                if col1 in df.columns and col2 in df.columns:\n",
        "                    expressions.extend([\n",
        "                        pl.when(pl.col(col2) != 0).then(pl.col(col1) / pl.col(col2)).otherwise(0).alias(f\"{col1}_{col2}_ratio\"),\n",
        "                        (pl.col(col1) * pl.col(col2)).alias(f\"{col1}_{col2}_prod\"),\n",
        "                        (pl.col(col1) - pl.col(col2)).alias(f\"{col1}_{col2}_diff\")\n",
        "                    ])\n",
        "\n",
        "    # 3. Percentile-based features –¥–ª—è continuous\n",
        "    for col in continuous_cols[:10]:\n",
        "        if col in df.columns:\n",
        "            q75 = df[col].quantile(0.75)\n",
        "            q25 = df[col].quantile(0.25)\n",
        "            median = df[col].median()\n",
        "\n",
        "            expressions.extend([\n",
        "                (pl.col(col) > q75).cast(pl.Int8).alias(f\"{col}_q75_flag\"),\n",
        "                (pl.col(col) < q25).cast(pl.Int8).alias(f\"{col}_q25_flag\"),\n",
        "                (pl.col(col) > median).cast(pl.Int8).alias(f\"{col}_median_flag\")\n",
        "            ])\n",
        "\n",
        "    # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –±–∞—Ç—á–∞–º–∏\n",
        "    if expressions:\n",
        "        batch_size = 50\n",
        "        for i in range(0, len(expressions), batch_size):\n",
        "            batch = expressions[i:i+batch_size]\n",
        "            try:\n",
        "                df = df.with_columns(batch)\n",
        "            except Exception as e:\n",
        "                print(f\"–û—à–∏–±–∫–∞ –≤ batch {i}: {e}\")\n",
        "                continue\n",
        "\n",
        "    print(f\"–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(expressions)} –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è {stage}\")\n",
        "    return df\n",
        "\n",
        "OptimizedTextAwarePipeline.advanced_feature_engineering = advanced_feature_engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1c9b4d5-2f95-49c8-830e-46ef67f029d7",
      "metadata": {
        "id": "b1c9b4d5-2f95-49c8-830e-46ef67f029d7"
      },
      "source": [
        "## –£–ª—É—á—à–µ–Ω–Ω—ã–π –æ—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1daf21b5-2ef7-4620-a46c-6cdd5ffce058",
      "metadata": {
        "id": "1daf21b5-2ef7-4620-a46c-6cdd5ffce058"
      },
      "outputs": [],
      "source": [
        "def improved_feature_selection(self, df_pandas: pd.DataFrame, stage: str, max_features: int = 30) -> list:\n",
        "\n",
        "    print(f\"Feature selection –¥–ª—è {stage} (–º–∞–∫—Å. {max_features} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)...\")\n",
        "\n",
        "    # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∫—Ä–æ–º–µ —Å–ª—É–∂–µ–±–Ω—ã—Ö\n",
        "    feature_cols = [col for col in df_pandas.columns\n",
        "                   if col not in self.col_drop + [self.target]]\n",
        "\n",
        "    if len(feature_cols) < 2:\n",
        "        return feature_cols\n",
        "\n",
        "    # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ —á–∏—Å–ª–æ–≤—ã–µ –∏ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ\n",
        "    numeric_features = []\n",
        "    text_features = []\n",
        "\n",
        "    for col in feature_cols:\n",
        "        if col in self.text_columns:\n",
        "            text_features.append(col)\n",
        "        else:\n",
        "            try:\n",
        "                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∫–æ–ª–æ–Ω–∫–∞ —á–∏—Å–ª–æ–≤–æ–π\n",
        "                pd.to_numeric(df_pandas[col], errors='raise')\n",
        "                numeric_features.append(col)\n",
        "            except (ValueError, TypeError):\n",
        "                # –ï—Å–ª–∏ –Ω–µ —á–∏—Å–ª–æ–≤–∞—è, –∫–æ–¥–∏—Ä—É–µ–º –∫–∞–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—É—é\n",
        "                if col not in self.label_encoders:\n",
        "                    self.label_encoders[col] = LabelEncoder()\n",
        "                    df_pandas[col] = self.label_encoders[col].fit_transform(df_pandas[col].fillna('missing'))\n",
        "                numeric_features.append(col)\n",
        "\n",
        "    selected_features = []\n",
        "\n",
        "    # Mutual information –¥–ª—è —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "    if numeric_features and len(numeric_features) > 0:\n",
        "        X_numeric = df_pandas[numeric_features].fillna(0)\n",
        "        y = df_pandas[self.target]\n",
        "\n",
        "        try:\n",
        "            mi_selector = SelectKBest(mutual_info_classif, k=min(max_features-5, len(numeric_features)))\n",
        "            mi_selector.fit(X_numeric, y)\n",
        "            selected_numeric = [numeric_features[i] for i in mi_selector.get_support(indices=True)]\n",
        "            selected_features.extend(selected_numeric)\n",
        "        except Exception as e:\n",
        "            print(f\"MI selection failed: {e}, using correlation\")\n",
        "            # Fallback –Ω–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é\n",
        "            corr_with_target = X_numeric.corrwith(y).abs().sort_values(ascending=False)\n",
        "            selected_features.extend(corr_with_target.head(max_features-5).index.tolist())\n",
        "\n",
        "    # –î–æ–±–∞–≤–ª—è–µ–º –≤–∞–∂–Ω—ã–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
        "    selected_features.extend(text_features[:5])\n",
        "\n",
        "    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ max_features\n",
        "    selected_features = selected_features[:max_features]\n",
        "\n",
        "    print(f\"–í—ã–±—Ä–∞–Ω–æ {len(selected_features)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\")\n",
        "    return selected_features\n",
        "\n",
        "OptimizedTextAwarePipeline.improved_feature_selection = improved_feature_selection"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f4effe2-8953-4ae2-95b3-e98576d60372",
      "metadata": {
        "id": "3f4effe2-8953-4ae2-95b3-e98576d60372"
      },
      "source": [
        "## –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å—Ç–∞–¥–∏–∏"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd548d9d-6f1b-449d-b5b0-801de768225c",
      "metadata": {
        "id": "cd548d9d-6f1b-449d-b5b0-801de768225c"
      },
      "outputs": [],
      "source": [
        "def visualize_stage_results(self, model, X, y, features, stage_name):\n",
        "\n",
        "    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
        "    preds = model.predict(X)\n",
        "    probas = model.predict_proba(X)[:, 1]\n",
        "\n",
        "    # –°–æ–∑–¥–∞–µ–º —Ñ–∏–≥—É—Ä—É —Å –ø–æ–¥–≥—Ä–∞—Ñ–∏–∫–∞–º–∏\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    fig.suptitle(f'–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –º–æ–¥–µ–ª–∏ {stage_name}', fontsize=16, fontweight='bold')\n",
        "\n",
        "    # 1. Confusion Matrix\n",
        "    cm = confusion_matrix(y, preds)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0,0])\n",
        "    axes[0,0].set_title('Confusion Matrix')\n",
        "    axes[0,0].set_xlabel('Predicted')\n",
        "    axes[0,0].set_ylabel('Actual')\n",
        "\n",
        "    # 2. Feature Importance\n",
        "    if hasattr(model, 'get_feature_importance'):\n",
        "        importances = model.get_feature_importance()\n",
        "        feat_imp = pd.DataFrame({\n",
        "            'feature': features,\n",
        "            'importance': importances\n",
        "        }).sort_values('importance', ascending=True).tail(10)\n",
        "\n",
        "        axes[0,1].barh(feat_imp['feature'], feat_imp['importance'])\n",
        "        axes[0,1].set_title('Top 10 Feature Importance')\n",
        "        axes[0,1].set_xlabel('Importance')\n",
        "\n",
        "    # 3. Probability Distribution\n",
        "    axes[1,0].hist(probas[y==0], bins=50, alpha=0.7, label='Class 0', color='red')\n",
        "    axes[1,0].hist(probas[y==1], bins=50, alpha=0.7, label='Class 1', color='green')\n",
        "    axes[1,0].axvline(self.hard_threshold_low, color='blue', linestyle='--', label=f'Low threshold ({self.hard_threshold_low})')\n",
        "    axes[1,0].axvline(self.hard_threshold_high, color='blue', linestyle='--', label=f'High threshold ({self.hard_threshold_high})')\n",
        "    axes[1,0].set_xlabel('Predicted Probability')\n",
        "    axes[1,0].set_ylabel('Count')\n",
        "    axes[1,0].set_title('Probability Distribution')\n",
        "    axes[1,0].legend()\n",
        "\n",
        "    # 4. Classification Report (—Ç–µ–∫—Å—Ç)\n",
        "    cr = classification_report(y, preds, output_dict=True)\n",
        "    report_text = f\"\"\"Classification Report:\n",
        "\n",
        "            Precision (Class 0): {cr['0']['precision']:.3f}\n",
        "            Recall (Class 0): {cr['0']['recall']:.3f}\n",
        "            F1-Score (Class 0): {cr['0']['f1-score']:.3f}\n",
        "\n",
        "            Precision (Class 1): {cr['1']['precision']:.3f}\n",
        "            Recall (Class 1): {cr['1']['recall']:.3f}\n",
        "            F1-Score (Class 1): {cr['1']['f1-score']:.3f}\n",
        "\n",
        "            Overall F1-Score: {cr['macro avg']['f1-score']:.3f}\n",
        "            Accuracy: {cr['accuracy']:.3f}\"\"\"\n",
        "\n",
        "    axes[1,1].text(0.1, 0.5, report_text, fontsize=10, verticalalignment='center',\n",
        "                  transform=axes[1,1].transAxes, family='monospace')\n",
        "    axes[1,1].set_title('Performance Metrics')\n",
        "    axes[1,1].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "OptimizedTextAwarePipeline.visualize_stage_results = visualize_stage_results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73d637c9-a914-4ba8-880e-354a104a2dbe",
      "metadata": {
        "id": "73d637c9-a914-4ba8-880e-354a104a2dbe"
      },
      "source": [
        "## –ó–∞–ø–∏—Å—å —É–≤–µ—Ä–µ–Ω–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π —Å –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–µ–π"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20342bc4-9cfa-42b0-a745-2fa3e672b4e0",
      "metadata": {
        "id": "20342bc4-9cfa-42b0-a745-2fa3e672b4e0"
      },
      "outputs": [],
      "source": [
        "def filter_confident_predictions(self, model, X, y, df_original, stage_name):\n",
        "    \"\"\"\"\"\"\n",
        "    print(f\"–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è confident predictions –¥–ª—è {stage_name}...\")\n",
        "\n",
        "    probas = model.predict_proba(X)[:, 1]\n",
        "    preds = model.predict(X)\n",
        "\n",
        "    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º confident –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
        "    low_confident = (probas < self.hard_threshold_low) & (preds == 0) & (y == 0)\n",
        "    high_confident = (probas > self.hard_threshold_high) & (preds == 1) & (y == 1)\n",
        "    confident_mask = low_confident | high_confident\n",
        "\n",
        "    # –ü–æ–ª—É—á–∞–µ–º ID –∏–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞\n",
        "    all_ids = df_original['id'].to_list()\n",
        "\n",
        "    # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ confident –∏ remaining\n",
        "    confident_ids = [all_ids[i] for i in range(len(confident_mask)) if confident_mask[i]]\n",
        "    remaining_ids = [all_ids[i] for i in range(len(confident_mask)) if not confident_mask[i]]\n",
        "\n",
        "    df_confident = df_original.filter(pl.col('id').is_in(confident_ids))\n",
        "    df_remaining = df_original.filter(pl.col('id').is_in(remaining_ids))\n",
        "\n",
        "    print(f\"Confident: {len(confident_ids)}, Remaining: {len(remaining_ids)}\")\n",
        "\n",
        "    return df_confident, df_remaining\n",
        "\n",
        "OptimizedTextAwarePipeline.filter_confident_predictions = filter_confident_predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f4ebe10-8bb6-4b4f-8639-26afd7a241e4",
      "metadata": {
        "id": "3f4ebe10-8bb6-4b4f-8639-26afd7a241e4"
      },
      "source": [
        "## –û–±—É—á–µ–Ω–∏–µ —Å—Ç–∞–¥–∏–π —Å –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–µ–π"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aae71fad-3f0b-47b4-a0cc-67312e596857",
      "metadata": {
        "id": "aae71fad-3f0b-47b4-a0cc-67312e596857"
      },
      "outputs": [],
      "source": [
        "def train_stage(self, df: pl.DataFrame, stage_name: str, max_features: int):\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"–û–ë–£–ß–ï–ù–ò–ï –°–¢–ê–î–ò–ò {stage_name.upper()}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"–†–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏: {len(df)}\")\n",
        "\n",
        "    # Feature engineering\n",
        "    df_processed = self.advanced_feature_engineering(df, stage_name)\n",
        "    df_pandas = df_processed.to_pandas()\n",
        "\n",
        "    # Feature selection\n",
        "    features = self.improved_feature_selection(df_pandas, stage_name, max_features)\n",
        "\n",
        "    if len(features) == 0:\n",
        "        print(f\"–ù–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è {stage_name}!\")\n",
        "        return None, df_pandas, features\n",
        "\n",
        "    X = df_pandas[features].copy()\n",
        "    y = df_pandas[self.target].copy()\n",
        "\n",
        "    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "    for col in features:\n",
        "        if col in self.text_columns:\n",
        "            X[col] = X[col].fillna('missing').astype(str)\n",
        "\n",
        "    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
        "    cat_features_idx = []\n",
        "    text_features_idx = []\n",
        "\n",
        "    for i, col in enumerate(features):\n",
        "        if col in self.text_columns:\n",
        "            text_features_idx.append(i)\n",
        "        elif X[col].dtype == 'object' or (X[col].nunique() <= 50 and X[col].dtype in ['int64', 'int32']):\n",
        "            cat_features_idx.append(i)\n",
        "\n",
        "    print(f\"–í—Å–µ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(features)}\")\n",
        "    print(f\"–ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö: {len(cat_features_idx)}\")\n",
        "    print(f\"–¢–µ–∫—Å—Ç–æ–≤—ã—Ö: {len(text_features_idx)}\")\n",
        "\n",
        "    # –°–æ–∑–¥–∞–µ–º Pool\n",
        "    train_pool = Pool(X, y, cat_features=cat_features_idx, text_features=text_features_idx)\n",
        "\n",
        "    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–¥–µ–ª–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Å—Ç–∞–¥–∏–π\n",
        "    stage_params = self.base_catboost_params.copy()\n",
        "    if stage_name == 'hard':\n",
        "        stage_params.update({'iterations': 400, 'depth': 6, 'learning_rate': 0.1})\n",
        "    elif stage_name == 'soft':\n",
        "        stage_params.update({'iterations': 600, 'depth': 8, 'learning_rate': 0.08})\n",
        "    else:  # error\n",
        "        stage_params.update({'iterations': 800, 'depth': 10, 'learning_rate': 0.05})\n",
        "\n",
        "    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
        "    model = CatBoostClassifier(**stage_params)\n",
        "    model.fit(train_pool, verbose=False)\n",
        "\n",
        "    # Cross-validation score\n",
        "    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=255)\n",
        "    cv_scores = []\n",
        "\n",
        "    for train_idx, val_idx in skf.split(X, y):\n",
        "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "        temp_pool = Pool(X_train, y_train, cat_features=cat_features_idx, text_features=text_features_idx)\n",
        "        temp_model = CatBoostClassifier(**stage_params)\n",
        "        temp_model.fit(temp_pool, verbose=False)\n",
        "\n",
        "        preds = temp_model.predict(X_val)\n",
        "        f1 = f1_score(y_val, preds)\n",
        "        cv_scores.append(f1)\n",
        "\n",
        "    cv_f1 = np.mean(cv_scores)\n",
        "    print(f\"CV F1-score: {cv_f1:.4f} ¬± {np.std(cv_scores):.4f}\")\n",
        "\n",
        "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –∏ –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
        "    self.models[stage_name] = model\n",
        "    self.features[stage_name] = features\n",
        "\n",
        "    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
        "    self.visualize_stage_results(model, X, y, features, stage_name)\n",
        "\n",
        "    return model, df_pandas, features\n",
        "\n",
        "OptimizedTextAwarePipeline.train_stage = train_stage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24812b38-d000-4b26-ac51-a9e66c2047f1",
      "metadata": {
        "id": "24812b38-d000-4b26-ac51-a9e66c2047f1"
      },
      "outputs": [],
      "source": [
        "## –û–±—â–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞–π–ø–ª–∞–π–Ω–∞"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d462992-44ea-46e2-af6e-0ba48e60ede7",
      "metadata": {
        "id": "5d462992-44ea-46e2-af6e-0ba48e60ede7"
      },
      "outputs": [],
      "source": [
        "def visualize_pipeline_overview(self, stage_stats):\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    fig.suptitle('–û–±–∑–æ—Ä –ø–∞–π–ø–ª–∞–π–Ω–∞ Text-Aware Classification', fontsize=16, fontweight='bold')\n",
        "\n",
        "    # 1. –†–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏ –ø–æ —Å—Ç–∞–¥–∏—è–º\n",
        "    stages = list(stage_stats.keys())\n",
        "    sample_sizes = [stage_stats[stage]['sample_size'] for stage in stages]\n",
        "\n",
        "    axes[0,0].bar(stages, sample_sizes, color=['#ff7f0e', '#2ca02c', '#d62728'])\n",
        "    axes[0,0].set_title('–†–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏ –ø–æ —Å—Ç–∞–¥–∏—è–º')\n",
        "    axes[0,0].set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–∑—Ü–æ–≤')\n",
        "    for i, v in enumerate(sample_sizes):\n",
        "        axes[0,0].text(i, v + max(sample_sizes)*0.01, str(v), ha='center', va='bottom')\n",
        "\n",
        "    # 2. F1-score –ø–æ —Å—Ç–∞–¥–∏—è–º\n",
        "    f1_scores = [stage_stats[stage]['f1_score'] for stage in stages if 'f1_score' in stage_stats[stage]]\n",
        "    if f1_scores:\n",
        "        axes[0,1].bar(stages[:len(f1_scores)], f1_scores, color=['#ff7f0e', '#2ca02c', '#d62728'])\n",
        "        axes[0,1].set_title('F1-Score –ø–æ —Å—Ç–∞–¥–∏—è–º')\n",
        "        axes[0,1].set_ylabel('F1-Score')\n",
        "        axes[0,1].set_ylim(0, 1)\n",
        "        for i, v in enumerate(f1_scores):\n",
        "            axes[0,1].text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')\n",
        "\n",
        "    # 3. –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ —Å—Ç–∞–¥–∏—è–º\n",
        "    feature_counts = [stage_stats[stage]['feature_count'] for stage in stages if 'feature_count' in stage_stats[stage]]\n",
        "    if feature_counts:\n",
        "        axes[1,0].bar(stages[:len(feature_counts)], feature_counts, color=['#ff7f0e', '#2ca02c', '#d62728'])\n",
        "        axes[1,0].set_title('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ —Å—Ç–∞–¥–∏—è–º')\n",
        "        axes[1,0].set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤')\n",
        "        for i, v in enumerate(feature_counts):\n",
        "            axes[1,0].text(i, v + max(feature_counts)*0.01, str(v), ha='center', va='bottom')\n",
        "\n",
        "    # 4. –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n",
        "    total_samples = sum(sample_sizes)\n",
        "    processing_reduction = (sample_sizes[0] - sample_sizes[-1]) / sample_sizes[0] * 100 if len(sample_sizes) > 1 else 0\n",
        "\n",
        "    stats_text = f\"\"\"–û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–∞–π–ø–ª–∞–π–Ω–∞:\n",
        "\n",
        "        –í—Å–µ–≥–æ –æ–±—Ä–∞–∑—Ü–æ–≤ –≤ –Ω–∞—á–∞–ª–µ: {sample_sizes[0]:,}\n",
        "        –û–±—Ä–∞–∑—Ü–æ–≤ –Ω–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–π —Å—Ç–∞–¥–∏–∏: {sample_sizes[-1]:,}\n",
        "        –°–æ–∫—Ä–∞—â–µ–Ω–∏–µ –æ–±—ä–µ–º–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {processing_reduction:.1f}%\n",
        "\n",
        "        –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç–∞–¥–∏–π: {len(stages)}\n",
        "        –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU: {'–î–∞' if self.gpu_available else '–ù–µ—Ç'}\n",
        "        –¢–µ–∫—Å—Ç–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(self.text_columns)}\n",
        "\n",
        "        –°—Ç—Ä–∞—Ç–µ–≥–∏—è: –ü–æ—ç—Ç–∞–ø–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è\n",
        "        confident predictions —Å —É–º–µ–Ω—å—à–µ–Ω–∏–µ–º\n",
        "        —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∑–∞–¥–∞—á–∏ –Ω–∞ –∫–∞–∂–¥–æ–π —Å—Ç–∞–¥–∏–∏\"\"\"\n",
        "\n",
        "    axes[1,1].text(0.05, 0.5, stats_text, fontsize=10, verticalalignment='center',\n",
        "                  transform=axes[1,1].transAxes, family='monospace')\n",
        "    axes[1,1].set_title('–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–∞–π–ø–ª–∞–π–Ω–∞')\n",
        "    axes[1,1].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "OptimizedTextAwarePipeline.visualize_pipeline_overview = visualize_pipeline_overview"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbcc700f-8167-4099-99f4-e2745770fcee",
      "metadata": {
        "id": "cbcc700f-8167-4099-99f4-e2745770fcee"
      },
      "source": [
        "## –ü–æ–ª–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bac3c3a-23ad-417e-96fd-51ba2b13741f",
      "metadata": {
        "id": "2bac3c3a-23ad-417e-96fd-51ba2b13741f"
      },
      "outputs": [],
      "source": [
        "def fit(self, train_df: pl.DataFrame):\n",
        "\n",
        "    print(\"üöÄ –ó–ê–ü–£–°–ö –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–û–ì–û TEXT-AWARE PIPELINE\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏\n",
        "    self.identify_text_columns(train_df)\n",
        "\n",
        "    # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏\n",
        "    expressions = []\n",
        "    for col, dtype in train_df.schema.items():\n",
        "        if col not in self.col_drop + [self.target]:\n",
        "            if dtype in [pl.Int8, pl.Int16, pl.Int32, pl.Int64, pl.Float32, pl.Float64]:\n",
        "                expressions.append(pl.col(col).fill_null(0).alias(col))\n",
        "            elif dtype == pl.String:\n",
        "                expressions.append(pl.col(col).fill_null(\"missing\").alias(col))\n",
        "\n",
        "    if expressions:\n",
        "        train_df = train_df.with_columns(expressions)\n",
        "\n",
        "    current_df = train_df\n",
        "    stages = [('hard', 25), ('soft', 35), ('error', 50)]\n",
        "    stage_stats = {}\n",
        "\n",
        "    for stage_name, max_features in stages:\n",
        "        stage_stats[stage_name] = {'sample_size': len(current_df)}\n",
        "\n",
        "        if len(current_df) < 50:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\n",
        "            print(f\"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å—Ç–∞–¥–∏–∏ {stage_name}: {len(current_df)}\")\n",
        "            break\n",
        "\n",
        "        # –û–±—É—á–µ–Ω–∏–µ —Å—Ç–∞–¥–∏–∏\n",
        "        model, df_pandas, features = self.train_stage(current_df, stage_name, max_features)\n",
        "\n",
        "        if model is None:\n",
        "            break\n",
        "\n",
        "        stage_stats[stage_name].update({\n",
        "            'feature_count': len(features),\n",
        "            'f1_score': np.mean([f1_score(df_pandas[self.target], model.predict(df_pandas[features])) for _ in [1]])\n",
        "        })\n",
        "\n",
        "        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è confident predictions\n",
        "        X = df_pandas[features].copy()\n",
        "        y = df_pandas[self.target].copy()\n",
        "\n",
        "        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏\n",
        "        for col in features:\n",
        "            if col in self.text_columns:\n",
        "                X[col] = X[col].fillna('missing').astype(str)\n",
        "\n",
        "        confident, remaining = self.filter_confident_predictions(model, X, y, current_df, stage_name)\n",
        "\n",
        "        current_df = remaining\n",
        "        gc.collect()\n",
        "\n",
        "        if len(current_df) == 0:\n",
        "            print(\"–í—Å–µ –æ–±—Ä–∞–∑—Ü—ã –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã —É–≤–µ—Ä–µ–Ω–Ω—ã–º–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏!\")\n",
        "            break\n",
        "\n",
        "    # –û–±—â–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\n",
        "    self.visualize_pipeline_overview(stage_stats)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"‚úÖ –û–ë–£–ß–ï–ù–ò–ï –ü–ê–ô–ü–õ–ê–ô–ù–ê –ó–ê–í–ï–†–®–ï–ù–û!\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    return self\n",
        "\n",
        "OptimizedTextAwarePipeline.fit = fit"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7dbad1bd-871e-4077-80e9-afdd71153e6e",
      "metadata": {
        "id": "7dbad1bd-871e-4077-80e9-afdd71153e6e"
      },
      "source": [
        "## –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –ø–æ—ç—Ç–∞–ø–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a7e9f37-7129-4eeb-b2cb-abaab9680e22",
      "metadata": {
        "id": "5a7e9f37-7129-4eeb-b2cb-abaab9680e22"
      },
      "outputs": [],
      "source": [
        "def predict(self, test_df: pl.DataFrame):\n",
        "\n",
        "    print(\"üîÆ –ù–ê–ß–ê–õ–û –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–Ø\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏\n",
        "    self.identify_text_columns(test_df)\n",
        "\n",
        "    # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏\n",
        "    expressions = []\n",
        "    for col, dtype in test_df.schema.items():\n",
        "        if col not in self.col_drop:\n",
        "            if dtype in [pl.Int8, pl.Int16, pl.Int32, pl.Int64, pl.Float32, pl.Float64]:\n",
        "                expressions.append(pl.col(col).fill_null(0).alias(col))\n",
        "            elif dtype == pl.String:\n",
        "                expressions.append(pl.col(col).fill_null(\"missing\").alias(col))\n",
        "\n",
        "    if expressions:\n",
        "        test_df = test_df.with_columns(expressions)\n",
        "\n",
        "    current_df = test_df\n",
        "    all_predictions = {}\n",
        "\n",
        "    for stage_name in ['hard', 'soft', 'error']:\n",
        "        if stage_name not in self.models or len(current_df) == 0:\n",
        "            continue\n",
        "\n",
        "        print(f\"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è —Å—Ç–∞–¥–∏–∏ {stage_name}: {len(current_df)} –æ–±—Ä–∞–∑—Ü–æ–≤\")\n",
        "\n",
        "        model = self.models[stage_name]\n",
        "        features = self.features[stage_name]\n",
        "\n",
        "        # Feature engineering\n",
        "        df_processed = self.advanced_feature_engineering(current_df, stage_name)\n",
        "        df_pandas = df_processed.to_pandas()\n",
        "\n",
        "        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "        available_features = [f for f in features if f in df_pandas.columns]\n",
        "        if len(available_features) == 0:\n",
        "            print(f\"–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è {stage_name}\")\n",
        "            continue\n",
        "\n",
        "        X_test = df_pandas[available_features].copy()\n",
        "\n",
        "        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "        for col in available_features:\n",
        "            if col in self.text_columns:\n",
        "                X_test[col] = X_test[col].fillna('missing').astype(str)\n",
        "            elif col in self.label_encoders:\n",
        "                # –ü—Ä–∏–º–µ–Ω—è–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–π encoder\n",
        "                X_test[col] = self.label_encoders[col].transform(X_test[col].fillna('missing'))\n",
        "\n",
        "        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
        "        probas = model.predict_proba(X_test)[:, 1]\n",
        "        preds = model.predict(X_test)\n",
        "\n",
        "        ids = current_df['id'].to_list()\n",
        "\n",
        "        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º confident predictions\n",
        "        low_confident = (probas < self.hard_threshold_low) & (preds == 0)\n",
        "        high_confident = (probas > self.hard_threshold_high) & (preds == 1)\n",
        "        confident_mask = low_confident | high_confident\n",
        "\n",
        "        # –°–æ—Ö—Ä–∞–Ω—è–µ–º confident predictions\n",
        "        for i, test_id in enumerate(ids):\n",
        "            if confident_mask[i]:\n",
        "                all_predictions[test_id] = preds[i]\n",
        "\n",
        "        # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –Ω–µ—É–≤–µ—Ä–µ–Ω–Ω—ã–µ –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–π —Å—Ç–∞–¥–∏–∏\n",
        "        remaining_ids = [ids[i] for i in range(len(confident_mask)) if not confident_mask[i]]\n",
        "        current_df = current_df.filter(pl.col('id').is_in(remaining_ids))\n",
        "\n",
        "        confident_count = np.sum(confident_mask)\n",
        "        print(f\"Confident predictions: {confident_count}, Remaining: {len(remaining_ids)}\")\n",
        "\n",
        "    # –§–∏–Ω–∞–ª—å–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
        "    test_ids = test_df['id'].to_list()\n",
        "    final_predictions = [all_predictions.get(test_id, 0) for test_id in test_ids]\n",
        "\n",
        "    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n",
        "    coverage = len(all_predictions) / len(test_ids) * 100\n",
        "    unique_vals, counts = np.unique(final_predictions, return_counts=True)\n",
        "\n",
        "    print(f\"\\n–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π:\")\n",
        "    print(f\"Coverage: {coverage:.1f}%\")\n",
        "    for val, count in zip(unique_vals, counts):\n",
        "        percentage = count / len(final_predictions) * 100\n",
        "        print(f\"Class {val}: {count} ({percentage:.1f}%)\")\n",
        "\n",
        "    print(\"‚úÖ –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!\")\n",
        "    return np.array(final_predictions)\n",
        "\n",
        "OptimizedTextAwarePipeline.predict = predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "800d49f2-f4a1-48a5-b046-1e26817e2981",
      "metadata": {
        "id": "800d49f2-f4a1-48a5-b046-1e26817e2981"
      },
      "outputs": [],
      "source": [
        "## –ó–∞–ø—É—Å–∫"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19450c05-07e9-4e6d-8bd1-12a2365cfd3a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19450c05-07e9-4e6d-8bd1-12a2365cfd3a",
        "outputId": "83cbf484-8fe6-4185-e3c8-8b91dcb25559"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üî• –ó–ê–ì–†–£–ó–ö–ê –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–û–ì–û PIPELINE\n",
            "üìä –ó–∞–≥—Ä—É–∑–∫–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...\n",
            "–†–∞–∑–º–µ—Ä —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏: 197198\n",
            "üöÄ –ó–ê–ü–£–°–ö –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–û–ì–û TEXT-AWARE PIPELINE\n",
            "================================================================================\n",
            "–ù–∞–π–¥–µ–Ω–æ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫: 4\n",
            "\n",
            "============================================================\n",
            "–û–ë–£–ß–ï–ù–ò–ï –°–¢–ê–î–ò–ò HARD\n",
            "============================================================\n",
            "–†–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏: 197198\n",
            "Feature engineering –¥–ª—è —Å—Ç–∞–¥–∏–∏ hard...\n",
            "Continuous: 34, Categorical: 3\n",
            "–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ 135 –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è hard\n",
            "Feature selection –¥–ª—è hard (–º–∞–∫—Å. 25 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)...\n"
          ]
        }
      ],
      "source": [
        "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –ø–æ–ª–Ω–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞\n",
        "def run_optimized_pipeline(train_path: str, test_path: str):\n",
        "\n",
        "    try:\n",
        "        print(\"üî• –ó–ê–ì–†–£–ó–ö–ê –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–û–ì–û PIPELINE\")\n",
        "\n",
        "        # –°–æ–∑–¥–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ –ø–∞–π–ø–ª–∞–π–Ω–∞\n",
        "        pipeline = OptimizedTextAwarePipeline()\n",
        "\n",
        "        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
        "        print(\"üìä –ó–∞–≥—Ä—É–∑–∫–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...\")\n",
        "        train_df = pl.read_csv(train_path)\n",
        "        print(f\"–†–∞–∑–º–µ—Ä —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏: {len(train_df)}\")\n",
        "\n",
        "        # –û–±—É—á–µ–Ω–∏–µ\n",
        "        pipeline.fit(train_df)\n",
        "\n",
        "        # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
        "        print(\"üß™ –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö...\")\n",
        "\n",
        "        app_test = 'ml_ozon_—Åounterfeit_new_test.csv'\n",
        "        df_test_old = pl.read_csv(test_path)\n",
        "        df_test_append = pl.read_csv(app_test)\n",
        "\n",
        "        test_df = df_test_old.vstack(df_test_append)\n",
        "\n",
        "\n",
        "        print(f\"–†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏: {len(test_df)}\")\n",
        "\n",
        "        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ\n",
        "        predictions = pipeline.predict(test_df)\n",
        "\n",
        "        # –°–æ–∑–¥–∞–Ω–∏–µ submission\n",
        "        submission = pd.DataFrame({\n",
        "            'id': test_df['id'].to_list(),\n",
        "            'prediction': predictions\n",
        "        })\n",
        "\n",
        "        submission.to_csv('optimized_submission.csv', index=False)\n",
        "        print(\"\\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ optimized_submission.csv\")\n",
        "        print(\"üéâ PIPELINE –í–´–ü–û–õ–ù–ï–ù –£–°–ü–ï–®–ù–û!\")\n",
        "\n",
        "        return pipeline, predictions\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå –û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None, None\n",
        "OptimizedTextAwarePipeline.run_optimized_pipeline = run_optimized_pipeline\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    train_path = '/content/drive/MyDrive/data/ml_ozon_—Åounterfeit_train.csv' # –£–∫–∞–∂–∏—Ç–µ –ø—É—Ç—å –∫ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–º –¥–∞–Ω–Ω—ã–º\n",
        "    test_path = '/content/drive/MyDrive/data/ml_ozon_—Åounterfeit_test.csv'    # –£–∫–∞–∂–∏—Ç–µ –ø—É—Ç—å –∫ —Ç–µ—Å—Ç–æ–≤—ã–º –¥–∞–Ω–Ω—ã–º\n",
        "\n",
        "    pipeline, predictions = run_optimized_pipeline(train_path, test_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c78dc99e-4041-432a-8485-53b762900a59",
      "metadata": {
        "id": "c78dc99e-4041-432a-8485-53b762900a59"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6ad6288-eb10-42f7-9118-9723d994dc78",
      "metadata": {
        "id": "b6ad6288-eb10-42f7-9118-9723d994dc78"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e21aa7ef-2e36-4fcf-ab08-df8f9be139e8",
      "metadata": {
        "id": "e21aa7ef-2e36-4fcf-ab08-df8f9be139e8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b1382dd-e732-43c6-bae6-4627ba01c59c",
      "metadata": {
        "id": "4b1382dd-e732-43c6-bae6-4627ba01c59c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2e60af9-e3c9-4f7e-8071-bbdf3cecdbc9",
      "metadata": {
        "id": "d2e60af9-e3c9-4f7e-8071-bbdf3cecdbc9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5328ce7a-6f67-4d0c-9ed4-2baba63e956c",
      "metadata": {
        "id": "5328ce7a-6f67-4d0c-9ed4-2baba63e956c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64cb7e44-e131-4800-a5e7-1b5712fa358d",
      "metadata": {
        "id": "64cb7e44-e131-4800-a5e7-1b5712fa358d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c76a7bac-db2c-4157-a097-299c588e675d",
      "metadata": {
        "id": "c76a7bac-db2c-4157-a097-299c588e675d"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}