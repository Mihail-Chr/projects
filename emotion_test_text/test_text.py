
"""
–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —É–ø–æ–º–∏–Ω–∞–Ω–∏–π –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ —Ä–µ–ø—É—Ç–∞—Ü–∏–∏ –±—Ä–µ–Ω–¥–æ–≤
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –æ—Ç–∫—Ä—ã—Ç—ã–µ NLP –º–æ–¥–µ–ª–∏ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏
"""

import re
import pandas as pd
from typing import List, Dict, Tuple, Optional
import numpy as np

# –î–ª—è —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ - —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å: pip install transformers torch sentencepiece
try:
    from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    print("–í–Ω–∏–º–∞–Ω–∏–µ: –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ transformers –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ: pip install transformers torch")

# –î–ª—è –±–∞–∑–æ–≤–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –∏ —Å—Ç–µ–º–º–∏–Ω–≥–∞ (–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞)
try:
    import nltk
    from nltk.corpus import stopwords
    from nltk.tokenize import word_tokenize
    NLTK_AVAILABLE = True
    # –°–∫–∞—á–∏–≤–∞–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Ä–µ—Å—É—Ä—Å—ã
    try:
        nltk.data.find('tokenizers/punkt')
        nltk.data.find('corpora/stopwords')
    except:
        print("–°–∫–∞—á–∏–≤–∞–µ–º —Ä–µ—Å—É—Ä—Å—ã NLTK...")
        nltk.download('punkt', quiet=True)
        nltk.download('stopwords', quiet=True)
except ImportError:
    NLTK_AVAILABLE = False
    print("–í–Ω–∏–º–∞–Ω–∏–µ: NLTK –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ: pip install nltk")

class MentionAnalyzer:
    """
    –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —É–ø–æ–º–∏–Ω–∞–Ω–∏–π –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ —Ä–µ–ø—É—Ç–∞—Ü–∏–∏
    """
    
    # –ü—Ä–µ–¥–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–µ —Ç–µ–≥–∏ (—É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ –¥–ª—è –ª—é–±–æ–≥–æ –û–ú)
    UNIVERSAL_TAGS = {
        'quality_issue': '–ü—Ä–æ–±–ª–µ–º—ã —Å –∫–∞—á–µ—Å—Ç–≤–æ–º',
        'positive_feedback': '–ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π –æ—Ç–∑—ã–≤',
        'negative_feedback': '–ù–µ–≥–∞—Ç–∏–≤–Ω—ã–π –æ—Ç–∑—ã–≤',
        'comparison': '–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞–º–∏',
        'question': '–í–æ–ø—Ä–æ—Å/—Å–ø—Ä–∞–≤–∫–∞',
        'complaint': '–ñ–∞–ª–æ–±–∞',
        'recommendation': '–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è/—Å–æ–≤–µ—Ç',
        'social_responsibility': '–°–æ—Ü–∏–∞–ª—å–Ω–∞—è –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å',
        'corporate_info': '–ö–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è',
        'health_issue': '–ü—Ä–æ–±–ª–µ–º—ã —Å–æ –∑–¥–æ—Ä–æ–≤—å–µ–º',
        'price_issue': '–í–æ–ø—Ä–æ—Å—ã —Ü–µ–Ω—ã',
        'service_issue': '–ü—Ä–æ–±–ª–µ–º—ã —Å —Å–µ—Ä–≤–∏—Å–æ–º',
        'product_info': '–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø—Ä–æ–¥—É–∫—Ç–µ',
        'advertisement': '–†–µ–∫–ª–∞–º–∞/–∞–∫—Ü–∏—è'
    }
    
    def __init__(self, 
                 object_name: str,
                 keywords: List[str],
                 risk_words: Optional[List[str]] = None,
                 positive_words: Optional[List[str]] = None,
                 exclude_phrases: Optional[List[str]] = None,
                 use_advanced_nlp: bool = True):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
        
        Args:
            object_name: –ù–∞–∑–≤–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
            keywords: –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            risk_words: –°–ª–æ–≤–∞, —É–∫–∞–∑—ã–≤–∞—é—â–∏–µ –Ω–∞ —Ä–∏—Å–∫ (–¥–ª—è –æ—Ü–µ–Ω–∫–∏ –æ–ø–∞—Å–Ω–æ—Å—Ç–∏)
            positive_words: –°–ª–æ–≤–∞, —É–∫–∞–∑—ã–≤–∞—é—â–∏–µ –Ω–∞ –ø–æ–∑–∏—Ç–∏–≤
            exclude_phrases: –§—Ä–∞–∑—ã –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è –ª–æ–∂–Ω—ã—Ö —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–π
            use_advanced_nlp: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ NLP –º–æ–¥–µ–ª–∏
        """
        self.object_name = object_name
        self.keywords = [kw.lower() for kw in keywords]
        self.risk_words = [rw.lower() for rw in (risk_words or [])]
        self.positive_words = [pw.lower() for pw in (positive_words or [])]
        self.exclude_phrases = [ep.lower() for ep in (exclude_phrases or [])]
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è NLP –º–æ–¥–µ–ª–µ–π
        self.sentiment_analyzer = None
        self.tokenizer = None
        self.use_advanced_nlp = use_advanced_nlp
        
        if use_advanced_nlp and TRANSFORMERS_AVAILABLE:
            self._init_nlp_models()
        
        # –°–ª–æ–≤–∞—Ä—å –¥–ª—è —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Ç–µ–≥–æ–≤
        self.tag_mapping = self._create_tag_mapping()
    
    def _init_nlp_models(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è NLP –º–æ–¥–µ–ª–µ–π"""
        try:
            # –ú–æ–¥–µ–ª—å –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ
            model_name = "blanchefort/rubert-base-cased-sentiment"
            self.sentiment_analyzer = pipeline(
                "sentiment-analysis",
                model=model_name,
                tokenizer=model_name,
                framework="pt"
            )
            print(f"–ú–æ–¥–µ–ª—å —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ {model_name} –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        except Exception as e:
            print(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏: {e}")
            self.use_advanced_nlp = False
    
    def _create_tag_mapping(self) -> Dict[str, List[str]]:
        """–°–æ–∑–¥–∞–Ω–∏–µ –º–∞–ø–ø–∏–Ω–≥–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è —Ç–µ–≥–æ–≤"""
        # –ë–∞–∑–æ–≤—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã—Ö —Ç–µ–≥–æ–≤
        mapping = {
            'quality_issue': [
                '–∫–∞—á–µ—Å—Ç–≤', '–Ω–µ–∫–∞—á–µ—Å—Ç–≤–µ–Ω', '–ø–ª–æ—Ö', '—É–∂–∞—Å–Ω', '–∫–æ—à–º–∞—Ä', '–±—Ä–∞–∫',
                '–∏—Å–ø–æ—Ä—á', '–≥—Ä—è–∑–Ω', '–≤—Ä–µ–¥–Ω', '–æ–ø–∞—Å–Ω'
            ],
            'positive_feedback': [
                '–Ω—Ä–∞–≤–∏—Ç—Å—è', '–ª—é–±–ª—é', '–æ–±–æ–∂–∞—é', '–æ—Ç–ª–∏—á–Ω', '–ø—Ä–µ–∫—Ä–∞—Å–Ω', '–∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω',
                '—Ö–æ—Ä–æ—à', '—Å—É–ø–µ—Ä', '–∫–ª–∞—Å—Å', '–ª—É—á—à'
            ],
            'negative_feedback': [
                '–Ω–µ –Ω—Ä–∞–≤–∏—Ç—Å—è', '–Ω–µ–Ω–∞–≤–∏–∂—É', '—É–∂–∞—Å–Ω', '–ø–ª–æ—Ö', '—Ä–∞–∑–æ—á–∞—Ä–æ–≤–∞–Ω',
                '–æ—Ç–≤—Ä–∞—Ç–∏—Ç–µ–ª—å–Ω', '–∫–æ—à–º–∞—Ä'
            ],
            'comparison': [
                '–ª—É—á—à–µ —á–µ–º', '—Ö—É–∂–µ —á–µ–º', '—Å—Ä–∞–≤–Ω–µ–Ω', '–≤ –æ—Ç–ª–∏—á–∏–µ –æ—Ç',
                '–ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å', '—á–µ–º'
            ],
            'question': [
                '–∫–∞–∫', '—á—Ç–æ', '–≥–¥–µ', '–∫–æ–≥–¥–∞', '–ø–æ—á–µ–º—É', '–∑–∞—á–µ–º', '—Å–∫–æ–ª—å–∫–æ',
                '?', '–ø–æ–¥—Å–∫–∞–∂–∏—Ç', '–ø–æ—Å–æ–≤–µ—Ç—É–π', '—Ä–∞—Å—Å–∫–∞–∂–∏—Ç'
            ],
            'complaint': [
                '–∂–∞–ª–æ–±', '–Ω–µ–¥–æ–≤–æ–ª', '–≤–æ–∑–º—É—â–µ–Ω', '–ø—Ä–æ—Ç–µ—Å—Ç', '–ø—Ä–µ—Ç–µ–Ω–∑',
                '—Ç—Ä–µ–±—É—é', '–≤–µ—Ä–Ω–∏—Ç–µ', '–≤–µ—Ä–Ω—É—Ç—å'
            ],
            'health_issue': [
                '–∑–¥–æ—Ä–æ–≤—å', '–±–æ–ª–µ–∑–Ω', '–±–æ–ª—å–Ω', '–∞–ª–ª–µ—Ä–≥', '–æ—Ç—Ä–∞–≤–ª–µ–Ω',
                '—Å—ã–ø—å', '—Ä–≤–æ—Ç–∞', '–∑–∞–ø–æ—Ä', '–∫–æ–ª–∏–∫', '–¥–∏–∞—Ä–µ—è'
            ],
            'price_issue': [
                '–¥–æ—Ä–æ–≥', '–¥–µ—à–µ–≤', '—Ü–µ–Ω', '—Å—Ç–æ–∏–º–æ—Å—Ç—å', '–ø–µ—Ä–µ–ø–ª–∞—Ç',
                '–Ω–∞—Ü–µ–Ω–∫', '—Å–∫–∏–¥–∫', '–∞–∫—Ü–∏', '—Ä–∞—Å–ø—Ä–æ–¥–∞–∂'
            ],
            'service_issue': [
                '–æ–±—Å–ª—É–∂', '—Å–µ—Ä–≤–∏—Å', '–ø–µ—Ä—Å–æ–Ω–∞–ª', '–∫–æ–Ω—Å—É–ª—å—Ç–∞', '–ø–æ–¥–¥–µ—Ä–∂–∫',
                '–º–∞—Å—Ç–µ—Ä', '—Ä–µ–º–æ–Ω—Ç', '–≥–∞—Ä–∞–Ω—Ç–∏'
            ]
        }
        return mapping
    
    def is_relevant(self, text: str) -> bool:
        """
        –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ —Å–æ–æ–±—â–µ–Ω–∏—è
        
        Args:
            text: –¢–µ–∫—Å—Ç —Å–æ–æ–±—â–µ–Ω–∏—è
            
        Returns:
            bool: True –µ—Å–ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ
        """
        text_lower = text.lower()
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∏—Å–∫–ª—é—á–∞—é—â–∏–µ —Ñ—Ä–∞–∑—ã (–ª–æ–∂–Ω—ã–µ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏—è)
        for exclude in self.exclude_phrases:
            if exclude in text_lower:
                return False
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        for keyword in self.keywords:
            # –ü–æ–∏—Å–∫ —Ç–æ—á–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –∏–ª–∏ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤ —Å–æ—Å—Ç–∞–≤–µ —Å–ª–æ–≤–∞
            pattern = r'\b' + re.escape(keyword) + r'\b'
            if re.search(pattern, text_lower, flags=re.IGNORECASE):
                return True
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤
        words = re.findall(r'\b\w+\b', text_lower)
        for word in words:
            for keyword in self.keywords:
                # –ß–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ (–¥–ª—è —Å–∫–ª–æ–Ω–µ–Ω–∏–π, –æ–ø–µ—á–∞—Ç–æ–∫)
                if keyword in word or word in keyword:
                    return True
        
        return False
    
    def detect_sentiment_advanced(self, text: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º ML –º–æ–¥–µ–ª–∏"""
        if not self.sentiment_analyzer:
            return self.detect_sentiment_basic(text)
        
        try:
            result = self.sentiment_analyzer(text[:512])  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
            label = result[0]['label']
            score = result[0]['score']
            
            # –ú–∞–ø–ø–∏–Ω–≥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –º–æ–¥–µ–ª–∏ –Ω–∞ –Ω–∞—à–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            sentiment_map = {
                'POSITIVE': '–ü–æ–∑–∏—Ç–∏–≤–Ω–∞—è',
                'NEGATIVE': '–ù–µ–≥–∞—Ç–∏–≤–Ω–∞—è',
                'NEUTRAL': '–ù–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è'
            }
            
            return sentiment_map.get(label, '–ù–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è')
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏: {e}")
            return self.detect_sentiment_basic(text)
    
    def detect_sentiment_basic(self, text: str) -> str:
        """–ë–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º"""
        text_lower = text.lower()
        
        # –ü–æ–¥—Å—á–µ—Ç –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö –∏ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö –º–∞—Ä–∫–µ—Ä–æ–≤
        positive_count = sum(1 for word in self.positive_words if word in text_lower)
        negative_count = sum(1 for word in self.risk_words if word in text_lower)
        
        # –ê–Ω–∞–ª–∏–∑ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã—Ö –º–∞—Ä–∫–µ—Ä–æ–≤
        positive_patterns = [
            r'\b(–æ—Ç–ª–∏—á–Ω|–ø—Ä–µ–∫—Ä–∞—Å–Ω|–∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω|—Å—É–ø–µ—Ä|–∫–ª–∞—Å—Å|–ª—É—á—à|—Ö–æ—Ä–æ—à)\b',
            r'[üòÄüòÉüòÑüòÅüòÜüòçü§©]',
            r'\b(—Å–ø–∞—Å–∏–±–æ|–±–ª–∞–≥–æ–¥–∞—Ä|—Ä–µ–∫–æ–º–µ–Ω–¥—É—é|—Å–æ–≤–µ—Ç—É—é)\b'
        ]
        
        negative_patterns = [
            r'\b(–ø–ª–æ—Ö|—É–∂–∞—Å–Ω|–∫–æ—à–º–∞—Ä|–æ—Ç–≤—Ä–∞—Ç–∏—Ç–µ–ª—å–Ω|—É–∂–∞—Å–Ω–æ|–ø–ª–æ—Ö–æ)\b',
            r'[üò†üò°ü§¨üò¢üò≠üò§]',
            r'\b(–∂–∞–ª–æ–±|–Ω–µ–¥–æ–≤–æ–ª|–≤–æ–∑–º—É—â–µ–Ω|—Ä–∞–∑–æ—á–∞—Ä–æ–≤–∞–Ω)\b'
        ]
        
        for pattern in positive_patterns:
            if re.search(pattern, text_lower, re.IGNORECASE):
                positive_count += 1
        
        for pattern in negative_patterns:
            if re.search(pattern, text_lower, re.IGNORECASE):
                negative_count += 1
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
        if positive_count > negative_count and positive_count > 0:
            return '–ü–æ–∑–∏—Ç–∏–≤–Ω–∞—è'
        elif negative_count > positive_count and negative_count > 0:
            return '–ù–µ–≥–∞—Ç–∏–≤–Ω–∞—è'
        elif positive_count == negative_count and positive_count > 0:
            return '–°–º–µ—à–∞–Ω–Ω–∞—è'
        else:
            return '–ù–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è'
    
    def assign_tags_advanced(self, text: str) -> List[str]:
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–∏—Å–≤–æ–µ–Ω–∏–µ —Ç–µ–≥–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º NLP"""
        tags = []
        text_lower = text.lower()
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞–∂–¥–æ–≥–æ —Ç–µ–≥–∞ –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º
        for tag_id, patterns in self.tag_mapping.items():
            for pattern in patterns:
                # –ü–æ–∏—Å–∫ –ø–∞—Ç—Ç–µ—Ä–Ω–∞ –≤ —Ç–µ–∫—Å—Ç–µ
                if re.search(r'\b' + pattern + r'\w*\b', text_lower, re.IGNORECASE):
                    if self.UNIVERSAL_TAGS[tag_id] not in tags:
                        tags.append(self.UNIVERSAL_TAGS[tag_id])
                    break  # –ü–µ—Ä–µ—Ö–æ–¥–∏–º –∫ —Å–ª–µ–¥—É—é—â–µ–º—É —Ç–µ–≥—É
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ª–æ–≥–∏–∫–∞ –¥–ª—è —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö —Ç–µ–≥–æ–≤
        self._assign_specific_tags(text_lower, tags)
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–≥–æ–≤ (–º–∞–∫—Å–∏–º—É–º 5)
        return tags[:5]
    
    def _assign_specific_tags(self, text: str, tags: List[str]):
        """–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ª–æ–≥–∏–∫–∞ –¥–ª—è —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö —Ç–µ–≥–æ–≤"""
        # –¢–µ–≥ "–í–æ–ø—Ä–æ—Å/—Å–ø—Ä–∞–≤–∫–∞"
        if re.search(r'\?|–ø–æ–¥—Å–∫–∞–∂–∏—Ç|–ø–æ—Å–æ–≤–µ—Ç—É–π|—Ä–∞—Å—Å–∫–∞–∂–∏—Ç|–∫–∞–∫\s+\w+|—á—Ç–æ\s+\w+', text):
            if '–í–æ–ø—Ä–æ—Å/—Å–ø—Ä–∞–≤–∫–∞' not in tags:
                tags.append('–í–æ–ø—Ä–æ—Å/—Å–ø—Ä–∞–≤–∫–∞')
        
        # –¢–µ–≥ "–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞–º–∏"
        competitors = ['–Ω—É—Ç—Ä–∏–ª–∞–∫', '–Ω—É—Ç—Ä–∏—Ü–∏—è', '–Ω–µ—Å—Ç–ª–µ', '—Ö–∏–ø–ø', '—Å–µ–º–ø–µ—Ä',
                      '–ª—É–∫–æ–π–ª', '—Ç–∞—Ç–Ω–µ—Ñ—Ç—å', '—Ä–æ—Å–Ω–µ—Ñ—Ç—å', 'shell', '–±–ø']
        if any(comp in text for comp in competitors):
            if '–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞–º–∏' not in tags:
                tags.append('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞–º–∏')
        
        # –¢–µ–≥ "–ñ–∞–ª–æ–±–∞"
        complaint_patterns = [
            r'–∂–∞–ª–æ–±\w+', r'–Ω–µ–¥–æ–≤–æ–ª\w+', r'–≤–æ–∑–º—É—â\w+', r'–ø—Ä–æ—Ç–µ—Å—Ç\w+',
            r'–ø—Ä–µ—Ç–µ–Ω–∑\w+', r'—Ç—Ä–µ–±—É—é', r'–≤–µ—Ä–Ω–∏—Ç–µ', r'–Ω–∞–ø–∏—à\w+\s+–∂–∞–ª–æ–±—É'
        ]
        if any(re.search(pattern, text) for pattern in complaint_patterns):
            if '–ñ–∞–ª–æ–±–∞' not in tags:
                tags.append('–ñ–∞–ª–æ–±–∞')
    
    def is_dangerous_for_reputation(self, text: str, sentiment: str) -> bool:
        """
        –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –¥–ª—è —Ä–µ–ø—É—Ç–∞—Ü–∏–∏
        
        Args:
            text: –¢–µ–∫—Å—Ç —Å–æ–æ–±—â–µ–Ω–∏—è
            sentiment: –û–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–∞—è —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å
            
        Returns:
            bool: True –µ—Å–ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ –æ–ø–∞—Å–Ω–æ –¥–ª—è —Ä–µ–ø—É—Ç–∞—Ü–∏–∏
        """
        text_lower = text.lower()
        
        # –ö—Ä–∏—Ç–µ—Ä–∏–∏ –æ–ø–∞—Å–Ω–æ—Å—Ç–∏
        danger_criteria = [
            # 1. –ù–µ–≥–∞—Ç–∏–≤–Ω–∞—è —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å
            sentiment == '–ù–µ–≥–∞—Ç–∏–≤–Ω–∞—è',
            
            # 2. –ù–∞–ª–∏—á–∏–µ —Ä–∏—Å–∫-—Å–ª–æ–≤
            any(risk_word in text_lower for risk_word in self.risk_words),
            
            # 3. –ñ–∞–ª–æ–±—ã –Ω–∞ –∑–¥–æ—Ä–æ–≤—å–µ/–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å
            any(health_word in text_lower for health_word in 
                ['–æ—Ç—Ä–∞–≤–ª–µ–Ω–∏–µ', '–∞–ª–ª–µ—Ä–≥–∏—è', '–æ–ø–∞—Å–Ω–æ', '–≤—Ä–µ–¥–Ω–æ', '—É–≥—Ä–æ–∑–∞']),
            
            # 4. –ü—Ä–∏–∑—ã–≤—ã –∫ –±–æ–π–∫–æ—Ç—É/–∂–∞–ª–æ–±–∞–º –≤ –æ—Ä–≥–∞–Ω—ã
            any(action_word in text_lower for action_word in
                ['—Ä–æ—Å–ø–æ—Ç—Ä–µ–±–Ω–∞–¥–∑–æ—Ä', '–ø–æ–∂–∞–ª—É—é—Å—å', '–∑–∞—è–≤–ª–µ–Ω', '–∏—Å–∫', '—Å—É–¥']),
            
            # 5. –í–∏—Ä—É—Å–Ω—ã–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª (–≤–æ—Å–∫–ª–∏—Ü–∞–Ω–∏—è, –∫–∞–ø—Å, –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∑–Ω–∞–∫–∏)
            bool(re.search(r'!{2,}|[A-Z–ê-–Ø]{5,}', text))
        ]
        
        # –°–æ–æ–±—â–µ–Ω–∏–µ –æ–ø–∞—Å–Ω–æ, –µ—Å–ª–∏ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è —Ö–æ—Ç—è –±—ã 2 –∫—Ä–∏—Ç–µ—Ä–∏—è
        return sum(danger_criteria) >= 2
    
    def analyze_mention(self, text: str) -> Dict:
        """
        –ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –æ–¥–Ω–æ–≥–æ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è
        
        Returns:
            Dict: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞
        """
        relevant = self.is_relevant(text)
        
        if not relevant:
            return {
                'relevant': False,
                'sentiment': '–ù–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ',
                'tags': [],
                'dangerous': False
            }
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
        if self.use_advanced_nlp and self.sentiment_analyzer:
            sentiment = self.detect_sentiment_advanced(text)
        else:
            sentiment = self.detect_sentiment_basic(text)
        
        # –ü—Ä–∏—Å–≤–æ–µ–Ω–∏–µ —Ç–µ–≥–æ–≤
        tags = self.assign_tags_advanced(text)
        
        # –û—Ü–µ–Ω–∫–∞ –æ–ø–∞—Å–Ω–æ—Å—Ç–∏
        dangerous = self.is_dangerous_for_reputation(text, sentiment)
        
        return {
            'relevant': True,
            'sentiment': sentiment,
            'tags': tags,
            'dangerous': dangerous
        }


class BatchAnalyzer:
    """–ü–∞–∫–µ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–∞–±–ª–∏—Ü"""
    
    def __init__(self, configs: Dict[str, Dict]):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –æ–±—ä–µ–∫—Ç–æ–≤ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
        
        Args:
            configs: –°–ª–æ–≤–∞—Ä—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π {object_name: config}
        """
        self.analyzers = {}
        for obj_name, config in configs.items():
            self.analyzers[obj_name] = MentionAnalyzer(
                object_name=obj_name,
                keywords=config.get('keywords', []),
                risk_words=config.get('risk_words', []),
                positive_words=config.get('positive_words', []),
                exclude_phrases=config.get('exclude_phrases', []),
                use_advanced_nlp=config.get('use_advanced_nlp', True)
            )
    
    def analyze_dataframe(self, 
                         df: pd.DataFrame,
                         text_column: str,
                         object_column: Optional[str] = None,
                         object_name: Optional[str] = None) -> pd.DataFrame:
        """
        –ê–Ω–∞–ª–∏–∑ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞ —Å —É–ø–æ–º–∏–Ω–∞–Ω–∏—è–º–∏
        
        Args:
            df: DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏
            text_column: –ù–∞–∑–≤–∞–Ω–∏–µ —Å—Ç–æ–ª–±—Ü–∞ —Å —Ç–µ–∫—Å—Ç–æ–º
            object_column: –°—Ç–æ–ª–±–µ—Ü —Å —É–∫–∞–∑–∞–Ω–∏–µ–º –æ–±—ä–µ–∫—Ç–∞ (–µ—Å–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ)
            object_name: –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞ (–µ—Å–ª–∏ –æ–¥–∏–Ω)
            
        Returns:
            DataFrame —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞
        """
        results = []
        
        for idx, row in df.iterrows():
            text = row[text_column]
            
            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
            if object_column:
                obj_name = row[object_column]
                analyzer = self.analyzers.get(obj_name)
                if not analyzer:
                    continue
            elif object_name:
                analyzer = self.analyzers.get(object_name)
                if not analyzer:
                    continue
            else:
                raise ValueError("–£–∫–∞–∂–∏—Ç–µ object_column –∏–ª–∏ object_name")
            
            # –ê–Ω–∞–ª–∏–∑
            analysis = analyzer.analyze_mention(text)
            
            # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç—Ä–æ–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            result_row = {
                '–¢–µ–∫—Å—Ç': text,
                '–°–æ–æ–±—â–µ–Ω–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ?': '–î–∞' if analysis['relevant'] else '–ù–µ—Ç',
                '–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å': analysis['sentiment'],
                '–û–ø–∞—Å–Ω–æ –¥–ª—è —Ä–µ–ø—É—Ç–∞—Ü–∏–∏': '–î–∞' if analysis['dangerous'] else '–ù–µ—Ç'
            }
            
            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ç–µ–≥–æ–≤ (–¥–æ 5)
            for i in range(5):
                tag_key = f'–¢–µ–≥ {i+1}'
                if i < len(analysis['tags']):
                    result_row[tag_key] = analysis['tags'][i]
                else:
                    result_row[tag_key] = ''
            
            results.append(result_row)
        
        return pd.DataFrame(results)


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
def main():
    """–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞"""
    
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –±—Ä–µ–Ω–¥–∞ "–ú–∞–ª—é—Ç–∫–∞"
    malyutka_config = {
        'keywords': ['–º–∞–ª—é—Ç–∫–∞', 'nutricia', '–Ω—É—Ç—Ä–∏—Ü–∏—è', '–¥–µ—Ç—Å–∫', '–ø–∏—Ç–∞–Ω–∏'],
        'risk_words': ['–∑–∞–ø–æ—Ä', '–∫–æ–ª–∏–∫–∏', '—Å—ã–ø—å', '—Ä–≤–æ—Ç–∞', '–∞–ª–ª–µ—Ä–≥–∏—è', 
                      '–ø–ª–æ—Ö', '—É–∂–∞—Å', '–∫–æ—à–º–∞—Ä', '–Ω–µ–∫–∞—á–µ—Å—Ç–≤–µ–Ω'],
        'positive_words': ['–Ω—Ä–∞–≤–∏—Ç—Å—è', '–ª—é–±–ª—é', '–æ–±–æ–∂–∞—é', '–æ—Ç–ª–∏—á–Ω', 
                          '—Ö–æ—Ä–æ—à', '—Ä–µ–∫–æ–º–µ–Ω–¥—É—é', '–¥–æ–≤–µ—Ä—è—é'],
        'exclude_phrases': ['–¥–æ–º –º–∞–ª—é—Ç–∫–∏', '–º–∞–ª—é—Ç–∫–∞ —Ä–æ–¥–∏–ª–∞—Å—å'],
        'use_advanced_nlp': TRANSFORMERS_AVAILABLE
    }
    
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è "–ì–∞–∑–ø—Ä–æ–º –Ω–µ—Ñ—Ç—å"
    gazprom_config = {
        'keywords': ['–≥–∞–∑–ø—Ä–æ–º–Ω–µ—Ñ—Ç—å', '–≥–∞–∑–ø—Ä–æ–º', '–∞–∑—Å –≥–∞–∑–ø—Ä–æ–º', 
                    'gazprom', '–≥–ø–Ω', 'gdrive'],
        'risk_words': ['–±–∞–¥—è–∂', '–Ω–µ–∫–∞—á–µ—Å—Ç–≤–µ–Ω', '–ø–ª–æ—Ö', '–±—Ä–∞–∫', 
                      '–æ–±–º–∞–Ω', '—Ä–∞–∑–≤–æ–¥', '–∂—É–ª—å–Ω–∏—á'],
        'positive_words': ['—Ö–æ—Ä–æ—à', '–∫–∞—á–µ—Å—Ç–≤–µ–Ω', '–æ—Ç–ª–∏—á–Ω', '—Ä–µ–∫–æ–º–µ–Ω–¥—É—é'],
        'exclude_phrases': [],
        'use_advanced_nlp': TRANSFORMERS_AVAILABLE
    }
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞–∫–µ—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
    configs = {
        '–ú–∞–ª—é—Ç–∫–∞': malyutka_config,
        '–ì–∞–∑–ø—Ä–æ–º –Ω–µ—Ñ—Ç—å': gazprom_config
    }
    
    analyzer = BatchAnalyzer(configs)
    
    print("–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω.")
    print(f"–ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ NLP –º–æ–¥–µ–ª–∏: {TRANSFORMERS_AVAILABLE}")
    
    # –ü—Ä–∏–º–µ—Ä –∞–Ω–∞–ª–∏–∑–∞ –æ–¥–Ω–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è
    test_texts = [
        "–ú–∞–ª—é—Ç–∫–∞ –æ—á–µ–Ω—å –≤–∫—É—Å–Ω—ã–µ –∫–∞—à–∏, –º–æ–∏ –¥–µ—Ç–∏ –µ–¥—è—Ç —Å —É–¥–æ–≤–æ–ª—å—Å—Ç–≤–∏–µ–º!",
        "–û—Ç –º–∞–ª—é—Ç–∫–∏ —É —Ä–µ–±–µ–Ω–∫–∞ –Ω–∞—á–∞–ª–∞—Å—å —Å—Ç—Ä–∞—à–Ω–∞—è –∞–ª–ª–µ—Ä–≥–∏—è –∏ —Å—ã–ø—å –ø–æ –≤—Å–µ–º—É —Ç–µ–ª—É!",
        "–ì–∞–∑–ø—Ä–æ–º –Ω–∞ –©–µ—Ä–±–∞–∫–æ–≤–∞ –æ–ø—è—Ç—å –±–∞–¥—è–∂–∏—Ç –±–µ–Ω–∑–∏–Ω, –º–∞—à–∏–Ω–∞ —Å–ª–æ–º–∞–ª–∞—Å—å!"
    ]
    
    for text in test_texts:
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–±—ä–µ–∫—Ç –ø–æ —Ç–µ–∫—Å—Ç—É
        if any(kw in text.lower() for kw in malyutka_config['keywords']):
            analyzer_name = '–ú–∞–ª—é—Ç–∫–∞'
        else:
            analyzer_name = '–ì–∞–∑–ø—Ä–æ–º –Ω–µ—Ñ—Ç—å'
        
        result = analyzer.analyzers[analyzer_name].analyze_mention(text)
        print(f"\n–ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞: {text[:50]}...")
        print(f"–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {result['relevant']}")
        print(f"–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å: {result['sentiment']}")
        print(f"–¢–µ–≥–∏: {result['tags']}")
        print(f"–û–ø–∞—Å–Ω–æ: {result['dangerous']}")


if __name__ == "__main__":
    main()