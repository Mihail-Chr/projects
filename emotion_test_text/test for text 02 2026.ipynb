{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "602a752b-6a31-476f-8acb-5eab488845f7",
   "metadata": {},
   "source": [
    "## –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∏–±–ª–∏–æ—Ç–µ–∫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30222b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –û—Å–Ω–æ–≤–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n",
    "!pip install pybind11\n",
    "!pip install pandas openpyxl numpy\n",
    "# –î–ª—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å transformers (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)\n",
    "!pip install transformers torch\n",
    "\n",
    "# –î–ª—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π\n",
    "!pip install scikit-learn \n",
    "!pip install gensim \n",
    "\n",
    "# –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞\n",
    "!pip install razdel\n",
    "!pip install pymorphy3 chardet\n",
    "\n",
    "# –î–ª—è Dostoevsky –Ω—É–∂–Ω–∞ –º–æ–¥–µ–ª—å\n",
    "!pip install dostoevsky --no-deps\n",
    "!pip install fasttext-wheel==0.9.2\n",
    "\n",
    "#!python -m dostoevsky download fasttext-social-network-model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8caee165-78d5-4c3c-b673-7c0cb81a3f93",
   "metadata": {},
   "source": [
    "## –ò–º–ø–æ—Ä—Ç –∏ –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ—Å—Ç–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫ –∏ –º–æ–¥–µ–ª–µ–π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f94f89e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "–ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ò–ô –ê–ù–ê–õ–ò–ó–ê–¢–û–† –£–ü–û–ú–ï–ù–ê–ù–ò–ô –î–õ–Ø –ù–ï–°–ö–û–õ–¨–ö–ò–• –ë–†–ï–ù–î–û–í\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —É–ø–æ–º–∏–Ω–∞–Ω–∏–π –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ —Ä–µ–ø—É—Ç–∞—Ü–∏–∏ –±—Ä–µ–Ω–¥–æ–≤\n",
    "–ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import warnings\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "import chardet\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ —É—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫\n",
    "def install_missing_packages():\n",
    "    \"\"\"–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏—Ö –ø–∞–∫–µ—Ç–æ–≤\"\"\"\n",
    "    required_packages = {\n",
    "        'pandas': 'pandas',\n",
    "        'numpy': 'numpy',\n",
    "        'openpyxl': 'openpyxl',\n",
    "        'xlrd': 'xlrd>=1.2.0',\n",
    "        'chardet': 'chardet',\n",
    "        'tqdm': 'tqdm'\n",
    "    }\n",
    "    \n",
    "    missing = []\n",
    "    for package, install_name in required_packages.items():\n",
    "        try:\n",
    "            __import__(package)\n",
    "        except ImportError:\n",
    "            missing.append(install_name)\n",
    "    \n",
    "    if missing:\n",
    "        print(f\"–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –ø–∞–∫–µ—Ç—ã: {', '.join(missing)}\")\n",
    "        response = input(\"–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∏—Ö –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏? (y/n): \")\n",
    "        if response.lower() == 'y':\n",
    "            import subprocess\n",
    "            try:\n",
    "                subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\"] + missing)\n",
    "                print(\"–ü–∞–∫–µ—Ç—ã —É—Å–ø–µ—à–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã!\")\n",
    "            except Exception as e:\n",
    "                print(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ —É—Å—Ç–∞–Ω–æ–≤–∫–µ –ø–∞–∫–µ—Ç–æ–≤: {e}\")\n",
    "                print(\"–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∏—Ö –≤—Ä—É—á–Ω—É—é:\")\n",
    "                print(f\"pip install {' '.join(missing)}\")\n",
    "        else:\n",
    "            print(\"–ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ —É—Å—Ç–∞–Ω–æ–≤–∫–∏...\")\n",
    "\n",
    "# –í—ã–∑—ã–≤–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É –ø–∞–∫–µ—Ç–æ–≤\n",
    "install_missing_packages()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"–ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ò–ô –ê–ù–ê–õ–ò–ó–ê–¢–û–† –£–ü–û–ú–ï–ù–ê–ù–ò–ô –î–õ–Ø –ù–ï–°–ö–û–õ–¨–ö–ò–• –ë–†–ï–ù–î–û–í\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –±—Ä–µ–Ω–¥–æ–≤\n",
    "BRAND_CONFIGS = {\n",
    "    1: {\n",
    "        'name': '–ì–∞–∑–ø—Ä–æ–º –Ω–µ—Ñ—Ç—å',\n",
    "        'keywords': [\n",
    "            '–≥–∞–∑–ø—Ä–æ–º–Ω–µ—Ñ—Ç—å', '–≥–∞–∑–ø—Ä–æ–º', '–∞–∑—Å –≥–∞–∑–ø—Ä–æ–º', \n",
    "            '–≥–ø–Ω', 'gazprom', 'gazpromneft', 'gdrive',\n",
    "            '–≥–∞–∑–ø—Ä–æ–º –Ω–µ—Ñ—Ç—å', '–≥–∞–∑–ø—Ä–æ–º–Ω–µ—Ñ—Ç–∏', '–≥–∞–∑–ø—Ä–æ–º–Ω–µ—Ñ—Ç—å—é'\n",
    "        ],\n",
    "        'risk_words': [\n",
    "            '–±–∞–¥—è–∂', '–Ω–µ–∫–∞—á–µ—Å—Ç–≤–µ–Ω', '–ø–ª–æ—Ö', '—É–∂–∞—Å', '–∫–æ—à–º–∞—Ä',\n",
    "            '–æ–±–º–∞–Ω', '—Ä–∞–∑–≤–æ–¥', '–∂—É–ª—å–Ω–∏—á', '–±—Ä–∞–∫', '–∏—Å–ø–æ—Ä—á',\n",
    "            '–≥—Ä—è–∑–Ω', '–≤—Ä–µ–¥–Ω', '–æ–ø–∞—Å–Ω', '—Ç—Ä—è—Å–µ—Ç', '–¥–µ—Ä–≥–∞–µ—Ç',\n",
    "            '–¥—ã–º', '—Ç—Ä–æ–∏—Ç', '–≥–ª–æ—Ö–Ω–µ—Ç', '—è–º–∞', '—Ä–∞–∑–±–∏—Ç'\n",
    "        ],\n",
    "        'positive_words': [\n",
    "            '—Ö–æ—Ä–æ—à', '–æ—Ç–ª–∏—á–Ω', '–ø—Ä–µ–∫—Ä–∞—Å–Ω', '–∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω',\n",
    "            '—Ä–µ–∫–æ–º–µ–Ω–¥—É—é', '—Å–æ–≤–µ—Ç—É—é', '—Å–ø–∞—Å–∏–±–æ', '–±–ª–∞–≥–æ–¥–∞—Ä—é',\n",
    "            '–¥–æ–≤–æ–ª–µ–Ω', '—Ä–∞–¥', '–∫–ª–∞—Å—Å', '—Å—É–ø–µ—Ä', '–ª—É—á—à'\n",
    "        ],\n",
    "        'competitors': [\n",
    "            '–ª—É–∫–æ–π–ª', '—Ä–æ—Å–Ω–µ—Ñ—Ç—å', '—Ç–∞—Ç–Ω–µ—Ñ—Ç—å', 'shell', '–±–ø',\n",
    "            'energy', '—ç–Ω–µ—Ä–≥–∏—è', '–∞–∏', '–Ω–µ—Ñ—Ç–µ–º–∞–≥–∏—Å—Ç—Ä–∞–ª—å'\n",
    "        ],\n",
    "        'output_prefix': '–ì–ü–ù'\n",
    "    },\n",
    "    2: {\n",
    "        'name': '–ú–∞–ª—é—Ç–∫–∞',\n",
    "        'keywords': [\n",
    "            '–º–∞–ª—é—Ç–∫–∞', 'nutricia', '–Ω—É—Ç—Ä–∏—Ü–∏—è', '–Ω—É—Ç—Ä–∏—Ü–∏—è',\n",
    "            '–º–∞–ª—é—Ç–∫', '–º–∞–ª—é—Ç–∫–∏', '–º–∞–ª—é—Ç–∫—É', '–º–∞–ª—é—Ç–∫–æ–π',\n",
    "            'malyutka', '–¥–µ—Ç—Å–∫', '–ø–∏—Ç–∞–Ω–∏', '—Å–º–µ—Å', '–∫–∞—à',\n",
    "            '–º–æ–ª–æ—á–∫', '–ø—é—Ä–µ'\n",
    "        ],\n",
    "        'risk_words': [\n",
    "            '–∑–∞–ø–æ—Ä', '–∫–æ–ª–∏–∫–∏', '—Å—ã–ø—å', '—Ä–≤–æ—Ç–∞', '–∞–ª–ª–µ—Ä–≥–∏',\n",
    "            '–ø–ª–æ—Ö', '—É–∂–∞—Å', '–∫–æ—à–º–∞—Ä', '–Ω–µ–∫–∞—á–µ—Å—Ç–≤–µ–Ω', '–≤—Ä–µ–¥–Ω',\n",
    "            '–æ–ø–∞—Å–Ω', '–æ—Ç—Ä–∞–≤–ª–µ–Ω', '–±–æ–ª–µ–∑–Ω', '–±–æ–ª—å–Ω', '–ª–µ—á–µ–Ω'\n",
    "        ],\n",
    "        'positive_words': [\n",
    "            '–≤–∫—É—Å–Ω', '–Ω—Ä–∞–≤–∏—Ç—Å—è', '–ª—é–±–ª—é', '–æ–±–æ–∂–∞—é', '–æ—Ç–ª–∏—á–Ω',\n",
    "            '—Ö–æ—Ä–æ—à', '—Ä–µ–∫–æ–º–µ–Ω–¥—É—é', '—Å–ø–∞—Å–∏–±–æ', '–¥–æ–≤–µ—Ä—è—é',\n",
    "            '–ø–æ–ª–µ–∑–Ω', '–∑–¥–æ—Ä–æ–≤', '–∫–∞—á–µ—Å—Ç–≤–µ–Ω', '–Ω–∞—Ç—É—Ä–∞–ª—å–Ω'\n",
    "        ],\n",
    "        'competitors': [\n",
    "            '–Ω—É—Ç—Ä–∏–ª–∞–∫', '–Ω–µ—Å—Ç–ª–µ', '—Ö–∏–ø–ø', '—Å–µ–º–ø–µ—Ä', '—Ñ—Ä–∏—Å–æ',\n",
    "            '–∞–≥—É—à–∞', '—Ç–µ–º–∞', '–±–µ–ª–ª–∞–∫—Ç', '–±–µ–±–∏'\n",
    "        ],\n",
    "        'output_prefix': '–ú–∞–ª—é—Ç–∫–∞'\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "class ModelDownloader:\n",
    "    \"\"\"–ö–ª–∞—Å—Å –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –º–æ–¥–µ–ª—è–º–∏\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def download_dostoevsky_model():\n",
    "        \"\"\"–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ Dostoevsky\"\"\"\n",
    "        print(\"–ü–æ–ø—ã—Ç–∫–∞ —Å–∫–∞—á–∞—Ç—å –º–æ–¥–µ–ª—å Dostoevsky...\")\n",
    "        try:\n",
    "            import requests\n",
    "            import zipfile\n",
    "            import tempfile\n",
    "            import shutil\n",
    "            \n",
    "            # URL –º–æ–¥–µ–ª–∏ Dostoevsky\n",
    "            model_url = \"https://github.com/bureaucratic-labs/dostoevsky/raw/master/dostoevsky/data/models/fasttext-social-network-model.bin\"\n",
    "            \n",
    "            # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é\n",
    "            temp_dir = tempfile.mkdtemp()\n",
    "            model_path = os.path.join(temp_dir, \"fasttext-social-network-model.bin\")\n",
    "            \n",
    "            # –°–∫–∞—á–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å\n",
    "            print(f\"–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ Dostoevsky...\")\n",
    "            response = requests.get(model_url, stream=True)\n",
    "            total_size = int(response.headers.get('content-length', 0))\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                with open(model_path, 'wb') as f:\n",
    "                    for chunk in response.iter_content(chunk_size=8192):\n",
    "                        if chunk:\n",
    "                            f.write(chunk)\n",
    "                \n",
    "                # –ö–æ–ø–∏—Ä—É–µ–º –≤ –Ω—É–∂–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é\n",
    "                import dostoevsky\n",
    "                dostoevsky_dir = os.path.dirname(dostoevsky.__file__)\n",
    "                target_dir = os.path.join(dostoevsky_dir, 'data', 'models')\n",
    "                os.makedirs(target_dir, exist_ok=True)\n",
    "                target_path = os.path.join(target_dir, 'fasttext-social-network-model.bin')\n",
    "                \n",
    "                shutil.copy(model_path, target_path)\n",
    "                print(f\"–ú–æ–¥–µ–ª—å Dostoevsky —É—Å–ø–µ—à–Ω–æ —Å–∫–∞—á–∞–Ω–∞ –≤: {target_path}\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"–û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏: {response.status_code}\")\n",
    "                return False\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ –º–æ–¥–µ–ª–∏ Dostoevsky: {e}\")\n",
    "            print(\"–ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—É—é –º–æ–¥–µ–ª—å...\")\n",
    "            return False\n",
    "    \n",
    "    @staticmethod\n",
    "    def check_and_fix_dostoevsky():\n",
    "        \"\"\"–ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ Dostoevsky\"\"\"\n",
    "        try:\n",
    "            from dostoevsky.tokenization import RegexTokenizer\n",
    "            from dostoevsky.models import FastTextSocialNetworkModel\n",
    "            \n",
    "            # –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å\n",
    "            tokenizer = RegexTokenizer()\n",
    "            model = FastTextSocialNetworkModel(tokenizer=tokenizer)\n",
    "            \n",
    "            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–±–æ—Ç—É –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º —Ç–µ–∫—Å—Ç–µ\n",
    "            test_result = model.predict([\"—Ç–µ—Å—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç\"], k=2)\n",
    "            \n",
    "            print(\"‚úì Dostoevsky –ø—Ä–æ–≤–µ—Ä–µ–Ω –∏ —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Dostoevsky —Ç—Ä–µ–±—É–µ—Ç –Ω–∞—Å—Ç—Ä–æ–π–∫–∏: {str(e)[:100]}\")\n",
    "            \n",
    "            # –ü—Ä–æ–±—É–µ–º —Å–∫–∞—á–∞—Ç—å –º–æ–¥–µ–ª—å\n",
    "            return ModelDownloader.download_dostoevsky_model()\n",
    "\n",
    "#ModelDownloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4f2b2a-293a-4e1b-a539-97ba5025e445",
   "metadata": {},
   "source": [
    "## –£–ª—É—á—à–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ —Å –∞–≤—Ç–æ–≤—ã–±–æ—Ä–æ–º –º–æ–¥–µ–ª–µ–π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b190941a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalyzer:\n",
    "    \"\"\"–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self._init_model()\n",
    "    \n",
    "    def _init_model(self):\n",
    "        \"\"\"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏\"\"\"\n",
    "        print(\"  –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏...\")\n",
    "        \n",
    "        # –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å Dostoevsky\n",
    "        try:\n",
    "            from dostoevsky.tokenization import RegexTokenizer\n",
    "            from dostoevsky.models import FastTextSocialNetworkModel\n",
    "            \n",
    "            self.tokenizer = RegexTokenizer()\n",
    "            self.model = FastTextSocialNetworkModel(tokenizer=self.tokenizer)\n",
    "            print(\"  ‚úì Dostoevsky –∑–∞–≥—Ä—É–∂–µ–Ω\")\n",
    "            self.model_type = 'dostoevsky'\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚úó Dostoevsky –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª—Å—è: {e}\")\n",
    "            print(\"  –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º\")\n",
    "            self.model_type = 'basic'\n",
    "    \n",
    "    def analyze(self, text: str, risk_words: List[str], positive_words: List[str]) -> Dict:\n",
    "        \"\"\"\n",
    "        –ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞\n",
    "        \n",
    "        Args:\n",
    "            text: –¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞\n",
    "            risk_words: –°–ª–æ–≤–∞, —É–∫–∞–∑—ã–≤–∞—é—â–∏–µ –Ω–∞ —Ä–∏—Å–∫\n",
    "            positive_words: –ü–æ–∑–∏—Ç–∏–≤–Ω—ã–µ –º–∞—Ä–∫–µ—Ä—ã\n",
    "            \n",
    "        Returns:\n",
    "            Dict: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞\n",
    "        \"\"\"\n",
    "        if not text or len(text.strip()) < 3:\n",
    "            return {'sentiment': '–ù–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è', 'confidence': 0.5}\n",
    "        \n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # –ï—Å–ª–∏ –µ—Å—Ç—å –º–æ–¥–µ–ª—å Dostoevsky, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ—ë\n",
    "        if self.model_type == 'dostoevsky' and self.model:\n",
    "            try:\n",
    "                results = self.model.predict([text], k=5)[0]\n",
    "                \n",
    "                sentiment_map = {\n",
    "                    'positive': '–ü–æ–∑–∏—Ç–∏–≤–Ω–∞—è',\n",
    "                    'negative': '–ù–µ–≥–∞—Ç–∏–≤–Ω–∞—è', \n",
    "                    'neutral': '–ù–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è'\n",
    "                }\n",
    "                \n",
    "                if results:\n",
    "                    main_sentiment = max(results.items(), key=lambda x: x[1])\n",
    "                    sentiment_label = sentiment_map.get(main_sentiment[0], '–ù–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è')\n",
    "                    \n",
    "                    return {\n",
    "                        'sentiment': sentiment_label,\n",
    "                        'confidence': float(main_sentiment[1]),\n",
    "                        'model': 'dostoevsky'\n",
    "                    }\n",
    "            except Exception as e:\n",
    "                print(f\"    –û—à–∏–±–∫–∞ Dostoevsky: {e}\")\n",
    "        \n",
    "        # –ë–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º\n",
    "        return self._basic_analysis(text_lower, risk_words, positive_words)\n",
    "    \n",
    "    def _basic_analysis(self, text_lower: str, risk_words: List[str], positive_words: List[str]) -> Dict:\n",
    "        \"\"\"–ë–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º\"\"\"\n",
    "        # –ü–æ–¥—Å—á–µ—Ç —Ä–∏—Å–∫-—Å–ª–æ–≤\n",
    "        risk_count = sum(1 for word in risk_words if word in text_lower)\n",
    "        \n",
    "        # –ü–æ–¥—Å—á–µ—Ç –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö —Å–ª–æ–≤\n",
    "        positive_count = sum(1 for word in positive_words if word in text_lower)\n",
    "        \n",
    "        # –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –º–∞—Ä–∫–µ—Ä—ã\n",
    "        negative_emojis = ['üò†', 'üò°', 'ü§¨', 'üò¢', 'üò≠', 'üò§', 'üëé', 'üíî']\n",
    "        positive_emojis = ['üòÄ', 'üòÉ', 'üòÑ', 'üòÅ', 'üòÜ', 'üòç', 'ü§©', 'üëç', '‚ù§Ô∏è']\n",
    "        \n",
    "        for emoji in negative_emojis:\n",
    "            if emoji in text_lower:\n",
    "                risk_count += 1\n",
    "        \n",
    "        for emoji in positive_emojis:\n",
    "            if emoji in text_lower:\n",
    "                positive_count += 1\n",
    "        \n",
    "        # –ö–ª—é—á–µ–≤—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞\n",
    "        negative_patterns = [\n",
    "            r'\\b(–ø–ª–æ—Ö|—É–∂–∞—Å–Ω|–∫–æ—à–º–∞—Ä|–æ—Ç–≤—Ä–∞—Ç–∏—Ç–µ–ª—å–Ω|–Ω–µ–∫–∞—á–µ—Å—Ç–≤–µ–Ω|–∂–∞–ª–æ–±|–Ω–µ–¥–æ–≤–æ–ª–µ–Ω)\\b',\n",
    "            r'\\b(–≤–æ–∑–º—É—â–µ–Ω|—Ä–∞–∑–æ—á–∞—Ä–æ–≤–∞–Ω|–ø—Ä–æ—Ç–µ—Å—Ç—É—é|—Ç—Ä–µ–±—É—é|–≤–µ—Ä–Ω–∏—Ç–µ)\\b'\n",
    "        ]\n",
    "        \n",
    "        positive_patterns = [\n",
    "            r'\\b(–æ—Ç–ª–∏—á–Ω|–ø—Ä–µ–∫—Ä–∞—Å–Ω|–∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω|—Å—É–ø–µ—Ä|–∫–ª–∞—Å—Å|–ª—É—á—à|—Ö–æ—Ä–æ—à)\\b',\n",
    "            r'\\b(—Ä–µ–∫–æ–º–µ–Ω–¥—É—é|—Å–æ–≤–µ—Ç—É—é|—Å–ø–∞—Å–∏–±–æ|–±–ª–∞–≥–æ–¥–∞—Ä—é|–¥–æ–≤–æ–ª–µ–Ω|—Ä–∞–¥)\\b'\n",
    "        ]\n",
    "        \n",
    "        for pattern in negative_patterns:\n",
    "            if re.search(pattern, text_lower):\n",
    "                risk_count += 1\n",
    "        \n",
    "        for pattern in positive_patterns:\n",
    "            if re.search(pattern, text_lower):\n",
    "                positive_count += 1\n",
    "        \n",
    "        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏\n",
    "        if risk_count > positive_count and risk_count > 0:\n",
    "            confidence = min(0.5 + (risk_count - positive_count) * 0.1, 0.9)\n",
    "            return {'sentiment': '–ù–µ–≥–∞—Ç–∏–≤–Ω–∞—è', 'confidence': confidence, 'model': 'basic'}\n",
    "        elif positive_count > risk_count and positive_count > 0:\n",
    "            confidence = min(0.5 + (positive_count - risk_count) * 0.1, 0.9)\n",
    "            return {'sentiment': '–ü–æ–∑–∏—Ç–∏–≤–Ω–∞—è', 'confidence': confidence, 'model': 'basic'}\n",
    "        elif risk_count == positive_count and risk_count > 0:\n",
    "            return {'sentiment': '–°–º–µ—à–∞–Ω–Ω–∞—è', 'confidence': 0.5, 'model': 'basic'}\n",
    "        else:\n",
    "            return {'sentiment': '–ù–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è', 'confidence': 0.5, 'model': 'basic'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8409fd94-85d7-49b2-8a42-f95c026789cb",
   "metadata": {},
   "source": [
    "## –û—Å–Ω–æ–≤–Ω–æ–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —É–ø–æ–º–∏–Ω–∞–Ω–∏–π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9050021a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MentionAnalyzer:\n",
    "    \"\"\"–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —É–ø–æ–º–∏–Ω–∞–Ω–∏–π –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –±—Ä–µ–Ω–¥–∞\"\"\"\n",
    "    \n",
    "    # –û–±—â–∏–µ —Ç–µ–≥–∏ –¥–ª—è –≤—Å–µ—Ö –±—Ä–µ–Ω–¥–æ–≤\n",
    "    UNIVERSAL_TAGS = [\n",
    "        '–ö–∞—á–µ—Å—Ç–≤–æ –ø—Ä–æ–¥—É–∫—Ç–∞/—É—Å–ª—É–≥–∏',\n",
    "        '–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞–º–∏', \n",
    "        '–ñ–∞–ª–æ–±–∞/–ø—Ä–æ–±–ª–µ–º–∞',\n",
    "        '–ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π –æ—Ç–∑—ã–≤',\n",
    "        '–í–æ–ø—Ä–æ—Å/—Å–ø—Ä–∞–≤–∫–∞',\n",
    "        '–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è/—Å–æ–≤–µ—Ç',\n",
    "        '–¶–µ–Ω–∞/—Å—Ç–æ–∏–º–æ—Å—Ç—å',\n",
    "        '–°–µ—Ä–≤–∏—Å/–æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ',\n",
    "        '–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã',\n",
    "        '–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å/–∑–¥–æ—Ä–æ–≤—å–µ'\n",
    "    ]\n",
    "    \n",
    "    def __init__(self, config: Dict):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –±—Ä–µ–Ω–¥–∞\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.brand_name = config['name']\n",
    "        self.keywords = config['keywords']\n",
    "        self.risk_words = config['risk_words']\n",
    "        self.positive_words = config['positive_words']\n",
    "        self.competitors = config['competitors']\n",
    "        \n",
    "        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏\n",
    "        self.sentiment_analyzer = SentimentAnalyzer()\n",
    "        \n",
    "        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n",
    "        self.stats = {\n",
    "            'total': 0,\n",
    "            'relevant': 0,\n",
    "            'negative': 0,\n",
    "            'positive': 0,\n",
    "            'dangerous': 0\n",
    "        }\n",
    "        \n",
    "        print(f\"  –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è: {self.brand_name}\")\n",
    "    \n",
    "    def is_relevant(self, text: str) -> bool:\n",
    "        \"\"\"–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞\"\"\"\n",
    "        if not text or len(text.strip()) < 5:\n",
    "            return False\n",
    "        \n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤\n",
    "        for keyword in self.keywords:\n",
    "            if keyword in text_lower:\n",
    "                return True\n",
    "        \n",
    "        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —á–∞—Å—Ç–∏—á–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π\n",
    "        words = re.findall(r'\\b\\w+\\b', text_lower)\n",
    "        for word in words:\n",
    "            for keyword in self.keywords:\n",
    "                if keyword in word:\n",
    "                    return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def extract_tags(self, text: str) -> List[str]:\n",
    "        \"\"\"–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–≥–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞\"\"\"\n",
    "        tags = []\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # 1. –ö–∞—á–µ—Å—Ç–≤–æ –ø—Ä–æ–¥—É–∫—Ç–∞\n",
    "        quality_patterns = [r'–∫–∞—á–µ—Å—Ç–≤', r'–Ω–µ–∫–∞—á–µ—Å—Ç–≤–µ–Ω', r'–ø–ª–æ—Ö', r'—É–∂–∞—Å', r'–∫–æ—à–º–∞—Ä', r'–±–∞–¥—è–∂']\n",
    "        if any(re.search(pattern, text_lower) for pattern in quality_patterns):\n",
    "            tags.append('–ö–∞—á–µ—Å—Ç–≤–æ –ø—Ä–æ–¥—É–∫—Ç–∞/—É—Å–ª—É–≥–∏')\n",
    "        \n",
    "        # 2. –ñ–∞–ª–æ–±—ã\n",
    "        if re.search(r'–∂–∞–ª–æ–±|–Ω–µ–¥–æ–≤–æ–ª|–≤–æ–∑–º—É—â|–ø—Ä–æ—Ç–µ—Å—Ç|–ø—Ä–µ—Ç–µ–Ω–∑', text_lower):\n",
    "            tags.append('–ñ–∞–ª–æ–±–∞/–ø—Ä–æ–±–ª–µ–º–∞')\n",
    "        \n",
    "        # 3. –ü–æ–∑–∏—Ç–∏–≤–Ω—ã–µ –æ—Ç–∑—ã–≤—ã\n",
    "        if re.search(r'–æ—Ç–ª–∏—á–Ω|–ø—Ä–µ–∫—Ä–∞—Å–Ω|–∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω|—Ö–æ—Ä–æ—à|—Ä–µ–∫–æ–º–µ–Ω–¥—É—é|—Å–ø–∞—Å–∏–±–æ', text_lower):\n",
    "            tags.append('–ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π –æ—Ç–∑—ã–≤')\n",
    "        \n",
    "        # 4. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞–º–∏\n",
    "        if self.competitors and any(comp in text_lower for comp in self.competitors):\n",
    "            tags.append('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞–º–∏')\n",
    "        \n",
    "        # 5. –í–æ–ø—Ä–æ—Å—ã\n",
    "        if '?' in text or any(word in text_lower for word in ['–∫–∞–∫', '—á—Ç–æ', '–≥–¥–µ', '–ø–æ–¥—Å–∫–∞–∂–∏—Ç–µ']):\n",
    "            tags.append('–í–æ–ø—Ä–æ—Å/—Å–ø—Ä–∞–≤–∫–∞')\n",
    "        \n",
    "        # 6. –¶–µ–Ω–∞\n",
    "        if re.search(r'—Ü–µ–Ω|—Å—Ç–æ–∏–º–æ—Å—Ç|–¥–æ—Ä–æ–≥|–¥–µ—à–µ–≤', text_lower):\n",
    "            tags.append('–¶–µ–Ω–∞/—Å—Ç–æ–∏–º–æ—Å—Ç—å')\n",
    "        \n",
    "        # 7. –°–µ—Ä–≤–∏—Å\n",
    "        if re.search(r'—Å–µ—Ä–≤–∏—Å|–æ–±—Å–ª—É–∂–∏–≤–∞–Ω|–ø–µ—Ä—Å–æ–Ω–∞–ª|–∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü', text_lower):\n",
    "            tags.append('–°–µ—Ä–≤–∏—Å/–æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ')\n",
    "        \n",
    "        # 8. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏\n",
    "        if re.search(r'—Ä–µ–∫–æ–º–µ–Ω–¥|—Å–æ–≤–µ—Ç', text_lower):\n",
    "            tags.append('–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è/—Å–æ–≤–µ—Ç')\n",
    "        \n",
    "        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º 5 —Ç–µ–≥–∞–º–∏\n",
    "        unique_tags = []\n",
    "        for tag in tags:\n",
    "            if tag not in unique_tags:\n",
    "                unique_tags.append(tag)\n",
    "        \n",
    "        return unique_tags[:5]\n",
    "    \n",
    "    def analyze_text(self, text: str) -> Dict:\n",
    "        \"\"\"\n",
    "        –ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞\n",
    "        \n",
    "        Returns:\n",
    "            Dict: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞\n",
    "        \"\"\"\n",
    "        self.stats['total'] += 1\n",
    "        \n",
    "        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏\n",
    "        relevant = self.is_relevant(text)\n",
    "        if not relevant:\n",
    "            return {\n",
    "                'relevant': False,\n",
    "                'sentiment': '–ù–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ',\n",
    "                'tags': [],\n",
    "                'dangerous': False\n",
    "            }\n",
    "        \n",
    "        self.stats['relevant'] += 1\n",
    "        \n",
    "        # –ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏\n",
    "        sentiment_result = self.sentiment_analyzer.analyze(text, self.risk_words, self.positive_words)\n",
    "        sentiment = sentiment_result['sentiment']\n",
    "        \n",
    "        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏\n",
    "        if sentiment == '–ù–µ–≥–∞—Ç–∏–≤–Ω–∞—è':\n",
    "            self.stats['negative'] += 1\n",
    "        elif sentiment == '–ü–æ–∑–∏—Ç–∏–≤–Ω–∞—è':\n",
    "            self.stats['positive'] += 1\n",
    "        \n",
    "        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–≥–æ–≤\n",
    "        tags = self.extract_tags(text)\n",
    "        \n",
    "        # –û—Ü–µ–Ω–∫–∞ –æ–ø–∞—Å–Ω–æ—Å—Ç–∏\n",
    "        dangerous = self._is_dangerous(text, sentiment, tags)\n",
    "        if dangerous:\n",
    "            self.stats['dangerous'] += 1\n",
    "        \n",
    "        return {\n",
    "            'relevant': True,\n",
    "            'sentiment': sentiment,\n",
    "            'confidence': sentiment_result.get('confidence', 0.5),\n",
    "            'tags': tags,\n",
    "            'dangerous': dangerous,\n",
    "            'model': sentiment_result.get('model', 'basic')\n",
    "        }\n",
    "    \n",
    "    def _is_dangerous(self, text: str, sentiment: str, tags: List[str]) -> bool:\n",
    "        \"\"\"–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –¥–ª—è —Ä–µ–ø—É—Ç–∞—Ü–∏–∏\"\"\"\n",
    "        text_lower = text.lower()\n",
    "        danger_score = 0\n",
    "        \n",
    "        # 1. –ù–µ–≥–∞—Ç–∏–≤–Ω–∞—è —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å\n",
    "        if sentiment == '–ù–µ–≥–∞—Ç–∏–≤–Ω–∞—è':\n",
    "            danger_score += 2\n",
    "        \n",
    "        # 2. –ñ–∞–ª–æ–±–∞ –≤ —Ç–µ–≥–∞—Ö\n",
    "        if '–ñ–∞–ª–æ–±–∞/–ø—Ä–æ–±–ª–µ–º–∞' in tags:\n",
    "            danger_score += 2\n",
    "        \n",
    "        # 3. –†–∏—Å–∫-—Å–ª–æ–≤–∞\n",
    "        for risk_word in self.risk_words:\n",
    "            if risk_word in text_lower:\n",
    "                danger_score += 1\n",
    "        \n",
    "        # 4. –ü—Ä–∏–∑—ã–≤—ã –∫ –¥–µ–π—Å—Ç–≤–∏—é\n",
    "        if re.search(r'–Ω–µ\\s+–ø–æ–∫—É–ø–∞–π|–Ω–µ\\s+–ø–æ–ª—å–∑—É–π|–±–æ–π–∫–æ—Ç–∏—Ä|—Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–∏|–ø—Ä–µ–¥—É–ø—Ä–µ–¥–∏', text_lower):\n",
    "            danger_score += 2\n",
    "        \n",
    "        # –û–ø–∞—Å–Ω—ã–º —Å—á–∏—Ç–∞–µ—Ç—Å—è —Å–æ–æ–±—â–µ–Ω–∏–µ —Å 3+ –±–∞–ª–ª–∞–º–∏\n",
    "        return danger_score >= 3\n",
    "    \n",
    "    def print_stats(self):\n",
    "        \"\"\"–í—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∞–Ω–∞–ª–∏–∑–∞\"\"\"\n",
    "        print(f\"\\n  –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è {self.brand_name}:\")\n",
    "        print(f\"    –í—Å–µ–≥–æ —Ç–µ–∫—Å—Ç–æ–≤: {self.stats['total']}\")\n",
    "        \n",
    "        if self.stats['total'] > 0:\n",
    "            relevant_pct = (self.stats['relevant'] / self.stats['total']) * 100\n",
    "            print(f\"    –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö: {self.stats['relevant']} ({relevant_pct:.1f}%)\")\n",
    "            \n",
    "            if self.stats['relevant'] > 0:\n",
    "                negative_pct = (self.stats['negative'] / self.stats['relevant']) * 100\n",
    "                positive_pct = (self.stats['positive'] / self.stats['relevant']) * 100\n",
    "                \n",
    "                print(f\"    –ù–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö: {self.stats['negative']} ({negative_pct:.1f}%)\")\n",
    "                print(f\"    –ü–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö: {self.stats['positive']} ({positive_pct:.1f}%)\")\n",
    "                print(f\"    –û–ø–∞—Å–Ω—ã—Ö –¥–ª—è —Ä–µ–ø—É—Ç–∞—Ü–∏–∏: {self.stats['dangerous']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fc02cd-7daf-4b95-97da-69efe482136f",
   "metadata": {},
   "source": [
    "## –û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Ñ–∞–π–ª–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f67862d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileManager:\n",
    "    \"\"\"–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ñ–∞–π–ª–∞–º–∏ —Å –Ω—É–º–µ—Ä–∞—Ü–∏–µ–π\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def find_input_files() -> Dict[int, str]:\n",
    "        \"\"\"\n",
    "        –ü–æ–∏—Å–∫ –≤—Ö–æ–¥–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –ø–æ —à–∞–±–ª–æ–Ω—É date_reqN.—Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ\n",
    "        \n",
    "        Returns:\n",
    "            Dict[int, str]: –°–ª–æ–≤–∞—Ä—å {–Ω–æ–º–µ—Ä: –ø—É—Ç—å_–∫_—Ñ–∞–π–ª—É}\n",
    "        \"\"\"\n",
    "        files_found = {}\n",
    "        \n",
    "        # –®–∞–±–ª–æ–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞\n",
    "        patterns = [\n",
    "            'date_req[0-9]*.xls',\n",
    "            'date_req[0-9]*.xlsx', \n",
    "            'date_req[0-9]*.csv',\n",
    "            'date_req[0-9]*.txt'\n",
    "        ]\n",
    "        \n",
    "        print(\"\\n–ü–æ–∏—Å–∫ –≤—Ö–æ–¥–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤...\")\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            for file_path in glob.glob(pattern):\n",
    "                # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–æ–º–µ—Ä –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞\n",
    "                match = re.search(r'date_req(\\d+)', file_path, re.IGNORECASE)\n",
    "                if match:\n",
    "                    file_num = int(match.group(1))\n",
    "                    if file_num in BRAND_CONFIGS:\n",
    "                        files_found[file_num] = file_path\n",
    "                        print(f\"  ‚úì –ù–∞–π–¥–µ–Ω —Ñ–∞–π–ª –¥–ª—è {BRAND_CONFIGS[file_num]['name']}: {file_path}\")\n",
    "                    else:\n",
    "                        print(f\"  ‚ö† –§–∞–π–ª {file_path} –∏–º–µ–µ—Ç –Ω–æ–º–µ—Ä {file_num}, –Ω–æ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è —ç—Ç–æ–≥–æ –Ω–æ–º–µ—Ä–∞ –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞\")\n",
    "                else:\n",
    "                    # –§–∞–π–ª –±–µ–∑ –Ω–æ–º–µ—Ä–∞ (–ø—Ä–æ—Å—Ç–æ date_req)\n",
    "                    print(f\"  ‚ö† –§–∞–π–ª –±–µ–∑ –Ω–æ–º–µ—Ä–∞: {file_path}\")\n",
    "        \n",
    "        # –¢–∞–∫–∂–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∞–π–ª—ã —Å —è–≤–Ω—ã–º–∏ –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏\n",
    "        specific_files = {\n",
    "            'date_req_gpn.xls': 1,\n",
    "            'date_req_gpn.xlsx': 1,\n",
    "            'date_req_malyutka.xls': 2,\n",
    "            'date_req_malyutka.xlsx': 2,\n",
    "            'date_req_gazprom.xls': 1,\n",
    "            'date_req_gazprom.xlsx': 1\n",
    "        }\n",
    "        \n",
    "        for file_name, file_num in specific_files.items():\n",
    "            if os.path.exists(file_name) and file_num not in files_found:\n",
    "                files_found[file_num] = file_name\n",
    "                print(f\"  ‚úì –ù–∞–π–¥–µ–Ω —Ñ–∞–π–ª –ø–æ –∏–º–µ–Ω–∏ –¥–ª—è {BRAND_CONFIGS[file_num]['name']}: {file_name}\")\n",
    "        \n",
    "        if not files_found:\n",
    "            print(\"  ‚ö† –í—Ö–æ–¥–Ω—ã–µ —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã!\")\n",
    "            print(\"  –û–∂–∏–¥–∞—é—Ç—Å—è —Ñ–∞–π–ª—ã —Å –∏–º–µ–Ω–∞–º–∏:\")\n",
    "            print(\"    - date_req1.xls/date_req1.xlsx - –¥–ª—è –ì–∞–∑–ø—Ä–æ–º –Ω–µ—Ñ—Ç—å\")\n",
    "            print(\"    - date_req2.xls/date_req2.xlsx - –¥–ª—è –ú–∞–ª—é—Ç–∫–∞\")\n",
    "            print(\"    - date_req_gpn.xls - –¥–ª—è –ì–∞–∑–ø—Ä–æ–º –Ω–µ—Ñ—Ç—å\")\n",
    "            print(\"    - date_req_malyutka.xls - –¥–ª—è –ú–∞–ª—é—Ç–∫–∞\")\n",
    "        \n",
    "        return files_found\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_output_filename(file_num: int, input_path: str) -> str:\n",
    "        \"\"\"\n",
    "        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–º–µ–Ω–∏ –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞\n",
    "        \n",
    "        Args:\n",
    "            file_num: –ù–æ–º–µ—Ä —Ñ–∞–π–ª–∞ (1 –∏–ª–∏ 2)\n",
    "            input_path: –ü—É—Ç—å –∫ –≤—Ö–æ–¥–Ω–æ–º—É —Ñ–∞–π–ª—É\n",
    "            \n",
    "        Returns:\n",
    "            str: –ò–º—è –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞\n",
    "        \"\"\"\n",
    "        brand_name = BRAND_CONFIGS[file_num]['output_prefix']\n",
    "        input_name = os.path.basename(input_path)\n",
    "        \n",
    "        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ\n",
    "        if input_path.endswith('.xlsx'):\n",
    "            output_ext = '.xlsx'\n",
    "        elif input_path.endswith('.xls'):\n",
    "            output_ext = '.xlsx'  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ\n",
    "        elif input_path.endswith('.csv'):\n",
    "            output_ext = '.csv'\n",
    "        else:\n",
    "            output_ext = '.xlsx'\n",
    "        \n",
    "        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–º—è —Ñ–∞–π–ª–∞\n",
    "        timestamp = pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')\n",
    "        \n",
    "        # –í–∞—Ä–∏–∞–Ω—Ç—ã –∏–º–µ–Ω:\n",
    "        # 1. –° –Ω–æ–º–µ—Ä–æ–º: date_res1.xlsx\n",
    "        # 2. –° –ø—Ä–µ—Ñ–∏–∫—Å–æ–º –±—Ä–µ–Ω–¥–∞: –ì–ü–ù_—Ä–µ–∑—É–ª—å—Ç–∞—Ç_20241215.xlsx\n",
    "        # 3. –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ: date_res1_–ì–ü–ù.xlsx\n",
    "        \n",
    "        output_names = [\n",
    "            f\"date_res{file_num}{output_ext}\",  # –ü–æ –Ω–æ–º–µ—Ä—É\n",
    "            f\"{brand_name}_—Ä–µ–∑—É–ª—å—Ç–∞—Ç_{timestamp}{output_ext}\",  # –° –±—Ä–µ–Ω–¥–æ–º –∏ –≤—Ä–µ–º–µ–Ω–µ–º\n",
    "            f\"date_res{file_num}_{brand_name}{output_ext}\",  # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ\n",
    "            f\"—Ä–µ–∑—É–ª—å—Ç–∞—Ç_{brand_name}{output_ext}\"  # –ü—Ä–æ—Å—Ç–æ–µ\n",
    "        ]\n",
    "        \n",
    "        # –í—ã–±–∏—Ä–∞–µ–º –ø–µ—Ä–≤–æ–µ —Å–≤–æ–±–æ–¥–Ω–æ–µ –∏–º—è\n",
    "        for name in output_names:\n",
    "            if not os.path.exists(name):\n",
    "                return name\n",
    "        \n",
    "        # –ï—Å–ª–∏ –≤—Å–µ –∏–º–µ–Ω–∞ –∑–∞–Ω—è—Ç—ã, –¥–æ–±–∞–≤–ª—è–µ–º –Ω–æ–º–µ—Ä\n",
    "        counter = 1\n",
    "        while True:\n",
    "            name = f\"date_res{file_num}_{brand_name}_{counter}{output_ext}\"\n",
    "            if not os.path.exists(name):\n",
    "                return name\n",
    "            counter += 1\n",
    "    \n",
    "    @staticmethod\n",
    "    def read_file(file_path: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        –ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º —Ñ–æ—Ä–º–∞—Ç–∞\n",
    "        \n",
    "        Args:\n",
    "            file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É\n",
    "            \n",
    "        Returns:\n",
    "            pd.DataFrame: –ü—Ä–æ—á–∏—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
    "        \"\"\"\n",
    "        print(f\"  –ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞: {file_path}\")\n",
    "        \n",
    "        try:\n",
    "            if file_path.endswith('.xlsx'):\n",
    "                df = pd.read_excel(file_path, engine='openpyxl')\n",
    "            elif file_path.endswith('.xls'):\n",
    "                # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –¥–≤–∏–∂–∫–∏ –¥–ª—è —Å—Ç–∞—Ä—ã—Ö .xls —Ñ–∞–π–ª–æ–≤\n",
    "                try:\n",
    "                    df = pd.read_excel(file_path, engine='xlrd')\n",
    "                except:\n",
    "                    df = pd.read_excel(file_path, engine='openpyxl')\n",
    "            elif file_path.endswith('.csv'):\n",
    "                # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –∫–æ–¥–∏—Ä–æ–≤–∫–∏ –¥–ª—è CSV\n",
    "                encodings = ['utf-8', 'cp1251', 'windows-1251', 'latin1']\n",
    "                for encoding in encodings:\n",
    "                    try:\n",
    "                        df = pd.read_csv(file_path, encoding=encoding, sep=None, engine='python')\n",
    "                        print(f\"    –ü—Ä–æ—á–∏—Ç–∞–Ω–æ —Å –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π: {encoding}\")\n",
    "                        break\n",
    "                    except:\n",
    "                        continue\n",
    "                else:\n",
    "                    raise ValueError(\"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å CSV —Ñ–∞–π–ª\")\n",
    "            else:\n",
    "                raise ValueError(f\"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞: {file_path}\")\n",
    "            \n",
    "            print(f\"  ‚úì –£—Å–ø–µ—à–Ω–æ –ø—Ä–æ—á–∏—Ç–∞–Ω–æ {len(df)} —Å—Ç—Ä–æ–∫\")\n",
    "            \n",
    "            # –ò—â–µ–º –∫–æ–ª–æ–Ω–∫—É —Å —Ç–µ–∫—Å—Ç–æ–º\n",
    "            text_columns = [col for col in df.columns if any(word in col.lower() \n",
    "                           for word in ['—Ç–µ–∫—Å—Ç', 'text', '—Å–æ–æ–±—â–µ–Ω–∏–µ', 'message', '–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π'])]\n",
    "            \n",
    "            if text_columns:\n",
    "                if '–¢–µ–∫—Å—Ç' not in df.columns and text_columns[0] != '–¢–µ–∫—Å—Ç':\n",
    "                    df = df.rename(columns={text_columns[0]: '–¢–µ–∫—Å—Ç'})\n",
    "                    print(f\"  ‚úì –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∞ –∫–æ–ª–æ–Ω–∫–∞ '{text_columns[0]}' –≤ '–¢–µ–∫—Å—Ç'\")\n",
    "            elif '–¢–µ–∫—Å—Ç' not in df.columns and len(df.columns) > 0:\n",
    "                # –ï—Å–ª–∏ –Ω–µ—Ç —è–≤–Ω–æ–π –∫–æ–ª–æ–Ω–∫–∏ —Å —Ç–µ–∫—Å—Ç–æ–º, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—É—é –∫–æ–ª–æ–Ω–∫—É\n",
    "                first_col = df.columns[0]\n",
    "                df = df.rename(columns={first_col: '–¢–µ–∫—Å—Ç'})\n",
    "                print(f\"  ‚úì –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—É—é –∫–æ–ª–æ–Ω–∫—É '{first_col}' –∫–∞–∫ '–¢–µ–∫—Å—Ç'\")\n",
    "            \n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚úó –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞: {e}\")\n",
    "            # –°–æ–∑–¥–∞–µ–º –ø—É—Å—Ç–æ–π DataFrame —Å –∫–æ–ª–æ–Ω–∫–æ–π –¢–µ–∫—Å—Ç\n",
    "            return pd.DataFrame(columns=['–¢–µ–∫—Å—Ç'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5eab64-986d-47f2-b7c7-f08d1dc27461",
   "metadata": {},
   "source": [
    "## –ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è –±—Ä–µ–Ω–¥–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "502d8571",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrandAnalyzer:\n",
    "    \"\"\"–û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –±—Ä–µ–Ω–¥–æ–≤\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.file_manager = FileManager()\n",
    "        self.processed_files = []\n",
    "    \n",
    "    def process_brand(self, file_num: int, input_path: str) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ –±—Ä–µ–Ω–¥–∞\n",
    "        \n",
    "        Args:\n",
    "            file_num: –ù–æ–º–µ—Ä —Ñ–∞–π–ª–∞ (1 –∏–ª–∏ 2)\n",
    "            input_path: –ü—É—Ç—å –∫ –≤—Ö–æ–¥–Ω–æ–º—É —Ñ–∞–π–ª—É\n",
    "            \n",
    "        Returns:\n",
    "            Optional[str]: –ü—É—Ç—å –∫ –≤—ã—Ö–æ–¥–Ω–æ–º—É —Ñ–∞–π–ª—É –∏–ª–∏ None –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞ {file_num}: {BRAND_CONFIGS[file_num]['name']}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏\n",
    "        config = BRAND_CONFIGS[file_num]\n",
    "        \n",
    "        # –ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞\n",
    "        df = self.file_manager.read_file(input_path)\n",
    "        \n",
    "        if df.empty or '–¢–µ–∫—Å—Ç' not in df.columns:\n",
    "            print(f\"  ‚úó –§–∞–π–ª –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –¥–∞–Ω–Ω—ã—Ö –∏–ª–∏ –∫–æ–ª–æ–Ω–∫–∏ '–¢–µ–∫—Å—Ç'\")\n",
    "            return None\n",
    "        \n",
    "        # –°–æ–∑–¥–∞–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞\n",
    "        analyzer = MentionAnalyzer(config)\n",
    "        \n",
    "        # –ê–Ω–∞–ª–∏–∑ –∫–∞–∂–¥–æ–≥–æ —Ç–µ–∫—Å—Ç–∞\n",
    "        print(f\"  –ù–∞—á–∞–ª–æ –∞–Ω–∞–ª–∏–∑–∞ {len(df)} —Å—Ç—Ä–æ–∫...\")\n",
    "        \n",
    "        results = []\n",
    "        for idx, row in df.iterrows():\n",
    "            text = str(row['–¢–µ–∫—Å—Ç']) if pd.notna(row['–¢–µ–∫—Å—Ç']) else ''\n",
    "            \n",
    "            # –ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞\n",
    "            analysis = analyzer.analyze_text(text)\n",
    "            \n",
    "            # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞\n",
    "            result = {\n",
    "                '–°–æ–æ–±—â–µ–Ω–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ?': '–î–∞' if analysis['relevant'] else '–ù–µ—Ç',\n",
    "                '–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å': analysis['sentiment'],\n",
    "                '–û–ø–∞—Å–Ω–æ –¥–ª—è —Ä–µ–ø—É—Ç–∞—Ü–∏–∏': '–î–∞' if analysis['dangerous'] else '–ù–µ—Ç',\n",
    "                '–ú–æ–¥–µ–ª—å –∞–Ω–∞–ª–∏–∑–∞': analysis.get('model', 'basic'),\n",
    "                '–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å': f\"{analysis.get('confidence', 0.5):.2f}\"\n",
    "            }\n",
    "            \n",
    "            # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–≥–∏\n",
    "            tags = analysis['tags']\n",
    "            for i in range(1, 6):\n",
    "                result[f'–¢–µ–≥ {i}'] = tags[i-1] if i <= len(tags) else ''\n",
    "            \n",
    "            results.append(result)\n",
    "            \n",
    "            # –ü—Ä–æ–≥—Ä–µ—Å—Å\n",
    "            if (idx + 1) % 50 == 0:\n",
    "                print(f\"    –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {idx + 1}/{len(df)} —Å—Ç—Ä–æ–∫...\")\n",
    "        \n",
    "        # –°–æ–∑–¥–∞–Ω–∏–µ DataFrame —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏\n",
    "        results_df = pd.DataFrame(results)\n",
    "        \n",
    "        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –∏—Å—Ö–æ–¥–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏\n",
    "        output_df = pd.concat([df, results_df], axis=1)\n",
    "        \n",
    "        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–º–µ–Ω–∏ –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞\n",
    "        output_path = self.file_manager.generate_output_filename(file_num, input_path)\n",
    "        \n",
    "        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "        try:\n",
    "            if output_path.endswith('.xlsx'):\n",
    "                output_df.to_excel(output_path, index=False, engine='openpyxl')\n",
    "            elif output_path.endswith('.csv'):\n",
    "                output_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "            else:\n",
    "                output_path = output_path + '.xlsx'\n",
    "                output_df.to_excel(output_path, index=False, engine='openpyxl')\n",
    "            \n",
    "            print(f\"  ‚úì –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {output_path}\")\n",
    "            \n",
    "            # –í—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏\n",
    "            analyzer.print_stats()\n",
    "            \n",
    "            self.processed_files.append({\n",
    "                'brand': config['name'],\n",
    "                'input': input_path,\n",
    "                'output': output_path,\n",
    "                'stats': analyzer.stats\n",
    "            })\n",
    "            \n",
    "            return output_path\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚úó –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–∞: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def process_all_files(self):\n",
    "        \"\"\"–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤\"\"\"\n",
    "        # –ü–æ–∏—Å–∫ —Ñ–∞–π–ª–æ–≤\n",
    "        files = self.file_manager.find_input_files()\n",
    "        \n",
    "        if not files:\n",
    "            print(\"\\n‚ö† –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏!\")\n",
    "            print(\"  –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏...\")\n",
    "            self._create_test_files()\n",
    "            files = self.file_manager.find_input_files()\n",
    "        \n",
    "        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –Ω–æ–º–µ—Ä—É —Ñ–∞–π–ª–∞\n",
    "        sorted_files = sorted(files.items(), key=lambda x: x[0])\n",
    "        \n",
    "        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–∂–¥–æ–≥–æ —Ñ–∞–π–ª–∞\n",
    "        results = []\n",
    "        for file_num, file_path in sorted_files:\n",
    "            if file_num in BRAND_CONFIGS:\n",
    "                result_path = self.process_brand(file_num, file_path)\n",
    "                if result_path:\n",
    "                    results.append((file_num, result_path))\n",
    "            else:\n",
    "                print(f\"\\n‚ö† –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ñ–∞–π–ª {file_path}: –Ω–æ–º–µ—Ä {file_num} –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è\")\n",
    "        \n",
    "        # –í—ã–≤–æ–¥ –∏—Ç–æ–≥–æ–≤\n",
    "        self._print_summary(results)\n",
    "    \n",
    "    def _create_test_files(self):\n",
    "        \"\"\"–°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤ –µ—Å–ª–∏ –Ω–µ—Ç –≤—Ö–æ–¥–Ω—ã—Ö\"\"\"\n",
    "        test_data_gpn = [\n",
    "            \"–ê–ó–° –ì–∞–∑–ø—Ä–æ–º –Ω–∞ –©–µ—Ä–±–∞–∫–æ–≤–∞ 2. –í–∏–¥–∏–º–æ –æ–ø—è—Ç—å –Ω–∞—á–∞–ª–∏ –±–∞–¥—è–∂–∏—Ç—å –±–µ–Ω–∑–∏–Ω... –î–≤–µ –Ω–µ–¥–µ–ª–∏ –Ω–∞–∑–∞–¥ –∑–∞–ª–∏–ª–∏ –ø–æ–ª–Ω—ã–π –±–∞–∫ 95–≥–æ.\",\n",
    "            \"–†–µ–±—è—Ç–∞ –Ω—É–∂–Ω–∞ –ø–æ–º–æ—â—å, —á–µ–ª–æ–≤–µ–∫ –æ—Å—Ç–∞–ª—Å—è –±–µ–∑ –¥–µ–Ω–µ–≥, —Å—Ç–æ–∏—Ç –Ω–∞ –∑–∞–ø—Ä–∞–≤–∫–µ –ì–∞–∑–ø—Ä–æ–º–Ω–µ—Ñ—Ç—å –Ω–∞ —É–ª–∏—Ü–µ –õ–µ–Ω–∏–Ω–∞ 129.\",\n",
    "            \"–£ —Ç–µ–±—è –¥–∞–∂–µ –Ω–∞ –∫—Ä—ã—à–∫–µ –±–∞–∫–∞ –Ω–∞–ø–∏—Å–∞–Ω–æ, —á—Ç–æ –º–∏–Ω–∏–º—É–º 95. –Ø –≤ —Å–≤–æ–π —Ñ—Ñ3 –ª—å—é –ê–ò-100 –Ω–∞ –≥–∞–∑–ø—Ä–æ–º–µ, –æ—á–µ–Ω—å —Ö–æ—Ä–æ—à–æ –º–∞—à–∏–Ω–µ, —Ä–∞—Å—Ö–æ–¥ 8.3\",\n",
    "            \"—Å–º–µ—Ö, –≥–∞–∑–ø—Ä–æ–º–Ω–µ—Ñ—Ç—å —Å–∞–º —Å–µ–±—è –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞ –≤—à–∏–≤–æ—Å—Ç—å) —ç—Ç–æ –∫–∞–∫ –∫–æ—Ä—Ä—É–º–ø–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ª—é–¥–∏ –≤ –æ—Ç–¥–µ–ª–µ –±–æ—Ä—å–±—ã —Å –∫–æ—Ä—Ä—É–ø—Ü–∏–µ–π)\",\n",
    "            \"–•–æ—Ä–æ—à–µ–µ —Ç–æ–ø–ª–∏–≤–æ. –•–æ—Ç—è –∫–∞—á–µ—Å—Ç–≤–æ –±–µ–Ω–∑–∏–Ω–∞ –∂–∞–ª—É—é—Ç—Å—è.\",\n",
    "        ]\n",
    "        \n",
    "        test_data_malyutka = [\n",
    "            \"–° 3 –Ω–µ–¥–µ–ª—å –ø–æ–¥–∫–∞—Ä–º–ª–∏–≤–∞—é —Å–º–µ—Å—å—é, –Ω–∞—á–∞–ª–∏ —Å –º–∞–ª—é—Ç–∫–∏, –Ω–∞—á–∞–ª–∏—Å—å –∑–∞–ø–æ—Ä—ã. –ù–∞—Å –ø–æ—Å–æ–≤–µ—Ç–æ–≤–∞–ª–∏ –Ω—É—Ç—Ä–∏–ª–∞–∫.\",\n",
    "            \"–ú–æ–π —Å—ã–Ω –ø—Ä–æ—Å—Ç–æ –æ–±–æ–∂–∞–µ—Ç –¥–µ—Ç—Å–∫–æ–µ –º–æ–ª–æ—á–∫–æ –ú–∞–ª—é—Ç–∫–∞! –í—ã–ø–∏–≤–∞–µ—Ç –ø–æ —Ü–µ–ª–æ–π –±—É—Ç—ã–ª–æ—á–∫–µ –ø–µ—Ä–µ–¥ —Å–Ω–æ–º.\",\n",
    "            \"–ú–∞–ª—é—Ç–∫–∞ –æ—á–µ–Ω—å –≤–∫—É—Å–Ω—ã–µ –∫–∞—à–∏, –º–æ–∏ –æ–±–µ –µ–ª–∏ —Ö–æ—Ä–æ—à–æ –∏–º–µ–Ω–Ω–æ –≥—Ä–µ—á–Ω–µ–≤—É—é. –û–Ω–∏ –ø—Ä—è–º–æ –ø–∞—Ö–Ω—É—Ç –≤–∫—É—Å–Ω–æ.\",\n",
    "            \"–ö—Ä–∏—Å—Ç–∏–Ω–∞: –º–∞–ª—é—Ç–∫–∞ —Å–∞–º–∞—è –ø–ª–æ—Ö–∞—è –∏–∑ –≤—Å–µ—Ö —Å–º–µ—Å–µ–π —á—Ç–æ –µ—Å—Ç—å –Ω–∞ –ø—Ä–∏–ª–∞–≤–∫–∞—Ö –†–æ—Å—Å–∏–∏, –ø–æ —Å–æ—Å—Ç–∞–≤—É.\",\n",
    "            \"–†–∞—Å—Å–∫–∞–∂–∏—Ç–µ –ø–æ–∂–∞–ª—É–π—Å—Ç–∞ –ø—Ä–æ —Å–º–µ—Å—å ¬´–º–∞–ª—é—Ç–∫–∞¬ª –æ—Ç 0, –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∏?\",\n",
    "        ]\n",
    "        \n",
    "        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
    "        pd.DataFrame({'–¢–µ–∫—Å—Ç': test_data_gpn}).to_excel('date_req1.xlsx', index=False)\n",
    "        pd.DataFrame({'–¢–µ–∫—Å—Ç': test_data_malyutka}).to_excel('date_req2.xlsx', index=False)\n",
    "        \n",
    "        print(\"  ‚úì –°–æ–∑–¥–∞–Ω—ã —Ç–µ—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã: date_req1.xlsx –∏ date_req2.xlsx\")\n",
    "    \n",
    "    def _print_summary(self, results: List[Tuple[int, str]]):\n",
    "        \"\"\"–í—ã–≤–æ–¥ –∏—Ç–æ–≥–æ–≤–æ–≥–æ –æ—Ç—á–µ—Ç–∞\"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"–ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        if not results:\n",
    "            print(\"  –ù–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞\")\n",
    "            return\n",
    "        \n",
    "        print(f\"  –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(results)}\")\n",
    "        print()\n",
    "        \n",
    "        total_stats = {\n",
    "            'total_texts': 0,\n",
    "            'relevant_texts': 0,\n",
    "            'negative_texts': 0,\n",
    "            'dangerous_texts': 0\n",
    "        }\n",
    "        \n",
    "        for file_num, output_path in results:\n",
    "            brand_name = BRAND_CONFIGS[file_num]['name']\n",
    "            print(f\"  {brand_name}:\")\n",
    "            print(f\"    –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {output_path}\")\n",
    "            \n",
    "            # –ù–∞—Ö–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–ª—è —ç—Ç–æ–≥–æ —Ñ–∞–π–ª–∞\n",
    "            for processed in self.processed_files:\n",
    "                if processed['output'] == output_path:\n",
    "                    stats = processed['stats']\n",
    "                    print(f\"    –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ —Ç–µ–∫—Å—Ç–æ–≤: {stats['total']}\")\n",
    "                    print(f\"    –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö: {stats['relevant']}\")\n",
    "                    print(f\"    –ù–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö: {stats['negative']}\")\n",
    "                    print(f\"    –û–ø–∞—Å–Ω—ã—Ö –¥–ª—è —Ä–µ–ø—É—Ç–∞—Ü–∏–∏: {stats['dangerous']}\")\n",
    "                    \n",
    "                    # –°—É–º–º–∏—Ä—É–µ–º –æ–±—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É\n",
    "                    total_stats['total_texts'] += stats['total']\n",
    "                    total_stats['relevant_texts'] += stats['relevant']\n",
    "                    total_stats['negative_texts'] += stats['negative']\n",
    "                    total_stats['dangerous_texts'] += stats['dangerous']\n",
    "                    break\n",
    "        \n",
    "        # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n",
    "        print(f\"\\n  –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:\")\n",
    "        print(f\"    –í—Å–µ–≥–æ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ —Ç–µ–∫—Å—Ç–æ–≤: {total_stats['total_texts']}\")\n",
    "        if total_stats['total_texts'] > 0:\n",
    "            relevant_pct = (total_stats['relevant_texts'] / total_stats['total_texts']) * 100\n",
    "            print(f\"    –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤: {total_stats['relevant_texts']} ({relevant_pct:.1f}%)\")\n",
    "            \n",
    "            if total_stats['relevant_texts'] > 0:\n",
    "                negative_pct = (total_stats['negative_texts'] / total_stats['relevant_texts']) * 100\n",
    "                print(f\"    –ù–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö —É–ø–æ–º–∏–Ω–∞–Ω–∏–π: {total_stats['negative_texts']} ({negative_pct:.1f}%)\")\n",
    "                print(f\"    –û–ø–∞—Å–Ω—ã—Ö –¥–ª—è —Ä–µ–ø—É—Ç–∞—Ü–∏–∏: {total_stats['dangerous_texts']}\")\n",
    "        \n",
    "        print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d66824-634c-47ac-9cb1-10433d5a6fe5",
   "metadata": {},
   "source": [
    "## –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e267d4db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "–ó–∞–ø—É—Å–∫ –∞–Ω–∞–ª–∏–∑–∞ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –±—Ä–µ–Ω–¥–æ–≤...\n",
      "–û–∂–∏–¥–∞—é—Ç—Å—è —Ñ–∞–π–ª—ã —Å –Ω–æ–º–µ—Ä–∞–º–∏:\n",
      "  date_req1.xls/xlsx - –¥–ª—è –ì–∞–∑–ø—Ä–æ–º –Ω–µ—Ñ—Ç—å\n",
      "  date_req2.xls/xlsx - –¥–ª—è –ú–∞–ª—é—Ç–∫–∞\n",
      "  –∏–ª–∏ date_req_gpn.xls, date_req_malyutka.xls\n",
      "----------------------------------------------------------------------\n",
      "‚úì Pandas –¥–æ—Å—Ç—É–ø–µ–Ω\n",
      "‚úì Dostoevsky –¥–æ—Å—Ç—É–ø–µ–Ω\n",
      "\n",
      "–ü–æ–∏—Å–∫ –≤—Ö–æ–¥–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤...\n",
      "  ‚úì –ù–∞–π–¥–µ–Ω —Ñ–∞–π–ª –¥–ª—è –ì–∞–∑–ø—Ä–æ–º –Ω–µ—Ñ—Ç—å: date_req1.xls\n",
      "  ‚úì –ù–∞–π–¥–µ–Ω —Ñ–∞–π–ª –¥–ª—è –ú–∞–ª—é—Ç–∫–∞: date_req2.xls\n",
      "\n",
      "============================================================\n",
      "–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞ 1: –ì–∞–∑–ø—Ä–æ–º –Ω–µ—Ñ—Ç—å\n",
      "============================================================\n",
      "  –ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞: date_req1.xls\n",
      "  ‚úì –£—Å–ø–µ—à–Ω–æ –ø—Ä–æ—á–∏—Ç–∞–Ω–æ 11 —Å—Ç—Ä–æ–∫\n",
      "  –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏...\n",
      "  ‚úó Dostoevsky –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª—Å—è: No module named 'fasttext'\n",
      "  –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º\n",
      "  –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è: –ì–∞–∑–ø—Ä–æ–º –Ω–µ—Ñ—Ç—å\n",
      "  –ù–∞—á–∞–ª–æ –∞–Ω–∞–ª–∏–∑–∞ 11 —Å—Ç—Ä–æ–∫...\n",
      "  ‚úì –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: –ì–ü–ù_—Ä–µ–∑—É–ª—å—Ç–∞—Ç_20260208_134650.xlsx\n",
      "\n",
      "  –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è –ì–∞–∑–ø—Ä–æ–º –Ω–µ—Ñ—Ç—å:\n",
      "    –í—Å–µ–≥–æ —Ç–µ–∫—Å—Ç–æ–≤: 11\n",
      "    –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö: 10 (90.9%)\n",
      "    –ù–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö: 3 (30.0%)\n",
      "    –ü–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö: 2 (20.0%)\n",
      "    –û–ø–∞—Å–Ω—ã—Ö –¥–ª—è —Ä–µ–ø—É—Ç–∞—Ü–∏–∏: 3\n",
      "\n",
      "============================================================\n",
      "–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞ 2: –ú–∞–ª—é—Ç–∫–∞\n",
      "============================================================\n",
      "  –ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞: date_req2.xls\n",
      "  ‚úì –£—Å–ø–µ—à–Ω–æ –ø—Ä–æ—á–∏—Ç–∞–Ω–æ 11 —Å—Ç—Ä–æ–∫\n",
      "  –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏...\n",
      "  ‚úó Dostoevsky –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª—Å—è: No module named 'fasttext'\n",
      "  –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º\n",
      "  –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è: –ú–∞–ª—é—Ç–∫–∞\n",
      "  –ù–∞—á–∞–ª–æ –∞–Ω–∞–ª–∏–∑–∞ 11 —Å—Ç—Ä–æ–∫...\n",
      "  ‚úì –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: date_res2.xlsx\n",
      "\n",
      "  –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è –ú–∞–ª—é—Ç–∫–∞:\n",
      "    –í—Å–µ–≥–æ —Ç–µ–∫—Å—Ç–æ–≤: 11\n",
      "    –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö: 11 (100.0%)\n",
      "    –ù–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö: 3 (27.3%)\n",
      "    –ü–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö: 4 (36.4%)\n",
      "    –û–ø–∞—Å–Ω—ã—Ö –¥–ª—è —Ä–µ–ø—É—Ç–∞—Ü–∏–∏: 3\n",
      "\n",
      "======================================================================\n",
      "–ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢\n",
      "======================================================================\n",
      "  –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: 2\n",
      "\n",
      "  –ì–∞–∑–ø—Ä–æ–º –Ω–µ—Ñ—Ç—å:\n",
      "    –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: –ì–ü–ù_—Ä–µ–∑—É–ª—å—Ç–∞—Ç_20260208_134650.xlsx\n",
      "    –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ —Ç–µ–∫—Å—Ç–æ–≤: 11\n",
      "    –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö: 10\n",
      "    –ù–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö: 3\n",
      "    –û–ø–∞—Å–Ω—ã—Ö –¥–ª—è —Ä–µ–ø—É—Ç–∞—Ü–∏–∏: 3\n",
      "  –ú–∞–ª—é—Ç–∫–∞:\n",
      "    –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: date_res2.xlsx\n",
      "    –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ —Ç–µ–∫—Å—Ç–æ–≤: 11\n",
      "    –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö: 11\n",
      "    –ù–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö: 3\n",
      "    –û–ø–∞—Å–Ω—ã—Ö –¥–ª—è —Ä–µ–ø—É—Ç–∞—Ü–∏–∏: 3\n",
      "\n",
      "  –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:\n",
      "    –í—Å–µ–≥–æ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ —Ç–µ–∫—Å—Ç–æ–≤: 22\n",
      "    –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤: 21 (95.5%)\n",
      "    –ù–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö —É–ø–æ–º–∏–Ω–∞–Ω–∏–π: 6 (28.6%)\n",
      "    –û–ø–∞—Å–Ω—ã—Ö –¥–ª—è —Ä–µ–ø—É—Ç–∞—Ü–∏–∏: 6\n",
      "======================================================================\n",
      "\n",
      "–ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è\"\"\"\n",
    "    print(\"\\n–ó–∞–ø—É—Å–∫ –∞–Ω–∞–ª–∏–∑–∞ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –±—Ä–µ–Ω–¥–æ–≤...\")\n",
    "    print(\"–û–∂–∏–¥–∞—é—Ç—Å—è —Ñ–∞–π–ª—ã —Å –Ω–æ–º–µ—Ä–∞–º–∏:\")\n",
    "    print(\"  date_req1.xls/xlsx - –¥–ª—è –ì–∞–∑–ø—Ä–æ–º –Ω–µ—Ñ—Ç—å\")\n",
    "    print(\"  date_req2.xls/xlsx - –¥–ª—è –ú–∞–ª—é—Ç–∫–∞\")\n",
    "    print(\"  –∏–ª–∏ date_req_gpn.xls, date_req_malyutka.xls\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π\n",
    "    try:\n",
    "        import pandas as pd\n",
    "        print(\"‚úì Pandas –¥–æ—Å—Ç—É–ø–µ–Ω\")\n",
    "    except ImportError:\n",
    "        print(\"‚úó Pandas –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install pandas openpyxl xlrd\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        import dostoevsky\n",
    "        print(\"‚úì Dostoevsky –¥–æ—Å—Ç—É–ø–µ–Ω\")\n",
    "    except ImportError:\n",
    "        print(\"‚ö† Dostoevsky –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –ë—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –±–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑.\")\n",
    "        print(\"  –î–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install dostoevsky fasttext\")\n",
    "    \n",
    "    # –ó–∞–ø—É—Å–∫ –∞–Ω–∞–ª–∏–∑–∞\n",
    "    analyzer = BrandAnalyzer()\n",
    "    analyzer.process_all_files()\n",
    "    \n",
    "    print(\"\\n–ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee9ed1f-9d33-4fcf-99dc-81075269c2db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42a4dc5-7b7c-421e-9a3b-42ee35720197",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
