{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30222b32",
   "metadata": {},
   "source": [
    "# –û—Å–Ω–æ–≤–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n",
    "!pip install pybind11\n",
    "!pip install pandas openpyxl numpy\n",
    "# –î–ª—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å transformers (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)\n",
    "!pip install transformers torch\n",
    "\n",
    "# –î–ª—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π\n",
    "!pip install scikit-learn \n",
    "!pip install gensim \n",
    "\n",
    "# –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞\n",
    "!pip install razdel\n",
    "!pip install pymorphy3\n",
    "\n",
    "# –î–ª—è Dostoevsky –Ω—É–∂–Ω–∞ –º–æ–¥–µ–ª—å\n",
    "!pip install dostoevsky --no-deps\n",
    "!pip install fasttext-wheel==0.9.2\n",
    "DOSTOEVSKY_AVAILABLE = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f94f89e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —É–ø–æ–º–∏–Ω–∞–Ω–∏–π –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ —Ä–µ–ø—É—Ç–∞—Ü–∏–∏ –±—Ä–µ–Ω–¥–æ–≤\n",
    "–ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "import chardet\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ —É—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫\n",
    "def install_missing_packages():\n",
    "    \"\"\"–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏—Ö –ø–∞–∫–µ—Ç–æ–≤\"\"\"\n",
    "    required_packages = {\n",
    "        'pandas': 'pandas',\n",
    "        'numpy': 'numpy',\n",
    "        'openpyxl': 'openpyxl',\n",
    "        'xlrd': 'xlrd>=1.2.0',\n",
    "        'chardet': 'chardet',\n",
    "        'tqdm': 'tqdm'\n",
    "    }\n",
    "    \n",
    "    missing = []\n",
    "    for package, install_name in required_packages.items():\n",
    "        try:\n",
    "            __import__(package)\n",
    "        except ImportError:\n",
    "            missing.append(install_name)\n",
    "    \n",
    "    if missing:\n",
    "        print(f\"–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –ø–∞–∫–µ—Ç—ã: {', '.join(missing)}\")\n",
    "        response = input(\"–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∏—Ö –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏? (y/n): \")\n",
    "        if response.lower() == 'y':\n",
    "            import subprocess\n",
    "            try:\n",
    "                subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\"] + missing)\n",
    "                print(\"–ü–∞–∫–µ—Ç—ã —É—Å–ø–µ—à–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã!\")\n",
    "            except Exception as e:\n",
    "                print(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ —É—Å—Ç–∞–Ω–æ–≤–∫–µ –ø–∞–∫–µ—Ç–æ–≤: {e}\")\n",
    "                print(\"–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∏—Ö –≤—Ä—É—á–Ω—É—é:\")\n",
    "                print(f\"pip install {' '.join(missing)}\")\n",
    "        else:\n",
    "            print(\"–ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ —É—Å—Ç–∞–Ω–æ–≤–∫–∏...\")\n",
    "\n",
    "# –í—ã–∑—ã–≤–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É –ø–∞–∫–µ—Ç–æ–≤\n",
    "install_missing_packages()\n",
    "\n",
    "\n",
    "class ModelDownloader:\n",
    "    \"\"\"–ö–ª–∞—Å—Å –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –º–æ–¥–µ–ª—è–º–∏\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def download_dostoevsky_model():\n",
    "        \"\"\"–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ Dostoevsky\"\"\"\n",
    "        print(\"–ü–æ–ø—ã—Ç–∫–∞ —Å–∫–∞—á–∞—Ç—å –º–æ–¥–µ–ª—å Dostoevsky...\")\n",
    "        try:\n",
    "            import requests\n",
    "            import zipfile\n",
    "            import tempfile\n",
    "            import shutil\n",
    "            \n",
    "            # URL –º–æ–¥–µ–ª–∏ Dostoevsky\n",
    "            model_url = \"https://github.com/bureaucratic-labs/dostoevsky/raw/master/dostoevsky/data/models/fasttext-social-network-model.bin\"\n",
    "            \n",
    "            # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é\n",
    "            temp_dir = tempfile.mkdtemp()\n",
    "            model_path = os.path.join(temp_dir, \"fasttext-social-network-model.bin\")\n",
    "            \n",
    "            # –°–∫–∞—á–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å\n",
    "            print(f\"–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ Dostoevsky...\")\n",
    "            response = requests.get(model_url, stream=True)\n",
    "            total_size = int(response.headers.get('content-length', 0))\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                with open(model_path, 'wb') as f:\n",
    "                    for chunk in response.iter_content(chunk_size=8192):\n",
    "                        if chunk:\n",
    "                            f.write(chunk)\n",
    "                \n",
    "                # –ö–æ–ø–∏—Ä—É–µ–º –≤ –Ω—É–∂–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é\n",
    "                import dostoevsky\n",
    "                dostoevsky_dir = os.path.dirname(dostoevsky.__file__)\n",
    "                target_dir = os.path.join(dostoevsky_dir, 'data', 'models')\n",
    "                os.makedirs(target_dir, exist_ok=True)\n",
    "                target_path = os.path.join(target_dir, 'fasttext-social-network-model.bin')\n",
    "                \n",
    "                shutil.copy(model_path, target_path)\n",
    "                print(f\"–ú–æ–¥–µ–ª—å Dostoevsky —É—Å–ø–µ—à–Ω–æ —Å–∫–∞—á–∞–Ω–∞ –≤: {target_path}\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"–û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏: {response.status_code}\")\n",
    "                return False\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ –º–æ–¥–µ–ª–∏ Dostoevsky: {e}\")\n",
    "            print(\"–ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—É—é –º–æ–¥–µ–ª—å...\")\n",
    "            return False\n",
    "    \n",
    "    @staticmethod\n",
    "    def check_and_fix_dostoevsky():\n",
    "        \"\"\"–ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ Dostoevsky\"\"\"\n",
    "        try:\n",
    "            from dostoevsky.tokenization import RegexTokenizer\n",
    "            from dostoevsky.models import FastTextSocialNetworkModel\n",
    "            \n",
    "            # –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å\n",
    "            tokenizer = RegexTokenizer()\n",
    "            model = FastTextSocialNetworkModel(tokenizer=tokenizer)\n",
    "            \n",
    "            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–±–æ—Ç—É –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º —Ç–µ–∫—Å—Ç–µ\n",
    "            test_result = model.predict([\"—Ç–µ—Å—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç\"], k=2)\n",
    "            \n",
    "            print(\"‚úì Dostoevsky –ø—Ä–æ–≤–µ—Ä–µ–Ω –∏ —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Dostoevsky —Ç—Ä–µ–±—É–µ—Ç –Ω–∞—Å—Ç—Ä–æ–π–∫–∏: {str(e)[:100]}\")\n",
    "            \n",
    "            # –ü—Ä–æ–±—É–µ–º —Å–∫–∞—á–∞—Ç—å –º–æ–¥–µ–ª—å\n",
    "            return ModelDownloader.download_dostoevsky_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc3fda45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileReader:\n",
    "    \"\"\"–ö–ª–∞—Å—Å –¥–ª—è —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def detect_file_encoding(file_path: str) -> Tuple[str, float]:\n",
    "        \"\"\"–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–¥–∏—Ä–æ–≤–∫–∏ —Ñ–∞–π–ª–∞\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'rb') as f:\n",
    "                raw_data = f.read(100000)  # –ß–∏—Ç–∞–µ–º –ø–µ—Ä–≤—ã–µ 100–ö–ë –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–æ–¥–∏—Ä–æ–≤–∫–∏\n",
    "                result = chardet.detect(raw_data)\n",
    "                return result['encoding'], result['confidence']\n",
    "        except Exception as e:\n",
    "            print(f\"–û—à–∏–±–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–æ–¥–∏—Ä–æ–≤–∫–∏: {e}\")\n",
    "            return 'utf-8', 0.0\n",
    "    \n",
    "    @staticmethod\n",
    "    def detect_file_format(file_path: str) -> str:\n",
    "        \"\"\"–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ–æ—Ä–º–∞—Ç–∞ —Ñ–∞–π–ª–∞\"\"\"\n",
    "        if not os.path.exists(file_path):\n",
    "            raise FileNotFoundError(f\"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}\")\n",
    "        \n",
    "        # –ü–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é\n",
    "        ext = os.path.splitext(file_path)[1].lower()\n",
    "        \n",
    "        if ext in ['.xlsx', '.xls']:\n",
    "            return 'excel'\n",
    "        elif ext in ['.csv', '.txt']:\n",
    "            # –ß–∏—Ç–∞–µ–º –ø–µ—Ä–≤—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è\n",
    "            try:\n",
    "                encoding, confidence = FileReader.detect_file_encoding(file_path)\n",
    "                with open(file_path, 'r', encoding=encoding, errors='ignore') as f:\n",
    "                    first_lines = [f.readline() for _ in range(5)]\n",
    "                \n",
    "                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–π\n",
    "                for line in first_lines:\n",
    "                    if '|' in line and line.count('|') > 2:\n",
    "                        return 'csv_pipe'\n",
    "                    elif ';' in line and line.count(';') > 2:\n",
    "                        return 'csv_semicolon'\n",
    "                    elif '\\t' in line and line.count('\\t') > 2:\n",
    "                        return 'csv_tab'\n",
    "                    elif ',' in line and line.count(',') > 2:\n",
    "                        return 'csv_comma'\n",
    "                \n",
    "                return 'csv_auto'\n",
    "            except:\n",
    "                return 'csv_auto'\n",
    "        else:\n",
    "            return 'unknown'\n",
    "    \n",
    "    @staticmethod\n",
    "    def read_file(file_path: str) -> pd.DataFrame:\n",
    "        \"\"\"–ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞ –ª—é–±–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞\"\"\"\n",
    "        print(f\"–ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞: {file_path}\")\n",
    "        \n",
    "        if not os.path.exists(file_path):\n",
    "            raise FileNotFoundError(f\"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}\")\n",
    "        \n",
    "        file_format = FileReader.detect_file_format(file_path)\n",
    "        print(f\"–û–ø—Ä–µ–¥–µ–ª–µ–Ω —Ñ–æ—Ä–º–∞—Ç: {file_format}\")\n",
    "        \n",
    "        try:\n",
    "            if file_format == 'excel':\n",
    "                # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –¥–≤–∏–∂–∫–∏ –¥–ª—è Excel\n",
    "                try:\n",
    "                    df = pd.read_excel(file_path, engine='openpyxl')\n",
    "                    print(\"‚úì –ü—Ä–æ—á–∏—Ç–∞–Ω–æ —Å –ø–æ–º–æ—â—å—é openpyxl\")\n",
    "                except Exception as e1:\n",
    "                    print(f\"openpyxl –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª: {e1}\")\n",
    "                    try:\n",
    "                        df = pd.read_excel(file_path, engine='xlrd')\n",
    "                        print(\"‚úì –ü—Ä–æ—á–∏—Ç–∞–Ω–æ —Å –ø–æ–º–æ—â—å—é xlrd\")\n",
    "                    except Exception as e2:\n",
    "                        print(f\"xlrd –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª: {e2}\")\n",
    "                        # –ü—Ä–æ–±—É–µ–º –∫–∞–∫ CSV —Å –∞–≤—Ç–æ–¥–µ—Ç–µ–∫—Ç–æ–º\n",
    "                        encoding, _ = FileReader.detect_file_encoding(file_path)\n",
    "                        df = pd.read_csv(file_path, encoding=encoding, engine='python')\n",
    "                        print(\"‚úì –ü—Ä–æ—á–∏—Ç–∞–Ω–æ –∫–∞–∫ CSV (—Ä–µ–∑–µ—Ä–≤–Ω—ã–π –º–µ—Ç–æ–¥)\")\n",
    "            \n",
    "            elif file_format == 'csv_pipe':\n",
    "                encoding, _ = FileReader.detect_file_encoding(file_path)\n",
    "                df = pd.read_csv(file_path, sep='|', encoding=encoding, engine='python', on_bad_lines='skip')\n",
    "            \n",
    "            elif file_format == 'csv_semicolon':\n",
    "                encoding, _ = FileReader.detect_file_encoding(file_path)\n",
    "                df = pd.read_csv(file_path, sep=';', encoding=encoding, engine='python', on_bad_lines='skip')\n",
    "            \n",
    "            elif file_format == 'csv_tab':\n",
    "                encoding, _ = FileReader.detect_file_encoding(file_path)\n",
    "                df = pd.read_csv(file_path, sep='\\t', encoding=encoding, engine='python', on_bad_lines='skip')\n",
    "            \n",
    "            elif file_format == 'csv_comma':\n",
    "                encoding, _ = FileReader.detect_file_encoding(file_path)\n",
    "                df = pd.read_csv(file_path, sep=',', encoding=encoding, engine='python', on_bad_lines='skip')\n",
    "            \n",
    "            else:  # csv_auto –∏–ª–∏ unknown\n",
    "                encoding, _ = FileReader.detect_file_encoding(file_path)\n",
    "                # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏\n",
    "                for sep in [',', ';', '\\t', '|']:\n",
    "                    try:\n",
    "                        df = pd.read_csv(file_path, sep=sep, encoding=encoding, \n",
    "                                        engine='python', on_bad_lines='skip')\n",
    "                        print(f\"‚úì –ü—Ä–æ—á–∏—Ç–∞–Ω–æ —Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–º '{sep}'\")\n",
    "                        break\n",
    "                    except:\n",
    "                        continue\n",
    "                else:\n",
    "                    # –ü–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞: –±–µ–∑ —É–∫–∞–∑–∞–Ω–∏—è —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è\n",
    "                    df = pd.read_csv(file_path, encoding=encoding, engine='python', \n",
    "                                   on_bad_lines='skip')\n",
    "            \n",
    "            # –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "            df = FileReader.clean_dataframe(df)\n",
    "            \n",
    "            print(f\"‚úì –£—Å–ø–µ—à–Ω–æ –ø—Ä–æ—á–∏—Ç–∞–Ω–æ {len(df)} —Å—Ç—Ä–æ–∫\")\n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞: {e}\")\n",
    "            \n",
    "            # –ü–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞: –ø—Ä–æ—á–∏—Ç–∞—Ç—å –∫–∞–∫ –±–∏–Ω–∞—Ä–Ω—ã–π –∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å\n",
    "            try:\n",
    "                with open(file_path, 'rb') as f:\n",
    "                    content = f.read()\n",
    "                \n",
    "                # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –∫–æ–¥–∏—Ä–æ–≤–∫–∏\n",
    "                for encoding in ['utf-8', 'cp1251', 'cp1252', 'latin1', 'koi8-r']:\n",
    "                    try:\n",
    "                        text = content.decode(encoding)\n",
    "                        lines = text.split('\\n')\n",
    "                        \n",
    "                        # –°–æ–∑–¥–∞–µ–º DataFrame –∏–∑ —Å—Ç—Ä–æ–∫\n",
    "                        data = []\n",
    "                        for line in lines[:1000]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 1000 —Å—Ç—Ä–æ–∫\n",
    "                            if line.strip():\n",
    "                                data.append([line.strip()])\n",
    "                        \n",
    "                        df = pd.DataFrame(data, columns=['–¢–µ–∫—Å—Ç'])\n",
    "                        print(f\"‚úì –ü—Ä–æ—á–∏—Ç–∞–Ω–æ –≤—Ä—É—á–Ω—É—é —Å –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π {encoding}\")\n",
    "                        return df\n",
    "                    except:\n",
    "                        continue\n",
    "            except Exception as e2:\n",
    "                print(f\"–í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ —á—Ç–µ–Ω–∏—è –Ω–µ —É–¥–∞–ª–∏—Å—å: {e2}\")\n",
    "            \n",
    "            # –°–æ–∑–¥–∞–µ–º –ø—É—Å—Ç–æ–π DataFrame –µ—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å\n",
    "            print(\"‚ö† –°–æ–∑–¥–∞–µ–º –ø—É—Å—Ç–æ–π DataFrame\")\n",
    "            return pd.DataFrame(columns=['–¢–µ–∫—Å—Ç'])\n",
    "    \n",
    "    @staticmethod\n",
    "    def clean_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"–û—á–∏—Å—Ç–∫–∞ DataFrame\"\"\"\n",
    "        # –£–¥–∞–ª—è–µ–º –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø—É—Å—Ç—ã–µ —Å—Ç–æ–ª–±—Ü—ã\n",
    "        df = df.dropna(axis=1, how='all')\n",
    "        \n",
    "        # –£–¥–∞–ª—è–µ–º –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏\n",
    "        df = df.dropna(how='all')\n",
    "        \n",
    "        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤—Å–µ —Å—Ç–æ–ª–±—Ü—ã –≤ —Å—Ç—Ä–æ–∫–∏\n",
    "        for col in df.columns:\n",
    "            df[col] = df[col].astype(str).str.strip()\n",
    "        \n",
    "        # –ò—â–µ–º –∫–æ–ª–æ–Ω–∫—É —Å —Ç–µ–∫—Å—Ç–æ–º\n",
    "        text_columns = []\n",
    "        for col in df.columns:\n",
    "            col_lower = col.lower()\n",
    "            if any(keyword in col_lower for keyword in ['—Ç–µ–∫—Å—Ç', 'text', '—Å–æ–æ–±—â–µ–Ω–∏–µ', 'message', '–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π']):\n",
    "                text_columns.append(col)\n",
    "            elif df[col].str.len().mean() > 20:  # –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ > 20 —Å–∏–º–≤–æ–ª–æ–≤\n",
    "                text_columns.append(col)\n",
    "        \n",
    "        if text_columns:\n",
    "            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—É—é –Ω–∞–π–¥–µ–Ω–Ω—É—é –∫–æ–ª–æ–Ω–∫—É —Å —Ç–µ–∫—Å—Ç–æ–º\n",
    "            df = df.rename(columns={text_columns[0]: '–¢–µ–∫—Å—Ç'})\n",
    "            print(f\"‚úì –ù–∞–π–¥–µ–Ω–∞ –∫–æ–ª–æ–Ω–∫–∞ —Å —Ç–µ–∫—Å—Ç–æ–º: '{text_columns[0]}'\")\n",
    "        else:\n",
    "            # –°–æ–∑–¥–∞–µ–º –∫–æ–ª–æ–Ω–∫—É –¢–µ–∫—Å—Ç –∏–∑ –≤—Å–µ—Ö —Å—Ç–æ–ª–±—Ü–æ–≤\n",
    "            if len(df.columns) > 1:\n",
    "                df['–¢–µ–∫—Å—Ç'] = df.apply(lambda row: ' '.join(row.astype(str)), axis=1)\n",
    "                print(\"‚úì –°–æ–∑–¥–∞–Ω–∞ –∫–æ–ª–æ–Ω–∫–∞ '–¢–µ–∫—Å—Ç' –∏–∑ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö\")\n",
    "            elif len(df.columns) == 1:\n",
    "                df = df.rename(columns={df.columns[0]: '–¢–µ–∫—Å—Ç'})\n",
    "                print(f\"‚úì –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∞ –∫–æ–ª–æ–Ω–∫–∞ '{df.columns[0]}' –≤ '–¢–µ–∫—Å—Ç'\")\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b190941a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalyzer:\n",
    "    \"\"\"–£–ª—É—á—à–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ —Å –∞–≤—Ç–æ–≤—ã–±–æ—Ä–æ–º –º–æ–¥–µ–ª–µ–π\"\"\"\n",
    "    \n",
    "    def __init__(self, use_dostoevsky: bool = True):\n",
    "        self.models = {}\n",
    "        self.model_priority = []\n",
    "        \n",
    "        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏\n",
    "        self._init_models(use_dostoevsky)\n",
    "    \n",
    "    def _init_models(self, use_dostoevsky: bool):\n",
    "        \"\"\"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π\"\"\"\n",
    "        print(\"\\n–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏...\")\n",
    "        \n",
    "        # 1. Dostoevsky (–µ—Å–ª–∏ —Ç—Ä–µ–±—É–µ—Ç—Å—è –∏ –¥–æ—Å—Ç—É–ø–µ–Ω)\n",
    "        if use_dostoevsky:\n",
    "            dostoevsky_loaded = self._init_dostoevsky()\n",
    "            if dostoevsky_loaded:\n",
    "                self.models['dostoevsky'] = self.dostoevsky_analyze\n",
    "                self.model_priority.append('dostoevsky')\n",
    "        \n",
    "        # 2. Transformers (ruBERT) - –æ—Å–Ω–æ–≤–Ω–æ–π —Ä–µ–∑–µ—Ä–≤\n",
    "        transformers_loaded = self._init_transformers()\n",
    "        if transformers_loaded:\n",
    "            self.models['transformers'] = self.transformers_analyze\n",
    "            self.model_priority.append('transformers')\n",
    "        \n",
    "        # 3. –ü—Ä–æ—Å—Ç–∞—è –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ (–≤—Å–µ–≥–¥–∞ –¥–æ—Å—Ç—É–ø–Ω–∞)\n",
    "        self.models['basic'] = self.basic_analyze\n",
    "        self.model_priority.append('basic')\n",
    "        \n",
    "        print(f\"‚úì –î–æ—Å—Ç—É–ø–Ω–æ –º–æ–¥–µ–ª–µ–π: {len(self.models)} ({', '.join(self.models.keys())})\")\n",
    "        print(f\"  –ü–æ—Ä—è–¥–æ–∫ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞: {', '.join(self.model_priority)}\")\n",
    "    \n",
    "    def _init_dostoevsky(self) -> bool:\n",
    "        \"\"\"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Dostoevsky\"\"\"\n",
    "        try:\n",
    "            # –ü—Ä–æ–±—É–µ–º –∏—Å–ø—Ä–∞–≤–∏—Ç—å Dostoevsky –ø–µ—Ä–µ–¥ –∑–∞–≥—Ä—É–∑–∫–æ–π\n",
    "            if not ModelDownloader.check_and_fix_dostoevsky():\n",
    "                print(\"  ‚úó Dostoevsky: –Ω–µ —É–¥–∞–ª–æ—Å—å –∏—Å–ø—Ä–∞–≤–∏—Ç—å/–∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å\")\n",
    "                return False\n",
    "            \n",
    "            from dostoevsky.tokenization import RegexTokenizer\n",
    "            from dostoevsky.models import FastTextSocialNetworkModel\n",
    "            \n",
    "            self.dostoevsky_tokenizer = RegexTokenizer()\n",
    "            self.dostoevsky_model = FastTextSocialNetworkModel(tokenizer=self.dostoevsky_tokenizer)\n",
    "            \n",
    "            # –¢–µ—Å—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å\n",
    "            test_text = \"–≠—Ç–æ —Ç–µ—Å—Ç–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –º–æ–¥–µ–ª–∏.\"\n",
    "            test_result = self.dostoevsky_model.predict([test_text], k=2)\n",
    "            \n",
    "            print(\"  ‚úì Dostoevsky: –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏ —Ä–∞–±–æ—Ç–∞–µ—Ç\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚úó Dostoevsky: –æ—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ - {str(e)[:100]}\")\n",
    "            return False\n",
    "    \n",
    "    def _init_transformers(self) -> bool:\n",
    "        \"\"\"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Transformers\"\"\"\n",
    "        try:\n",
    "            from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "            \n",
    "            print(\"  –ó–∞–≥—Ä—É–∑–∫–∞ Transformers (ruBERT)...\")\n",
    "            \n",
    "            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ –ª–µ–≥–∫—É—é –º–æ–¥–µ–ª—å –µ—Å–ª–∏ –æ—Å–Ω–æ–≤–Ω–∞—è –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç\n",
    "            model_name = \"blanchefort/rubert-base-cased-sentiment\"\n",
    "            \n",
    "            try:\n",
    "                self.transformers_pipeline = pipeline(\n",
    "                    \"sentiment-analysis\",\n",
    "                    model=model_name,\n",
    "                    tokenizer=model_name,\n",
    "                    framework=\"pt\"\n",
    "                )\n",
    "                print(\"  ‚úì Transformers: –æ—Å–Ω–æ–≤–Ω–∞—è –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞\")\n",
    "            except:\n",
    "                # –†–µ–∑–µ—Ä–≤–Ω–∞—è –º–æ–¥–µ–ª—å\n",
    "                print(\"  –û—Å–Ω–æ–≤–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª–∞—Å—å, –ø—Ä–æ–±—É–µ–º —Ä–µ–∑–µ—Ä–≤–Ω—É—é...\")\n",
    "                model_name = \"seara/rubert-base-cased-ru-go-emotions\"\n",
    "                self.transformers_pipeline = pipeline(\n",
    "                    \"sentiment-analysis\",\n",
    "                    model=model_name,\n",
    "                    tokenizer=model_name,\n",
    "                    framework=\"pt\"\n",
    "                )\n",
    "                print(\"  ‚úì Transformers: —Ä–µ–∑–µ—Ä–≤–Ω–∞—è –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚úó Transformers: –æ—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ - {str(e)[:100]}\")\n",
    "            return False\n",
    "    \n",
    "    def dostoevsky_analyze(self, text: str) -> Dict:\n",
    "        \"\"\"–ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ —Å –ø–æ–º–æ—â—å—é Dostoevsky\"\"\"\n",
    "        if not hasattr(self, 'dostoevsky_model'):\n",
    "            raise ValueError(\"Dostoevsky –º–æ–¥–µ–ª—å –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞\")\n",
    "        \n",
    "        try:\n",
    "            results = self.dostoevsky_model.predict([text], k=5)[0]\n",
    "            \n",
    "            sentiment_map = {\n",
    "                'positive': '–ü–æ–∑–∏—Ç–∏–≤–Ω–∞—è',\n",
    "                'negative': '–ù–µ–≥–∞—Ç–∏–≤–Ω–∞—è', \n",
    "                'neutral': '–ù–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è',\n",
    "                'skip': '–ù–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è',\n",
    "                'speech': '–ù–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è'\n",
    "            }\n",
    "            \n",
    "            if results:\n",
    "                main_sentiment = max(results.items(), key=lambda x: x[1])\n",
    "                sentiment_label = sentiment_map.get(main_sentiment[0], '–ù–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è')\n",
    "                \n",
    "                return {\n",
    "                    'sentiment': sentiment_label,\n",
    "                    'confidence': float(main_sentiment[1]),\n",
    "                    'source': 'dostoevsky',\n",
    "                    'details': results\n",
    "                }\n",
    "        except Exception as e:\n",
    "            print(f\"  –û—à–∏–±–∫–∞ Dostoevsky –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ: {e}\")\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def transformers_analyze(self, text: str) -> Dict:\n",
    "        \"\"\"–ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ —Å –ø–æ–º–æ—â—å—é Transformers\"\"\"\n",
    "        if not hasattr(self, 'transformers_pipeline'):\n",
    "            raise ValueError(\"Transformers –º–æ–¥–µ–ª—å –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞\")\n",
    "        \n",
    "        try:\n",
    "            result = self.transformers_pipeline(text[:512])[0]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É\n",
    "            \n",
    "            sentiment_map = {\n",
    "                'POSITIVE': '–ü–æ–∑–∏—Ç–∏–≤–Ω–∞—è',\n",
    "                'NEGATIVE': '–ù–µ–≥–∞—Ç–∏–≤–Ω–∞—è',\n",
    "                'NEUTRAL': '–ù–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è',\n",
    "                'LABEL_0': '–ù–µ–≥–∞—Ç–∏–≤–Ω–∞—è',  # –î–ª—è –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –º–æ–¥–µ–ª–µ–π\n",
    "                'LABEL_1': '–ü–æ–∑–∏—Ç–∏–≤–Ω–∞—è',\n",
    "                'LABEL_2': '–ù–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è'\n",
    "            }\n",
    "            \n",
    "            sentiment = sentiment_map.get(result['label'], '–ù–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è')\n",
    "            \n",
    "            return {\n",
    "                'sentiment': sentiment,\n",
    "                'confidence': float(result['score']),\n",
    "                'source': 'transformers',\n",
    "                'details': result\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"  –û—à–∏–±–∫–∞ Transformers –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ: {e}\")\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def basic_analyze(self, text: str) -> Dict:\n",
    "        \"\"\"–ë–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º\"\"\"\n",
    "        if not text:\n",
    "            return {\n",
    "                'sentiment': '–ù–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è',\n",
    "                'confidence': 0.5,\n",
    "                'source': 'basic'\n",
    "            }\n",
    "        \n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # –°–ª–æ–≤–∞—Ä–∏ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞\n",
    "        positive_patterns = [\n",
    "            r'\\b(–æ—Ç–ª–∏—á–Ω|–ø—Ä–µ–∫—Ä–∞—Å–Ω|–∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω|—Å—É–ø–µ—Ä|–∫–ª–∞—Å—Å|–ª—É—á—à|—Ö–æ—Ä–æ—à|—Ä–µ–∫–æ–º–µ–Ω–¥—É—é|—Å–æ–≤–µ—Ç—É—é|—Å–ø–∞—Å–∏–±–æ|–±–ª–∞–≥–æ–¥–∞—Ä—é|–¥–æ–≤–æ–ª–µ–Ω|—Ä–∞–¥)\\b',\n",
    "            r'[üòÄüòÉüòÑüòÅüòÜüòçü§©üëç‚ù§Ô∏èüòäüôÇüí™]',\n",
    "            r'\\b(–ª—é–±–ª—é|–æ–±–æ–∂–∞—é|–Ω—Ä–∞–≤–∏—Ç—Å—è|–≤–æ—Å—Ö–∏—â–∞—é—Å—å|–æ–¥–æ–±—Ä—è—é|—Ö–≤–∞–ª—é)\\b'\n",
    "        ]\n",
    "        \n",
    "        negative_patterns = [\n",
    "            r'\\b(–ø–ª–æ—Ö|—É–∂–∞—Å–Ω|–∫–æ—à–º–∞—Ä|–æ—Ç–≤—Ä–∞—Ç–∏—Ç–µ–ª—å–Ω|–Ω–µ–∫–∞—á–µ—Å—Ç–≤–µ–Ω|–±–∞–¥—è–∂|–±—Ä–∞–∫|–æ–±–º–∞–Ω|—Ä–∞–∑–≤–æ–¥|–∂—É–ª—å–Ω–∏—á|–≤–æ—Ä—É—é—Ç|–∂–∞–ª–æ–±|–Ω–µ–¥–æ–≤–æ–ª–µ–Ω|–≤–æ–∑–º—É—â–µ–Ω|—Ä–∞–∑–æ—á–∞—Ä–æ–≤–∞–Ω)\\b',\n",
    "            r'[üò†üò°ü§¨üò¢üò≠üò§üëéüíîüòûüò£üò´üò©üòí]',\n",
    "            r'\\b(–ø—Ä–æ—Ç–µ—Å—Ç—É—é|—Ç—Ä–µ–±—É—é|–≤–µ—Ä–Ω–∏—Ç–µ|–Ω–∞–∫–∞–∂—É|–ø—Ä–æ–±–ª–µ–º|—Å–ª–æ–º–∞–ª|–∏—Å–ø–æ—Ä—Ç–∏–ª|–Ω–µ\\s+—Ä–∞–±–æ—Ç–∞–µ—Ç)\\b'\n",
    "        ]\n",
    "        \n",
    "        # –ü–æ–¥—Å—á–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤\n",
    "        pos_count = sum(1 for pattern in positive_patterns \n",
    "                       if re.search(pattern, text_lower, re.IGNORECASE))\n",
    "        neg_count = sum(1 for pattern in negative_patterns \n",
    "                       if re.search(pattern, text_lower, re.IGNORECASE))\n",
    "        \n",
    "        # –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –º–∞—Ä–∫–µ—Ä—ã\n",
    "        if 'üòî' in text or 'üòü' in text or 'üòï' in text or '‚òπÔ∏è' in text:\n",
    "            neg_count += 1\n",
    "        if 'üòÇ' in text or 'ü§£' in text or 'üòé' in text or 'ü§ó' in text:\n",
    "            pos_count += 1\n",
    "        \n",
    "        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏\n",
    "        if pos_count > neg_count and pos_count > 0:\n",
    "            confidence = min(0.5 + (pos_count - neg_count) * 0.1, 0.9)\n",
    "            return {'sentiment': '–ü–æ–∑–∏—Ç–∏–≤–Ω–∞—è', 'confidence': confidence, 'source': 'basic'}\n",
    "        elif neg_count > pos_count and neg_count > 0:\n",
    "            confidence = min(0.5 + (neg_count - pos_count) * 0.1, 0.9)\n",
    "            return {'sentiment': '–ù–µ–≥–∞—Ç–∏–≤–Ω–∞—è', 'confidence': confidence, 'source': 'basic'}\n",
    "        elif pos_count == neg_count and pos_count > 0:\n",
    "            return {'sentiment': '–°–º–µ—à–∞–Ω–Ω–∞—è', 'confidence': 0.5, 'source': 'basic'}\n",
    "        else:\n",
    "            return {'sentiment': '–ù–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è', 'confidence': 0.5, 'source': 'basic'}\n",
    "    \n",
    "    def analyze(self, text: str) -> Dict:\n",
    "        \"\"\"–ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ª—É—á—à–µ–π –¥–æ—Å—Ç—É–ø–Ω–æ–π –º–æ–¥–µ–ª–∏\"\"\"\n",
    "        if not text or len(text.strip()) < 3:\n",
    "            return {\n",
    "                'sentiment': '–ù–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è',\n",
    "                'confidence': 0.5,\n",
    "                'source': 'none'\n",
    "            }\n",
    "        \n",
    "        # –ü—Ä–æ–±—É–µ–º –º–æ–¥–µ–ª–∏ –≤ –ø–æ—Ä—è–¥–∫–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞\n",
    "        for model_name in self.model_priority:\n",
    "            if model_name in self.models:\n",
    "                try:\n",
    "                    result = self.models[model_name](text)\n",
    "                    if result:\n",
    "                        return result\n",
    "                except Exception as e:\n",
    "                    print(f\"  –ú–æ–¥–µ–ª—å {model_name} –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∞: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        # –ï—Å–ª–∏ –≤—Å–µ –º–æ–¥–µ–ª–∏ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—É—é\n",
    "        return self.basic_analyze(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9050021a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MentionAnalyzer:\n",
    "    \"\"\"–û—Å–Ω–æ–≤–Ω–æ–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —É–ø–æ–º–∏–Ω–∞–Ω–∏–π\"\"\"\n",
    "    \n",
    "    # –ü—Ä–µ–¥–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–µ —Ç–µ–≥–∏\n",
    "    UNIVERSAL_TAGS = {\n",
    "        'quality': '–ö–∞—á–µ—Å—Ç–≤–æ –ø—Ä–æ–¥—É–∫—Ç–∞/—É—Å–ª—É–≥–∏',\n",
    "        'comparison': '–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞–º–∏',\n",
    "        'complaint': '–ñ–∞–ª–æ–±–∞/–ø—Ä–æ–±–ª–µ–º–∞',\n",
    "        'positive': '–ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π –æ—Ç–∑—ã–≤',\n",
    "        'question': '–í–æ–ø—Ä–æ—Å/—Å–ø—Ä–∞–≤–∫–∞',\n",
    "        'recommendation': '–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è/—Å–æ–≤–µ—Ç',\n",
    "        'service': '–°–µ—Ä–≤–∏—Å/–æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ',\n",
    "        'price': '–í–æ–ø—Ä–æ—Å—ã —Ü–µ–Ω—ã',\n",
    "        'technical': '–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã',\n",
    "        'safety': '–í–æ–ø—Ä–æ—Å—ã –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏',\n",
    "        'infrastructure': '–ò–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞',\n",
    "        'corporate': '–ö–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è'\n",
    "    }\n",
    "    \n",
    "    def __init__(self, config: Dict):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞\n",
    "        \"\"\"\n",
    "        self.object_name = config.get('object_name', '–û–±—ä–µ–∫—Ç')\n",
    "        self.keywords = [kw.lower().strip() for kw in config.get('keywords', [])]\n",
    "        self.risk_words = [rw.lower().strip() for rw in config.get('risk_words', [])]\n",
    "        self.positive_words = [pw.lower().strip() for pw in config.get('positive_words', [])]\n",
    "        self.competitors = [c.lower().strip() for c in config.get('competitors', [])]\n",
    "        \n",
    "        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏\n",
    "        print(f\"\\n–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ –¥–ª—è: {self.object_name}\")\n",
    "        self.sentiment_analyzer = SentimentAnalyzer(\n",
    "            use_dostoevsky=config.get('use_dostoevsky', True)\n",
    "        )\n",
    "        \n",
    "        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n",
    "        self.stats = {\n",
    "            'total_mentions': 0,\n",
    "            'relevant_mentions': 0,\n",
    "            'negative_mentions': 0,\n",
    "            'dangerous_mentions': 0,\n",
    "            'sentiment_sources': {}\n",
    "        }\n",
    "    \n",
    "    def is_relevant(self, text: str) -> bool:\n",
    "        \"\"\"–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏\"\"\"\n",
    "        if not text or len(text.strip()) < 5:\n",
    "            return False\n",
    "        \n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤\n",
    "        for keyword in self.keywords:\n",
    "            if keyword in text_lower:\n",
    "                return True\n",
    "        \n",
    "        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —á–∞—Å—Ç–∏—á–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π\n",
    "        words = re.findall(r'\\b\\w+\\b', text_lower)\n",
    "        for word in words:\n",
    "            for keyword in self.keywords:\n",
    "                if keyword in word or word in keyword:\n",
    "                    return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def extract_tags(self, text: str) -> List[str]:\n",
    "        \"\"\"–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–≥–æ–≤\"\"\"\n",
    "        tags = []\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # 1. –ñ–∞–ª–æ–±—ã (–≤—ã—Å—à–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)\n",
    "        if re.search(r'–∂–∞–ª–æ–±|–Ω–µ–¥–æ–≤–æ–ª|–≤–æ–∑–º—É—â|–ø—Ä–æ—Ç–µ—Å—Ç|–ø—Ä–µ—Ç–µ–Ω–∑|—Ç—Ä–µ–±—É—é|–≤–µ—Ä–Ω–∏—Ç–µ', text_lower):\n",
    "            tags.append(self.UNIVERSAL_TAGS['complaint'])\n",
    "        \n",
    "        # 2. –ö–∞—á–µ—Å—Ç–≤–æ\n",
    "        if re.search(r'–∫–∞—á–µ—Å—Ç–≤|–Ω–µ–∫–∞—á–µ—Å—Ç–≤–µ–Ω|–ø–ª–æ—Ö|—É–∂–∞—Å|–∫–æ—à–º–∞—Ä|–±–∞–¥—è–∂', text_lower):\n",
    "            tags.append(self.UNIVERSAL_TAGS['quality'])\n",
    "        \n",
    "        # 3. –ü–æ–∑–∏—Ç–∏–≤–Ω—ã–µ –æ—Ç–∑—ã–≤—ã\n",
    "        if re.search(r'–æ—Ç–ª–∏—á–Ω|–ø—Ä–µ–∫—Ä–∞—Å–Ω|–∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω|—Ö–æ—Ä–æ—à|—Ä–µ–∫–æ–º–µ–Ω–¥—É—é|—Å–ø–∞—Å–∏–±–æ', text_lower):\n",
    "            tags.append(self.UNIVERSAL_TAGS['positive'])\n",
    "        \n",
    "        # 4. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞–º–∏\n",
    "        if self.competitors:\n",
    "            for competitor in self.competitors:\n",
    "                if competitor in text_lower:\n",
    "                    tags.append(self.UNIVERSAL_TAGS['comparison'])\n",
    "                    break\n",
    "        \n",
    "        # 5. –í–æ–ø—Ä–æ—Å—ã\n",
    "        if '?' in text or any(word in text_lower for word in ['–∫–∞–∫', '—á—Ç–æ', '–≥–¥–µ', '–ø–æ–¥—Å–∫–∞–∂–∏—Ç–µ']):\n",
    "            tags.append(self.UNIVERSAL_TAGS['question'])\n",
    "        \n",
    "        # 6. –¶–µ–Ω–∞\n",
    "        if re.search(r'—Ü–µ–Ω|—Å—Ç–æ–∏–º–æ—Å—Ç|–¥–æ—Ä–æ–≥|–¥–µ—à–µ–≤', text_lower):\n",
    "            tags.append(self.UNIVERSAL_TAGS['price'])\n",
    "        \n",
    "        # 7. –°–µ—Ä–≤–∏—Å\n",
    "        if re.search(r'—Å–µ—Ä–≤–∏—Å|–æ–±—Å–ª—É–∂–∏–≤–∞–Ω|–ø–µ—Ä—Å–æ–Ω–∞–ª|–∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü', text_lower):\n",
    "            tags.append(self.UNIVERSAL_TAGS['service'])\n",
    "        \n",
    "        # 8. –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã\n",
    "        if re.search(r'—Ç–µ—Ö–Ω–∏—á–µ—Å–∫|–¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫|—Ä–µ–º–æ–Ω—Ç|–¥–≤–∏–≥–∞—Ç–µ–ª|–±–µ–Ω–∑–∏–Ω|—Ç–æ–ø–ª–∏–≤', text_lower):\n",
    "            tags.append(self.UNIVERSAL_TAGS['technical'])\n",
    "        \n",
    "        # –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Ç–µ–≥–∏\n",
    "        unique_tags = []\n",
    "        for tag in tags:\n",
    "            if tag not in unique_tags:\n",
    "                unique_tags.append(tag)\n",
    "        \n",
    "        return unique_tags[:5]\n",
    "    \n",
    "    def is_dangerous(self, text: str, sentiment: str, tags: List[str]) -> bool:\n",
    "        \"\"\"–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –¥–ª—è —Ä–µ–ø—É—Ç–∞—Ü–∏–∏\"\"\"\n",
    "        text_lower = text.lower()\n",
    "        danger_score = 0\n",
    "        \n",
    "        # 1. –ù–µ–≥–∞—Ç–∏–≤–Ω–∞—è —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å\n",
    "        if sentiment == '–ù–µ–≥–∞—Ç–∏–≤–Ω–∞—è':\n",
    "            danger_score += 2\n",
    "        \n",
    "        # 2. –ñ–∞–ª–æ–±–∞ –≤ —Ç–µ–≥–∞—Ö\n",
    "        if self.UNIVERSAL_TAGS['complaint'] in tags:\n",
    "            danger_score += 2\n",
    "        \n",
    "        # 3. –†–∏—Å–∫-—Å–ª–æ–≤–∞\n",
    "        for risk_word in self.risk_words:\n",
    "            if risk_word in text_lower:\n",
    "                danger_score += 1\n",
    "                if danger_score >= 3:  # –£—Å–∫–æ—Ä–µ–Ω–Ω—ã–π –≤—ã—Ö–æ–¥\n",
    "                    return True\n",
    "        \n",
    "        # 4. –ü—Ä–∏–∑—ã–≤—ã –∫ –¥–µ–π—Å—Ç–≤–∏—é\n",
    "        if re.search(r'–Ω–µ\\s+–ø–æ–∫—É–ø–∞–π|–Ω–µ\\s+–ø–æ–ª—å–∑—É–π|–±–æ–π–∫–æ—Ç–∏—Ä|—Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–∏|–ø—Ä–µ–¥—É–ø—Ä–µ–¥–∏', text_lower):\n",
    "            danger_score += 2\n",
    "        \n",
    "        # –û–ø–∞—Å–Ω—ã–º —Å—á–∏—Ç–∞–µ—Ç—Å—è —Å–æ–æ–±—â–µ–Ω–∏–µ —Å 3+ –±–∞–ª–ª–∞–º–∏\n",
    "        return danger_score >= 3\n",
    "    \n",
    "    def analyze_mention(self, text: str) -> Dict:\n",
    "        \"\"\"–ê–Ω–∞–ª–∏–∑ –æ–¥–Ω–æ–≥–æ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è\"\"\"\n",
    "        self.stats['total_mentions'] += 1\n",
    "        \n",
    "        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏\n",
    "        relevant = self.is_relevant(text)\n",
    "        if not relevant:\n",
    "            return {\n",
    "                'relevant': False,\n",
    "                'sentiment': '–ù–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ',\n",
    "                'tags': [],\n",
    "                'dangerous': False\n",
    "            }\n",
    "        \n",
    "        self.stats['relevant_mentions'] += 1\n",
    "        \n",
    "        # –ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏\n",
    "        sentiment_result = self.sentiment_analyzer.analyze(text)\n",
    "        sentiment = sentiment_result['sentiment']\n",
    "        source = sentiment_result.get('source', 'unknown')\n",
    "        \n",
    "        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤\n",
    "        if source not in self.stats['sentiment_sources']:\n",
    "            self.stats['sentiment_sources'][source] = 0\n",
    "        self.stats['sentiment_sources'][source] += 1\n",
    "        \n",
    "        if sentiment == '–ù–µ–≥–∞—Ç–∏–≤–Ω–∞—è':\n",
    "            self.stats['negative_mentions'] += 1\n",
    "        \n",
    "        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–≥–æ–≤\n",
    "        tags = self.extract_tags(text)\n",
    "        \n",
    "        # –û—Ü–µ–Ω–∫–∞ –æ–ø–∞—Å–Ω–æ—Å—Ç–∏\n",
    "        dangerous = self.is_dangerous(text, sentiment, tags)\n",
    "        if dangerous:\n",
    "            self.stats['dangerous_mentions'] += 1\n",
    "        \n",
    "        return {\n",
    "            'relevant': True,\n",
    "            'sentiment': sentiment,\n",
    "            'tags': tags,\n",
    "            'dangerous': dangerous,\n",
    "            'confidence': sentiment_result.get('confidence', 0.5),\n",
    "            'source': source\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f67862d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExcelProcessor:\n",
    "    \"\"\"–û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Ñ–∞–π–ª–æ–≤\"\"\"\n",
    "    \n",
    "    def __init__(self, analyzer: MentionAnalyzer):\n",
    "        self.analyzer = analyzer\n",
    "    \n",
    "    def process_file(self, input_path: str, output_path: str):\n",
    "        \"\"\"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞\"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞: {input_path}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # –ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞\n",
    "        df = FileReader.read_file(input_path)\n",
    "        \n",
    "        if df.empty:\n",
    "            print(\"–§–∞–π–ª –ø—É—Å—Ç –∏–ª–∏ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –¥–∞–Ω–Ω—ã—Ö\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"–ù–∞–π–¥–µ–Ω–æ {len(df)} —Å—Ç—Ä–æ–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞\")\n",
    "        \n",
    "        # –ê–Ω–∞–ª–∏–∑ –∫–∞–∂–¥–æ–π —Å—Ç—Ä–æ–∫–∏\n",
    "        results = []\n",
    "        from tqdm import tqdm\n",
    "        \n",
    "        for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"–ê–Ω–∞–ª–∏–∑\"):\n",
    "            text = row.get('–¢–µ–∫—Å—Ç', '')\n",
    "            \n",
    "            if pd.isna(text) or not isinstance(text, str):\n",
    "                result = {\n",
    "                    '–°–æ–æ–±—â–µ–Ω–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ?': '–ù–µ—Ç',\n",
    "                    '–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å': '–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö',\n",
    "                    '–û–ø–∞—Å–Ω–æ –¥–ª—è —Ä–µ–ø—É—Ç–∞—Ü–∏–∏': '–ù–µ—Ç'\n",
    "                }\n",
    "            else:\n",
    "                analysis = self.analyzer.analyze_mention(str(text))\n",
    "                \n",
    "                result = {\n",
    "                    '–°–æ–æ–±—â–µ–Ω–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ?': '–î–∞' if analysis['relevant'] else '–ù–µ—Ç',\n",
    "                    '–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å': analysis['sentiment'],\n",
    "                    '–û–ø–∞—Å–Ω–æ –¥–ª—è —Ä–µ–ø—É—Ç–∞—Ü–∏–∏': '–î–∞' if analysis['dangerous'] else '–ù–µ—Ç'\n",
    "                }\n",
    "            \n",
    "            # –ó–∞–ø–æ–ª–Ω—è–µ–º —Ç–µ–≥–∏\n",
    "            tags = analysis.get('tags', []) if 'analysis' in locals() else []\n",
    "            for i in range(1, 6):\n",
    "                if i <= len(tags):\n",
    "                    result[f'–¢–µ–≥ {i}'] = tags[i-1]\n",
    "                else:\n",
    "                    result[f'–¢–µ–≥ {i}'] = ''\n",
    "            \n",
    "            results.append(result)\n",
    "        \n",
    "        # –°–æ–∑–¥–∞–µ–º DataFrame —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏\n",
    "        results_df = pd.DataFrame(results)\n",
    "        \n",
    "        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å –∏—Å—Ö–æ–¥–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏\n",
    "        output_df = pd.concat([df, results_df], axis=1)\n",
    "        \n",
    "        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n",
    "        return self.save_results(output_df, output_path)\n",
    "    \n",
    "    def save_results(self, df: pd.DataFrame, output_path: str) -> str:\n",
    "        \"\"\"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\"\"\"\n",
    "        try:\n",
    "            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ–æ—Ä–º–∞—Ç –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é\n",
    "            if output_path.endswith('.xlsx') or output_path.endswith('.xls'):\n",
    "                df.to_excel(output_path, index=False, engine='openpyxl')\n",
    "                print(f\"‚úì –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ Excel: {output_path}\")\n",
    "            else:\n",
    "                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ CSV –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é\n",
    "                if not output_path.endswith('.csv'):\n",
    "                    output_path += '.csv'\n",
    "                df.to_csv(output_path, index=False, encoding='utf-8-sig', sep=';')\n",
    "                print(f\"‚úì –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ CSV: {output_path}\")\n",
    "            \n",
    "            return output_path\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}\")\n",
    "            \n",
    "            # –ü—Ä–æ–±—É–µ–º —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫–∞–∫ CSV —Å –¥—Ä—É–≥–æ–π –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π\n",
    "            try:\n",
    "                csv_path = output_path.replace('.xls', '_backup.csv').replace('.xlsx', '_backup.csv')\n",
    "                df.to_csv(csv_path, index=False, encoding='utf-8', sep=',')\n",
    "                print(f\"‚úì –†–µ–∑–µ—Ä–≤–Ω–∞—è –∫–æ–ø–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {csv_path}\")\n",
    "                return csv_path\n",
    "            except Exception as e2:\n",
    "                print(f\"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –¥–∞–∂–µ —Ä–µ–∑–µ—Ä–≤–Ω—É—é –∫–æ–ø–∏—é: {e2}\")\n",
    "                return None\n",
    "    \n",
    "    def print_statistics(self):\n",
    "        \"\"\"–í—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏\"\"\"\n",
    "        stats = self.analyzer.stats\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"–°–¢–ê–¢–ò–°–¢–ò–ö–ê –ê–ù–ê–õ–ò–ó–ê\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"–í—Å–µ–≥–æ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π: {stats['total_mentions']}\")\n",
    "        \n",
    "        if stats['total_mentions'] > 0:\n",
    "            relevant_pct = (stats['relevant_mentions'] / stats['total_mentions']) * 100\n",
    "            print(f\"–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö: {stats['relevant_mentions']} ({relevant_pct:.1f}%)\")\n",
    "            print(f\"–ù–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö: {stats['negative_mentions']}\")\n",
    "            print(f\"–û–ø–∞—Å–Ω—ã—Ö –¥–ª—è —Ä–µ–ø—É—Ç–∞—Ü–∏–∏: {stats['dangerous_mentions']}\")\n",
    "            \n",
    "            if stats['sentiment_sources']:\n",
    "                print(\"\\n–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏:\")\n",
    "                for source, count in stats['sentiment_sources'].items():\n",
    "                    pct = (count / stats['relevant_mentions']) * 100 if stats['relevant_mentions'] > 0 else 0\n",
    "                    print(f\"  {source}: {count} ({pct:.1f}%)\")\n",
    "        \n",
    "        print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "502d8571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è –±—Ä–µ–Ω–¥–æ–≤\n",
    "def get_config(brand_name: str) -> Dict:\n",
    "    \"\"\"–ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è –±—Ä–µ–Ω–¥–∞\"\"\"\n",
    "    \n",
    "    if brand_name == '–ì–∞–∑–ø—Ä–æ–º –Ω–µ—Ñ—Ç—å':\n",
    "        return {\n",
    "            'object_name': '–ì–∞–∑–ø—Ä–æ–º –Ω–µ—Ñ—Ç—å',\n",
    "            'keywords': [\n",
    "                '–≥–∞–∑–ø—Ä–æ–º–Ω–µ—Ñ—Ç—å', '–≥–∞–∑–ø—Ä–æ–º', '–∞–∑—Å –≥–∞–∑–ø—Ä–æ–º', \n",
    "                '–≥–ø–Ω', 'gazprom', 'gazpromneft', 'gdrive',\n",
    "                '–≥–∞–∑–ø—Ä–æ–º –Ω–µ—Ñ—Ç—å', '–≥–∞–∑–ø—Ä–æ–º–Ω–µ—Ñ—Ç–∏'\n",
    "            ],\n",
    "            'risk_words': [\n",
    "                '–±–∞–¥—è–∂', '–Ω–µ–∫–∞—á–µ—Å—Ç–≤–µ–Ω', '–ø–ª–æ—Ö', '—É–∂–∞—Å', '–∫–æ—à–º–∞—Ä',\n",
    "                '–æ–±–º–∞–Ω', '—Ä–∞–∑–≤–æ–¥', '–∂—É–ª—å–Ω–∏—á', '–±—Ä–∞–∫', '–∏—Å–ø–æ—Ä—á',\n",
    "                '–≥—Ä—è–∑–Ω', '–≤—Ä–µ–¥–Ω', '–æ–ø–∞—Å–Ω', '—Ç—Ä—è—Å–µ—Ç', '–¥–µ—Ä–≥–∞–µ—Ç',\n",
    "                '–¥—ã–º', '—Ç—Ä–æ–∏—Ç', '–≥–ª–æ—Ö–Ω–µ—Ç', '—è–º–∞', '—Ä–∞–∑–±–∏—Ç'\n",
    "            ],\n",
    "            'positive_words': [\n",
    "                '—Ö–æ—Ä–æ—à', '–æ—Ç–ª–∏—á–Ω', '–ø—Ä–µ–∫—Ä–∞—Å–Ω', '–∑–∞–º–µ—á–∞—Ç–µ–ª—å–Ω',\n",
    "                '—Ä–µ–∫–æ–º–µ–Ω–¥—É—é', '—Å–æ–≤–µ—Ç—É—é', '—Å–ø–∞—Å–∏–±–æ', '–±–ª–∞–≥–æ–¥–∞—Ä—é',\n",
    "                '–¥–æ–≤–æ–ª–µ–Ω', '—Ä–∞–¥', '–∫–ª–∞—Å—Å', '—Å—É–ø–µ—Ä', '–ª—É—á—à'\n",
    "            ],\n",
    "            'competitors': [\n",
    "                '–ª—É–∫–æ–π–ª', '—Ä–æ—Å–Ω–µ—Ñ—Ç—å', '—Ç–∞—Ç–Ω–µ—Ñ—Ç—å', 'shell', '–±–ø',\n",
    "                'energy', '—ç–Ω–µ—Ä–≥–∏—è', '–∞–∏', '–Ω–µ—Ñ—Ç–µ–º–∞–≥–∏—Å—Ç—Ä–∞–ª—å'\n",
    "            ],\n",
    "            'use_dostoevsky': False  # –ü–æ—Å–∫–æ–ª—å–∫—É Dostoevsky –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç\n",
    "        }\n",
    "    \n",
    "    elif brand_name == '–ú–∞–ª—é—Ç–∫–∞':\n",
    "        return {\n",
    "            'object_name': '–ú–∞–ª—é—Ç–∫–∞',\n",
    "            'keywords': [\n",
    "                '–º–∞–ª—é—Ç–∫–∞', 'nutricia', '–Ω—É—Ç—Ä–∏—Ü–∏—è', '–º–∞–ª—é—Ç–∫',\n",
    "                'malyutka', '–¥–µ—Ç—Å–∫', '–ø–∏—Ç–∞–Ω–∏', '—Å–º–µ—Å', '–∫–∞—à'\n",
    "            ],\n",
    "            'risk_words': [\n",
    "                '–∑–∞–ø–æ—Ä', '–∫–æ–ª–∏–∫–∏', '—Å—ã–ø—å', '—Ä–≤–æ—Ç–∞', '–∞–ª–ª–µ—Ä–≥–∏',\n",
    "                '–ø–ª–æ—Ö', '—É–∂–∞—Å', '–∫–æ—à–º–∞—Ä', '–Ω–µ–∫–∞—á–µ—Å—Ç–≤–µ–Ω', '–≤—Ä–µ–¥–Ω'\n",
    "            ],\n",
    "            'positive_words': [\n",
    "                '–≤–∫—É—Å–Ω', '–Ω—Ä–∞–≤–∏—Ç—Å—è', '–ª—é–±–ª—é', '–æ–±–æ–∂–∞—é', '–æ—Ç–ª–∏—á–Ω',\n",
    "                '—Ö–æ—Ä–æ—à', '—Ä–µ–∫–æ–º–µ–Ω–¥—É—é', '—Å–ø–∞—Å–∏–±–æ', '–¥–æ–≤–µ—Ä—è—é'\n",
    "            ],\n",
    "            'competitors': [\n",
    "                '–Ω—É—Ç—Ä–∏–ª–∞–∫', '–Ω–µ—Å—Ç–ª–µ', '—Ö–∏–ø–ø', '—Å–µ–º–ø–µ—Ä', '—Ñ—Ä–∏—Å–æ'\n",
    "            ],\n",
    "            'use_dostoevsky': False\n",
    "        }\n",
    "    \n",
    "    else:\n",
    "        # –û–±—â–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é\n",
    "        return {\n",
    "            'object_name': brand_name,\n",
    "            'keywords': [brand_name.lower()],\n",
    "            'risk_words': ['–ø–ª–æ—Ö', '–Ω–µ–∫–∞—á–µ—Å—Ç–≤–µ–Ω', '–∂–∞–ª–æ–±', '–ø—Ä–æ–±–ª–µ–º'],\n",
    "            'positive_words': ['—Ö–æ—Ä–æ—à', '–æ—Ç–ª–∏—á–Ω', '—Ä–µ–∫–æ–º–µ–Ω–¥—É—é', '—Å–ø–∞—Å–∏–±–æ'],\n",
    "            'competitors': [],\n",
    "            'use_dostoevsky': False\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e267d4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "–ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ò–ô –ê–ù–ê–õ–ò–ó–ê–¢–û–† –£–ü–û–ú–ï–ù–ê–ù–ò–ô\n",
      "============================================================\n",
      "–ë—Ä–µ–Ω–¥ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: –ì–∞–∑–ø—Ä–æ–º –Ω–µ—Ñ—Ç—å\n",
      "\n",
      "–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ –¥–ª—è: –ì–∞–∑–ø—Ä–æ–º –Ω–µ—Ñ—Ç—å\n",
      "\n",
      "–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏...\n",
      "  –ó–∞–≥—Ä—É–∑–∫–∞ Transformers (ruBERT)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 201/201 [00:00<00:00, 629.98it/s, Materializing param=classifier.weight]                                      \n",
      "\u001b[1mBertForSequenceClassification LOAD REPORT\u001b[0m from: blanchefort/rubert-base-cased-sentiment\n",
      "Key                          | Status     |  | \n",
      "-----------------------------+------------+--+-\n",
      "bert.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úì Transformers: –æ—Å–Ω–æ–≤–Ω–∞—è –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞\n",
      "‚úì –î–æ—Å—Ç—É–ø–Ω–æ –º–æ–¥–µ–ª–µ–π: 2 (transformers, basic)\n",
      "  –ü–æ—Ä—è–¥–æ–∫ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞: transformers, basic\n",
      "\n",
      "============================================================\n",
      "–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞: date_req.xls\n",
      "============================================================\n",
      "–ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞: date_req.xls\n",
      "–û–ø—Ä–µ–¥–µ–ª–µ–Ω —Ñ–æ—Ä–º–∞—Ç: excel\n",
      "openpyxl –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª: File contains no valid workbook part\n",
      "‚úì –ü—Ä–æ—á–∏—Ç–∞–Ω–æ —Å –ø–æ–º–æ—â—å—é xlrd\n",
      "‚úì –ù–∞–π–¥–µ–Ω–∞ –∫–æ–ª–æ–Ω–∫–∞ —Å —Ç–µ–∫—Å—Ç–æ–º: '–¢–µ–∫—Å—Ç'\n",
      "‚úì –£—Å–ø–µ—à–Ω–æ –ø—Ä–æ—á–∏—Ç–∞–Ω–æ 11 —Å—Ç—Ä–æ–∫\n",
      "–ù–∞–π–¥–µ–Ω–æ 11 —Å—Ç—Ä–æ–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "–ê–Ω–∞–ª–∏–∑: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:01<00:00,  9.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ Excel: date_res.xlsx\n",
      "\n",
      "============================================================\n",
      "–°–¢–ê–¢–ò–°–¢–ò–ö–ê –ê–ù–ê–õ–ò–ó–ê\n",
      "============================================================\n",
      "–í—Å–µ–≥–æ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π: 11\n",
      "–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö: 10 (90.9%)\n",
      "–ù–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö: 4\n",
      "–û–ø–∞—Å–Ω—ã—Ö –¥–ª—è —Ä–µ–ø—É—Ç–∞—Ü–∏–∏: 1\n",
      "\n",
      "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏:\n",
      "  transformers: 10 (100.0%)\n",
      "============================================================\n",
      "\n",
      "‚úì –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!\n",
      "‚úì –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: date_res.xlsx\n",
      "\n",
      "–ü–µ—Ä–≤—ã–µ 5 —Å—Ç—Ä–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:\n",
      " Unnamed: 0                                                                                                                                                                                                                                                                                                                                                                                                                                       –¢–µ–∫—Å—Ç –°–æ–æ–±—â–µ–Ω–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ? –¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –û–ø–∞—Å–Ω–æ –¥–ª—è —Ä–µ–ø—É—Ç–∞—Ü–∏–∏                    –¢–µ–≥ 1                    –¢–µ–≥ 2          –¢–µ–≥ 3               –¢–µ–≥ 4  –¢–µ–≥ 5\n",
      "          1 –ê–ó–° –ì–∞–∑–ø—Ä–æ–º –Ω–∞ –©–µ—Ä–±–∞–∫–æ–≤–∞ 2. –í–∏–¥–∏–º–æ –æ–ø—è—Ç—å –Ω–∞—á–∞–ª–∏ –±–∞–¥—è–∂–∏—Ç—å –±–µ–Ω–∑–∏–Ω... –î–≤–µ –Ω–µ–¥–µ–ª–∏ –Ω–∞–∑–∞–¥ –∑–∞–ª–∏–ª–∏ –ø–æ–ª–Ω—ã–π –±–∞–∫ 95–≥–æ. –°–ø—É—Å—Ç—è —á–µ—Ç—ã—Ä–µ –¥–Ω—è- —Å–∏–∑—ã–π –¥—ã–º, –¥–≤–∏–≥–∞—Ç–µ–ª—å —Ç—Ä–æ–∏—Ç, –Ω–∞ —Ö–æ–ª–æ—Å—Ç—ã—Ö —á—É—Ç—å –Ω–µ –≥–ª–æ—Ö–Ω–µ—Ç... –ü—Ä–æ–≤–µ–ª–∏ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫—É —É –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–≥–æ –¥–∏–ª–µ—Ä–∞- –Ω–µ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –±–µ–Ω–∑–∏–Ω. –ê–∫—Ç–∞–Ω–æ–≤–æ–µ —á–∏—Å–ª–æ —Å–æ–æ—Ç–≤–µ—Ç—Å–≤—É–µ—Ç –ê–ò 76 (!!!), –Ω–æ –Ω–∏–∫–∞–∫ –Ω–µ –ê–ò 95.... –û–±–æ—à–ª–∏—Å—å –º–∞–ª–æ–π –∫—Ä–æ–≤—å—é- –ø—Ä–æ–º—ã–≤–∫–∞ —Ç–æ–ø–ª–∏–≤–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã, –Ω–æ –æ—Å–∞–¥–æ—á–µ–∫ –æ—Å—Ç–∞–ª—Å—è... –ü—Ä–∏ —Ç–æ–º –≤ –ø—Ä—è–º–æ–º –∏ –ø–µ—Ä–µ–Ω–æ—Å–Ω–æ–º —Å–º—ã—Å–ª–µ.                    –î–∞  –ù–µ–≥–∞—Ç–∏–≤–Ω–∞—è                   –î–∞ –ö–∞—á–µ—Å—Ç–≤–æ –ø—Ä–æ–¥—É–∫—Ç–∞/—É—Å–ª—É–≥–∏ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞–º–∏ –í–æ–ø—Ä–æ—Å/—Å–ø—Ä–∞–≤–∫–∞ –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã    NaN\n",
      "          2                                                                                                                                                                                                                                                 –†–µ–±—è—Ç–∞ –Ω—É–∂–Ω–∞ –ø–æ–º–æ—â—å, —á–µ–ª–æ–≤–µ–∫ –æ—Å—Ç–∞–ª—Å—è –±–µ–∑ –¥–µ–Ω–µ–≥, —Å—Ç—É–∫–∞–Ω—É–ª –¥–≤–∏–∂–æ–∫, –ø–æ—Ç—Ä–∞—Ç–∏–ª—Å—è –Ω–∞ —Ä–µ–º–æ–Ω—Ç, –µ–¥–µ—Ç –≤ –ò—Ä–∫—É—Ç—Å–∫. –µ–≥–æ –Ω–æ–º–µ—Ä —Ç–µ–ª–µ—Ñ–æ–Ω–∞ 89648222536. —Å—Ç–æ–∏—Ç –Ω–∞ –∑–∞–ø—Ä–∞–≤–∫–µ –Ω–∞ —É–ª–∏—Ü–µ –õ–µ–Ω–∏–Ω–∞ 129 —É –ì–∞–∑–ø—Ä–æ–º–Ω–µ—Ñ—Ç—å                    –î–∞ –ù–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è                  –ù–µ—Ç      –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã                      NaN            NaN                 NaN    NaN\n",
      "          3                                                                                                                                                                                                                                                                          –£ —Ç–µ–±—è –¥–∞–∂–µ –Ω–∞ –∫—Ä—ã—à–∫–µ –±–∞–∫–∞ –Ω–∞–ø–∏—Å–∞–Ω–æ, —á—Ç–æ –º–∏–Ω–∏–º—É–º 95. –ß–µ –≤—ã–∫—Ä–∞–∏–≤–∞—Ç—å 2 —Ä—É–±, –ª–µ–π —á—Ç–æ –ø–æ–ª–∞–≥–∞–µ—Ç—Å—è. –Ø –≤ —Å–≤–æ–π —Ñ—Ñ3 –ª—å—é –ê–ò-100 –Ω–∞ –≥–∞–∑–ø—Ä–æ–º–µ, –æ—á–µ–Ω—å —Ö–æ—Ä–æ—à–æ –º–∞—à–∏–Ω–µ, —Ä–∞—Å—Ö–æ–¥ 8.3                    –î–∞  –ù–µ–≥–∞—Ç–∏–≤–Ω–∞—è                  –ù–µ—Ç      –ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π –æ—Ç–∑—ã–≤ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞–º–∏ –í–æ–ø—Ä–æ—Å/—Å–ø—Ä–∞–≤–∫–∞                 NaN    NaN\n",
      "          4                                                                                                                                                                                                                                               –ó–µ—Ä–∫–∞–ª—å–Ω—ã–π –¥–≤–æ—Ä–∏–∫ ‚Äî —ç—Ç–æ –æ—Ñ–∏—Å–Ω–æ–µ –∑–¥–∞–Ω–∏–µ –ì–∞–∑–ø—Ä–æ–º-–Ω–µ—Ñ—Ç–∏. –û—Å–æ–±–µ–Ω–Ω–æ –∫–ª–∞—Å—Å–Ω–æ –¥–≤–æ—Ä–∏–∫ —Å–º–æ—Ç—Ä–∏—Ç—Å—è –≤ —Å–æ–ª–Ω–µ—á–Ω—É—é –ø–æ–≥–æ–¥—É, –ø–æ—Ç–æ–º—É —á—Ç–æ –∑–µ—Ä–∫–∞–ª—å–Ω—ã–µ —Å—Ç–µ–Ω—ã –æ—Ç–±—Ä–∞—Å—ã–≤–∞—é—Ç –æ–≥—Ä–æ–º–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–ª–∏–∫–æ–≤. #–ª–µ—Ç–æ2019z_90                    –î–∞ –ù–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è                  –ù–µ—Ç           –í–æ–ø—Ä–æ—Å/—Å–ø—Ä–∞–≤–∫–∞                      NaN            NaN                 NaN    NaN\n",
      "          5                                                                                                                                                                   –ö–æ–≥–¥–∞, —É–∂–µ —Å–¥–µ–ª–∞—é—Ç –¥–æ—Ä–æ–≥—É —É –∑–∞–ø—Ä–∞–≤–∫–∏ –ì–∞–∑–ø—Ä–æ–º —É –Ø—Å–µ–Ω—è, —è–º–∞ –Ω–∞ —è–º–µ, –¥–∞–∂–µ –∑–∞–ø—Ä–∞–≤–ª—è—Ç—å—Å—è –∑–∞–µ–∑–∂–∞—Ç—å –Ω–µ —Ö–æ—á–µ—Ç—Å—è –∏–∑-–∑–∞ —ç—Ç–æ–≥–æ. –ú–æ–∂–µ—Ç —Ç–æ—Ç, –∫—Ç–æ –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω—ã–π –∑–∞ —ç—Ç—É —Ç–µ—Ä—Ä–∏—Ç–æ—Ä–∏—é, (–Ω–∞–≤–µ—Ä–Ω–æ–µ —ç—Ç–æ –ø—Ä–∏–ª–∏–≥–∞—é—â–∞—è —Ç–µ—Ä—Ä–∏—Ç–æ—Ä–∏—è –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∫ –∑–∞–ø—Ä–∞–≤–∫–µ) —É–≤–∏–¥–∏—Ç –∏ –ø—Ä–∏–º–µ—Ç –º–µ—Ä—ã. –ê–Ω–æ–Ω–∏–º–Ω–æ –ø–æ–∂–∞–ª—É–π—Å—Ç–∞                    –î–∞ –ù–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è                  –ù–µ—Ç             –í–æ–ø—Ä–æ—Å—ã —Ü–µ–Ω—ã                      NaN            NaN                 NaN    NaN\n",
      "\n",
      "============================================================\n",
      "–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:\n",
      "1. –ï—Å–ª–∏ Dostoevsky –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ Transformers (ruBERT)\n",
      "2. –î–ª—è —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ openpyxl –¥–ª—è .xlsx –∏ xlrd –¥–ª—è .xls\n",
      "3. –î–ª—è CSV —Ñ–∞–π–ª–æ–≤ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç–µ –∫–æ–¥–∏—Ä–æ–≤–∫—É —Å –ø–æ–º–æ—â—å—é chardet\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"–ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ò–ô –ê–ù–ê–õ–ò–ó–ê–¢–û–† –£–ü–û–ú–ï–ù–ê–ù–ò–ô\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –±—Ä–µ–Ω–¥\n",
    "    brand_name = '–ì–∞–∑–ø—Ä–æ–º –Ω–µ—Ñ—Ç—å'\n",
    "    print(f\"–ë—Ä–µ–Ω–¥ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: {brand_name}\")\n",
    "    \n",
    "    # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é\n",
    "    config = get_config(brand_name)\n",
    "    \n",
    "    # –°–æ–∑–¥–∞–µ–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä\n",
    "    analyzer = MentionAnalyzer(config)\n",
    "    \n",
    "    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–∞–π–ª\n",
    "    input_file = 'date_req.xls'\n",
    "    \n",
    "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞\n",
    "    if not os.path.exists(input_file):\n",
    "        print(f\"\\n‚ö† –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {input_file}\")\n",
    "        print(\"–ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ...\")\n",
    "        \n",
    "        # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π DataFrame\n",
    "        test_data = [\n",
    "            \"–ê–ó–° –ì–∞–∑–ø—Ä–æ–º –Ω–∞ –©–µ—Ä–±–∞–∫–æ–≤–∞ 2. –í–∏–¥–∏–º–æ –æ–ø—è—Ç—å –Ω–∞—á–∞–ª–∏ –±–∞–¥—è–∂–∏—Ç—å –±–µ–Ω–∑–∏–Ω...\",\n",
    "            \"–†–µ–±—è—Ç–∞ –Ω—É–∂–Ω–∞ –ø–æ–º–æ—â—å, —á–µ–ª–æ–≤–µ–∫ –æ—Å—Ç–∞–ª—Å—è –±–µ–∑ –¥–µ–Ω–µ–≥, —Å—Ç–æ–∏—Ç –Ω–∞ –∑–∞–ø—Ä–∞–≤–∫–µ –ì–∞–∑–ø—Ä–æ–º–Ω–µ—Ñ—Ç—å\",\n",
    "            \"–£ —Ç–µ–±—è –¥–∞–∂–µ –Ω–∞ –∫—Ä—ã—à–∫–µ –±–∞–∫–∞ –Ω–∞–ø–∏—Å–∞–Ω–æ, —á—Ç–æ –º–∏–Ω–∏–º—É–º 95. –Ø –≤ —Å–≤–æ–π —Ñ—Ñ3 –ª—å—é –ê–ò-100 –Ω–∞ –≥–∞–∑–ø—Ä–æ–º–µ, –æ—á–µ–Ω—å —Ö–æ—Ä–æ—à–æ –º–∞—à–∏–Ω–µ\",\n",
    "            \"—Å–º–µ—Ö, –≥–∞–∑–ø—Ä–æ–º–Ω–µ—Ñ—Ç—å —Å–∞–º —Å–µ–±—è –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞ –≤—à–∏–≤–æ—Å—Ç—å)\",\n",
    "            \"–•–æ—Ä–æ—à–µ–µ —Ç–æ–ø–ª–∏–≤–æ. –•–æ—Ç—è –∫–∞—á–µ—Å—Ç–≤–æ –±–µ–Ω–∑–∏–Ω–∞ –∂–∞–ª—É—é—Ç—Å—è.\"\n",
    "        ]\n",
    "        \n",
    "        df = pd.DataFrame({'–¢–µ–∫—Å—Ç': test_data})\n",
    "        input_file = 'test_data.csv'\n",
    "        df.to_csv(input_file, index=False, encoding='utf-8')\n",
    "        print(f\"–°–æ–∑–¥–∞–Ω —Ç–µ—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª: {input_file}\")\n",
    "    \n",
    "    output_file = 'date_res.xlsx'\n",
    "    \n",
    "    # –û–±—Ä–∞–±–æ—Ç–∫–∞\n",
    "    processor = ExcelProcessor(analyzer)\n",
    "    result_file = processor.process_file(input_file, output_file)\n",
    "    \n",
    "    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n",
    "    processor.print_statistics()\n",
    "    \n",
    "    if result_file:\n",
    "        print(f\"\\n‚úì –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!\")\n",
    "        print(f\"‚úì –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {result_file}\")\n",
    "        \n",
    "        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n",
    "        try:\n",
    "            result_df = pd.read_excel(result_file) if result_file.endswith('.xlsx') else pd.read_csv(result_file, sep=';')\n",
    "            print(\"\\n–ü–µ—Ä–≤—ã–µ 5 —Å—Ç—Ä–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:\")\n",
    "            print(result_df.head().to_string(index=False))\n",
    "        except:\n",
    "            pass\n",
    "    else:\n",
    "        print(\"\\n‚úó –í –ø—Ä–æ—Ü–µ—Å—Å–µ –∞–Ω–∞–ª–∏–∑–∞ –≤–æ–∑–Ω–∏–∫–ª–∏ –æ—à–∏–±–∫–∏\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:\")\n",
    "    print(\"1. –ï—Å–ª–∏ Dostoevsky –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ Transformers (ruBERT)\")\n",
    "    print(\"2. –î–ª—è —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ openpyxl –¥–ª—è .xlsx –∏ xlrd –¥–ª—è .xls\")\n",
    "    print(\"3. –î–ª—è CSV —Ñ–∞–π–ª–æ–≤ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç–µ –∫–æ–¥–∏—Ä–æ–≤–∫—É —Å –ø–æ–º–æ—â—å—é chardet\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python 312 test for text (3.12.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
